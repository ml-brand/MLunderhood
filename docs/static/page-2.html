<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ML Underhood — статическая версия (стр. 2/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-11T19%3A43%3A43Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-11T19%3A43%3A43Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-11T19%3A43%3A43Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">ML Underhood</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+pMU3hEMtRO5jMzQy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="165" data-search="впечатления от первого дня recsys 2025 в праге проходит конференция recsys 2025 — и мы по традиции ведём репортаж с мероприятия. первыми впечатлениями с нашим каналом поделился иван романов из яндекс путешествий. слово ивану: прошёл первый день recsys, посвящённый в основном воркшопам. первые доклады разочаровали: много воды в духе «за хорошие рекомендации против плохих». хотел уйти, но попасть на cars (context-aware recsys) было сложно, поэтому остался на вторую часть — и не зря: стало интереснее. несмотря на, как мне кажется, индустриальное название самого воркшопа, большинство статей было академическими, и некоторые — сильно оторваны от реальности. одному человеку пришлось перейти от задачи с персонализацией — sasrec/argus-like арх-рой — к choice modeling, и он долгое время аудитории объяснял, зачем использовал что-то вроде feature tokenize transformer для своей задачи. и правда, зачем? думаю, чтобы самому было интереснее — на графиках от скейла трансформера метрика не росла: layers=1, head=1, dim=16 давал по сути топовый результат. был обзор статьи kp4poi: файнтюнят llm, промптируют посещённые места и после спецтокена ожидают новые poi (points of interest). ничего особенного, просто отметить что llm-based recsys набирает обороты. в тайтле статьи есть «on large-scale datasets», но на одном из слайдов было что-то вроде про 5–10 тыс. пользователей. многие работают над своими агентами для построения маршрутов путешествий с персонализацией. показали две демки, и ни в одной не было чехии, что странно — можно же было всем посетителям конференции попиарить приложение. а другая демка (cityriddler) была только по вене, где до конференции я провёл чудесный день. маршрут по городу и нескольким музеям мне составил chatgpt, вот и у спикеров напросился вопрос: «а не работает ли уже это out of the box в chatgpt и нужен ли продукт в целом, если это только промпт?» если не агентами богаты и llm не тюним, то тогда rag. здесь, несмотря на скепсис, был очень классный слайд от одного из спикеров: «llm as a judge» (можно оценивать не только релевантность ответа запросу, но и, например, вопросом «were retrieved chunks actually relevant» считать что-то вроде precision). не знаю, насколько рабочая конфигурация и не будет ли 100% корреляции скоров «судьи» под каждую из задач (recall, precision, faithfulness, answer relevancy). а ещё среди докладов обнаружил вот такую интересную идею — взяли mixup из image augmentation и применяют над пользовательскими векторами. эффект есть, но метрики были странные. ml underhood впечатления от первого дня recsys 2025 в праге проходит конференция recsys 2025 — и мы по традиции ведём репортаж с мероприятия. первыми впечатлениями с нашим каналом поделился иван романов из яндекс путешествий. слово ивану: прошёл первый день recsys, посвящённый в основном воркшопам. первые доклады разочаровали: много воды в духе «за хорошие рекомендации против плохих». хотел уйти, но попасть на cars (context-aware recsys) было сложно, поэтому остался на вторую часть — и не зря: стало интереснее. несмотря на, как мне кажется, индустриальное название самого воркшопа, большинство статей было академическими, и некоторые — сильно оторваны от реальности. одному человеку пришлось перейти от задачи с персонализацией — sasrec/argus-like арх-рой — к choice modeling, и он долгое время аудитории объяснял, зачем использовал что-то вроде feature tokenize transformer для своей задачи. и правда, зачем? думаю, чтобы самому было интереснее — на графиках от скейла трансформера метрика не росла: layers=1, head=1, dim=16 давал по сути топовый результат. был обзор статьи kp4poi: файнтюнят llm, промптируют посещённые места и после спецтокена ожидают новые poi (points of interest). ничего особенного, просто отметить что llm-based recsys набирает обороты. в тайтле статьи есть «on large-scale datasets», но на одном из слайдов было что-то вроде про 5–10 тыс. пользователей. многие работают над своими агентами для построения маршрутов путешествий с персонализацией. показали две демки, и ни в одной не было чехии, что странно — можно же было всем посетителям конференции попиарить приложение. а другая демка (cityriddler) была только по вене, где до конференции я провёл чудесный день. маршрут по городу и нескольким музеям мне составил chatgpt, вот и у спикеров напросился вопрос: «а не работает ли уже это out of the box в chatgpt и нужен ли продукт в целом, если это только промпт?» если не агентами богаты и llm не тюним, то тогда rag. здесь, несмотря на скепсис, был очень классный слайд от одного из спикеров: «llm as a judge» (можно оценивать не только релевантность ответа запросу, но и, например, вопросом «were retrieved chunks actually relevant» считать что-то вроде precision). не знаю, насколько рабочая конфигурация и не будет ли 100% корреляции скоров «судьи» под каждую из задач (recall, precision, faithfulness, answer relevancy). а ещё среди докладов обнаружил вот такую интересную идею — взяли mixup из image augmentation и применяют над пользовательскими векторами. эффект есть, но метрики были странные. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-23T09:43:49+00:00" href="./posts/165.html">2025-09-23 09:43 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от первого дня RecSys 2025</strong><br><br>В Праге проходит конференция RecSys 2025 — и мы по традиции ведём репортаж с мероприятия. Первыми впечатлениями с нашим каналом поделился Иван Романов из Яндекс Путешествий. Слово Ивану: <br><br><blockquote>Прошёл первый день RecSys, посвящённый в основном воркшопам. Первые доклады разочаровали: много воды в духе «за хорошие рекомендации против плохих». Хотел уйти, но попасть на CARS (Context-Aware RecSys) было сложно, поэтому остался на вторую часть — и не зря: стало интереснее.<br><br>Несмотря на, как мне кажется, индустриальное название самого воркшопа, большинство статей было академическими, и некоторые — сильно оторваны от реальности.<br><br>Одному человеку пришлось перейти от задачи с персонализацией — sasrec/argus-like арх-рой — к choice modeling, и он долгое время аудитории объяснял, зачем использовал что-то вроде feature tokenize transformer для своей задачи. И правда, зачем? Думаю, чтобы самому было интереснее — на графиках от скейла трансформера метрика не росла: layers=1, head=1, dim=16 давал по сути топовый результат.<br><br>Был обзор статьи KP4POI: файнтюнят LLM, промптируют посещённые места и после спецтокена ожидают новые POI (points of interest). Ничего особенного, просто отметить что LLM-based RecSys набирает обороты. В тайтле статьи есть «on Large-scale Datasets», но на одном из слайдов было что-то вроде про 5–10 тыс. пользователей.<br>    <br>Многие работают над своими агентами для построения маршрутов путешествий с персонализацией. Показали две демки, и ни в одной не было Чехии, что странно — можно же было всем посетителям конференции попиарить приложение. А другая демка (CityRiddler) была только по Вене, где до конференции я провёл чудесный день. Маршрут по городу и нескольким музеям мне составил ChatGPT, вот и у спикеров напросился вопрос: «А не работает ли уже это out of the box в ChatGPT и нужен ли продукт в целом, если это только промпт?»<br>    <br>Если не агентами богаты и LLM не тюним, то тогда RAG. Здесь, несмотря на скепсис, был очень классный слайд от одного из спикеров: «LLM as a judge» (можно оценивать не только релевантность ответа запросу, но и, например, вопросом «were retrieved chunks actually relevant» считать что-то вроде Precision). Не знаю, насколько рабочая конфигурация и не будет ли 100% корреляции скоров «судьи» под каждую из задач (recall, precision, faithfulness, answer relevancy).<br>    <br>А ещё среди докладов обнаружил вот такую интересную идею — взяли Mixup из image augmentation и применяют над пользовательскими векторами. Эффект есть, но метрики были странные.</blockquote><br><br><a href="https://t.me/+ItgEWswkEvU3ZmUy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/165_480.webp" srcset="../assets/media/thumbs/165_480.webp 480w, ../assets/media/165.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="165" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 429 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/165" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/165.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="164" data-search="self-rewarding language models сегодня разберём статью о том, как научить языковую модель самостоятельно оценивать качество своих ответов и итеративно улучшаться за счëт этого. direct preference optimization (dpo) раньше большие языковые модели учили примерно так: 1. предобучение без учителя на огромном корпусе текстов; 2. sft — supervised fine-tuning; 3. создание датасета предпочтений (сравнение качества нескольких гипотез llm между собой вручную); 4. обучение reward-модели на датасете предпочтений. 5. rl — reinforcement learning. метод dpo (direct preference optimization) предлагает заменить обучение reward-модели и rl на supervised fine-tuning llm на датасете предпочтений с некоторой лосс-функцией (подробнее в оригинальной статье про dpo). метод авторов статьи авторы предлагают учить llm не только отвечать на вопросы пользователя (instruction following), но и оценивать эти ответы с помощью механизма lmm-as-a-judge. благодаря этому можно автоматизировать создание датасета предпочтений. более подробно, взяв предобученную модель m₀, делают еë supervised fine-tuning на instruction following (ift данные) + оценивание качества ответа (eft данные) — и так получают модель m₁. далее начинается итеративный процесс, при котором: 1. модель mᵢ сама генерирует датасет предпочтений (генерирует гипотезы и оценивает их) обозначаемый aeft(mᵢ); 2. модель mᵢ дообучается на aeft(mᵢ) с помощью dpo — так получаем новую модель mᵢ₊₁. весь процесс выглядит так: m₀ — предобученная llm без fine-tuning. m₁ — модель, инициализированная m₀, а после дообученная на ift+eft в режиме supervised fine-tuning. m₂ — модель, инициализированная m₁ и дообученная на aeft(m₁) в режиме dpo. m₃ — модель, инициализированная m₂ и дообученная на aeft(m₂) в режиме dpo. авторы утверждают, что метод не только помогает нейросетям лучше справляться с инструкциями, но и улучшает их способности к оцениванию ответов. доработав llama 2 70b на трёх итерациях этого подхода, они получили модель, которая превосходит многие существующие системы в таблице лидеров alpacaeval 2.0: например, claude 2, gemini pro и gpt-4 0613. более подробно итерации обучения, применяемые в подходе, описали в канале «душный nlp». разбор подготовил ❣ никита фёдоров ml underhood self-rewarding language models сегодня разберём статью о том, как научить языковую модель самостоятельно оценивать качество своих ответов и итеративно улучшаться за счëт этого. direct preference optimization (dpo) раньше большие языковые модели учили примерно так: 1. предобучение без учителя на огромном корпусе текстов; 2. sft — supervised fine-tuning; 3. создание датасета предпочтений (сравнение качества нескольких гипотез llm между собой вручную); 4. обучение reward-модели на датасете предпочтений. 5. rl — reinforcement learning. метод dpo (direct preference optimization) предлагает заменить обучение reward-модели и rl на supervised fine-tuning llm на датасете предпочтений с некоторой лосс-функцией (подробнее в оригинальной статье про dpo). метод авторов статьи авторы предлагают учить llm не только отвечать на вопросы пользователя (instruction following), но и оценивать эти ответы с помощью механизма lmm-as-a-judge . благодаря этому можно автоматизировать создание датасета предпочтений. более подробно, взяв предобученную модель m₀, делают еë supervised fine-tuning на instruction following (ift данные) + оценивание качества ответа (eft данные) — и так получают модель m₁. далее начинается итеративный процесс, при котором: 1. модель mᵢ сама генерирует датасет предпочтений (генерирует гипотезы и оценивает их) обозначаемый aeft(mᵢ); 2. модель mᵢ дообучается на aeft(mᵢ) с помощью dpo — так получаем новую модель mᵢ₊₁. весь процесс выглядит так: m₀ — предобученная llm без fine-tuning. m₁ — модель, инициализированная m₀, а после дообученная на ift+eft в режиме supervised fine-tuning. m₂ — модель, инициализированная m₁ и дообученная на aeft(m₁) в режиме dpo. m₃ — модель, инициализированная m₂ и дообученная на aeft(m₂) в режиме dpo. авторы утверждают, что метод не только помогает нейросетям лучше справляться с инструкциями, но и улучшает их способности к оцениванию ответов. доработав llama 2 70b на трёх итерациях этого подхода, они получили модель, которая превосходит многие существующие системы в таблице лидеров alpacaeval 2.0: например, claude 2, gemini pro и gpt-4 0613. более подробно итерации обучения, применяемые в подходе, описали в канале «душный nlp». разбор подготовил ❣ никита фёдоров ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-09-02T10:31:31+00:00" href="./posts/164.html">2025-09-02 10:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>Self-rewarding Language Models<br></strong><br>Сегодня разберём <a href="https://arxiv.org/abs/2401.10020" rel="nofollow noopener noreferrer">статью</a> о том, как научить языковую модель самостоятельно оценивать качество своих ответов и итеративно улучшаться за счëт этого.<br><br><strong>Direct Preference Optimization (DPO)<br></strong><br>Раньше большие языковые модели учили примерно так: <br><br>1. Предобучение без учителя на огромном корпусе текстов;<br>2. SFT — supervised fine-tuning;<br>3. Создание датасета предпочтений (сравнение качества нескольких гипотез LLM между собой вручную);<br>4. Обучение reward-модели на датасете предпочтений.<br>5. RL — reinforcement learning.<br><br>Метод DPO (Direct Preference Optimization) предлагает заменить обучение reward-модели и RL на supervised fine-tuning LLM на датасете предпочтений с некоторой лосс-функцией (подробнее в <a href="https://arxiv.org/pdf/2305.18290" rel="nofollow noopener noreferrer">оригинальной статье</a> про DPO).<br><br><strong>Метод авторов статьи</strong><br><br>Авторы предлагают учить LLM не только отвечать на вопросы пользователя (instruction following), но и оценивать эти ответы с помощью механизма <a href="https://arxiv.org/pdf/2306.05685" rel="nofollow noopener noreferrer">LMM-as-a-Judge</a>. Благодаря этому можно автоматизировать создание датасета предпочтений. <br><br>Более подробно, взяв предобученную модель M₀, делают еë supervised fine-tuning на instruction following (IFT данные) + оценивание качества ответа (EFT данные) — и так получают модель M₁. Далее начинается итеративный процесс, при котором:<br><br>1. Модель Mᵢ сама генерирует датасет предпочтений (генерирует гипотезы и оценивает их) обозначаемый AEFT(Mᵢ);<br>2. Модель Mᵢ дообучается на AEFT(Mᵢ) с помощью DPO — так получаем новую модель Mᵢ₊₁.<br><br>Весь процесс выглядит так: <br><br>M₀ — предобученная LLM без fine-tuning.<br>M₁ — модель, инициализированная M₀, а после дообученная на IFT+EFT в режиме supervised fine-tuning. <br>M₂ — модель, инициализированная M₁ и дообученная на AEFT(M₁) в режиме DPO. <br>M₃ — модель, инициализированная M₂ и дообученная на AEFT(M₂) в режиме DPO. <br><br>Авторы утверждают, что метод не только помогает нейросетям лучше справляться с инструкциями, но и улучшает их способности к оцениванию ответов. Доработав Llama 2 70B на трёх итерациях этого подхода, они получили модель, которая превосходит многие существующие системы в таблице лидеров AlpacaEval 2.0: например, Claude 2, Gemini Pro и GPT-4 0613. <br><br>Более подробно итерации обучения, применяемые в подходе, <a href="https://t.me/stuffyNLP/35" rel="nofollow noopener noreferrer">описали</a> в канале «Душный NLP».<br><br><em>Разбор подготовил </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Никита Фёдоров<br></em><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/164_480.webp" srcset="../assets/media/thumbs/164_480.webp 480w, ../assets/media/164.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="164" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 015 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/164" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/164.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="158" data-search="и ещё несколько кадров, чтобы проникнуться атмосферой конференции. p. s. kdd 2026 анонсировали в южной корее: надеемся, про «игру в кальмара» они просто шутят! ml underhood и ещё несколько кадров, чтобы проникнуться атмосферой конференции. p. s. kdd 2026 анонсировали в южной корее: надеемся, про «игру в кальмара» они просто шутят! ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-14T13:22:16+00:00" href="./posts/158.html">2025-08-14 13:22 UTC</a></div>
      </div>
      <div class="post-body">И ещё несколько кадров, чтобы проникнуться атмосферой конференции.<br><br>P. S. KDD 2026 анонсировали в Южной Корее: надеемся, про «Игру в кальмара» они просто шутят! <br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/158_480.webp" srcset="../assets/media/thumbs/158_480.webp 480w, ../assets/media/158.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="158" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/159_480.webp" srcset="../assets/media/thumbs/159_480.webp 480w, ../assets/media/159.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="158" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/160_480.webp" srcset="../assets/media/thumbs/160_480.webp 480w, ../assets/media/160.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="158" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/161_480.webp" srcset="../assets/media/thumbs/161_480.webp 480w, ../assets/media/161.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="158" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/162_480.webp" srcset="../assets/media/thumbs/162_480.webp 480w, ../assets/media/162.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="158" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/163_480.webp" srcset="../assets/media/thumbs/163_480.webp 480w, ../assets/media/163.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="158" data-image-index="5" /></div></div>
      <div class="actions">
        <span>2 129 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/158" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/158.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="157" data-search="теперь вы знаете, кому сказать спасибо за прямые включения с kdd 2025 в начале августа в торонто прошла kdd 2025 — конференция о поиске знаний и анализе данных: — обзор свежих публикаций читайте в рекомендательной. — главные цифры конференции и лучшие работы ищите в душном nlp. запомнил и записал для вас всё самое интересное один из наших коллег — инженер сергей мить. именно его портрет рисует роборука на видео выше. привёз вам новости ❣ сергей мить ml underhood теперь вы знаете, кому сказать спасибо за прямые включения с kdd 2025 в начале августа в торонто прошла kdd 2025 — конференция о поиске знаний и анализе данных: — обзор свежих публикаций читайте в рекомендательной . — главные цифры конференции и лучшие работы ищите в душном nlp . запомнил и записал для вас всё самое интересное один из наших коллег — инженер сергей мить. именно его портрет рисует роборука на видео выше. привёз вам новости ❣ сергей мить ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-14T13:22:01+00:00" href="./posts/157.html">2025-08-14 13:22 UTC</a></div>
      </div>
      <div class="post-body"><strong>Теперь вы знаете, кому сказать спасибо за прямые включения с KDD 2025</strong><br><br>В начале августа в Торонто прошла KDD 2025 — конференция о поиске знаний и анализе данных: <br><br>— Обзор свежих публикаций читайте в <a href="https://t.me/RecSysChannel/140?single" rel="nofollow noopener noreferrer">Рекомендательной</a>.<br>— Главные цифры конференции и лучшие работы ищите в <a href="https://t.me/stuffyNLP/184?single" rel="nofollow noopener noreferrer">Душном NLP</a>. <br><br>Запомнил и записал для вас всё самое интересное один из наших коллег — инженер Сергей Мить. Именно его портрет рисует роборука на видео выше.<br><br><em>Привёз вам новости  </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Сергей Мить</em><br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>1 793 просмотров · 10 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/157" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/157.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="154" data-search="возвращаемся на icml 2025 собрали ещё несколько интересных работ вдогонку прошедшей конференции. сегодня — об оптимизации больших моделей, новых sgd-подходах и работе на тему conformal prediction. the surprising agreement between convex optimization theory and learning-rate scheduling for large model training крайне любопытная теоретическая статья, показывающая (без доказательства — прямо на постере: «we don&#x27;t know why»), что график лоссов llm-претрейна с cosine и wsd lr-шедулерами выглядит практически так же, как график теоретических верхних оценок расстояния финальной точки оптимизации от точки оптимума для выпуклых негладких задач при использовании sgd с cosine/wsd-шедулерами. что ещё любопытнее — теоретические оценки выводятся для sgd, однако графики llm-претрейна авторы запускали с adam — использование адаптивных оптимизаторов ведёт к таким же результатам. авторы также утверждают, что шедулер wsd удобнее для экспериментов со scaling law. general framework for online-to-nonconvex conversion: schedule-free sgd is also effective for nonconvex optimization статья берёт новый алгоритм оптимизации schedule-free из работы the road less scheduled, демонстрировавшей лучшие на момент публикации результаты в бенчмарке методов оптимизации, и разрабатывает инструменты для теоретического анализа в невыпуклых случаях. идея schedule-free в том, что большинство шедулеров на самом деле похожи на усреднение итераций — алгоритм выглядит схожим образом, как хитрое усреднение параметров итераций. достоинство подхода — алгоритму не нужно наперёд знать число итераций t для задания расписания шедулера. текущая статья — теоретическая работа, показывающая, как алгоритм ведёт себя в невыпуклых сценариях. значительное количество статей по методам оптимизации доказываются для задачи онлайн-обучения в терминах regret — в данной статье авторы также решили воспользоваться достоинствами regret-формулировок для анализа. decision theoretic foundations for conformal prediction: optimal uncertainty quantification for risk-averse agents на конференции было как минимум 10 (а скорее — больше) статей на тему conformal prediction — её сейчас активно исследуют. conformal prediction — когда модель предсказывает не один label, а множество, и нужно предсказать такое множество, которому с заданной уверенностью принадлежит правильный ответ. мы пообщались с автором этой работы — он подробнее рассказал о подходе, математических идеях, а также о том, как эти идеи соотносятся с реальными задачами. выглядит как хорошая точка входа, чтобы разобраться в теме. интересное отобрал ❣ алексей морозов ml underhood #yaicml25 возвращаемся на icml 2025 собрали ещё несколько интересных работ вдогонку прошедшей конференции. сегодня — об оптимизации больших моделей, новых sgd-подходах и работе на тему conformal prediction. the surprising agreement between convex optimization theory and learning-rate scheduling for large model training крайне любопытная теоретическая статья, показывающая (без доказательства — прямо на постере: «we don&amp;#x27;t know why»), что график лоссов llm-претрейна с cosine и wsd lr-шедулерами выглядит практически так же, как график теоретических верхних оценок расстояния финальной точки оптимизации от точки оптимума для выпуклых негладких задач при использовании sgd с cosine/wsd-шедулерами. что ещё любопытнее — теоретические оценки выводятся для sgd, однако графики llm-претрейна авторы запускали с adam — использование адаптивных оптимизаторов ведёт к таким же результатам. авторы также утверждают, что шедулер wsd удобнее для экспериментов со scaling law. general framework for online-to-nonconvex conversion: schedule-free sgd is also effective for nonconvex optimization статья берёт новый алгоритм оптимизации schedule-free из работы the road less scheduled , демонстрировавшей лучшие на момент публикации результаты в бенчмарке методов оптимизации, и разрабатывает инструменты для теоретического анализа в невыпуклых случаях. идея schedule-free в том, что большинство шедулеров на самом деле похожи на усреднение итераций — алгоритм выглядит схожим образом, как хитрое усреднение параметров итераций. достоинство подхода — алгоритму не нужно наперёд знать число итераций t для задания расписания шедулера. текущая статья — теоретическая работа, показывающая, как алгоритм ведёт себя в невыпуклых сценариях. значительное количество статей по методам оптимизации доказываются для задачи онлайн-обучения в терминах regret — в данной статье авторы также решили воспользоваться достоинствами regret-формулировок для анализа. decision theoretic foundations for conformal prediction: optimal uncertainty quantification for risk-averse agents на конференции было как минимум 10 (а скорее — больше) статей на тему conformal prediction — её сейчас активно исследуют. conformal prediction — когда модель предсказывает не один label, а множество, и нужно предсказать такое множество, которому с заданной уверенностью принадлежит правильный ответ. мы пообщались с автором этой работы — он подробнее рассказал о подходе, математических идеях, а также о том, как эти идеи соотносятся с реальными задачами. выглядит как хорошая точка входа, чтобы разобраться в теме. интересное отобрал ❣ алексей морозов ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-11T13:16:10+00:00" href="./posts/154.html">2025-08-11 13:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Возвращаемся на ICML 2025</strong><br><br>Собрали ещё несколько интересных работ вдогонку прошедшей конференции. Сегодня — об оптимизации больших моделей, новых SGD-подходах и работе на тему conformal prediction. <br><br><a href="https://arxiv.org/abs/2501.18965" rel="nofollow noopener noreferrer"><strong>The Surprising Agreement Between Convex Optimization Theory and Learning-Rate Scheduling for Large Model Training</strong></a><br><br>Крайне любопытная теоретическая статья, показывающая (без доказательства — прямо на постере: «we don&#x27;t know why»), что график лоссов LLM-претрейна с cosine и WSD lr-шедулерами выглядит практически так же, как график теоретических верхних оценок расстояния финальной точки оптимизации от точки оптимума для выпуклых негладких задач при использовании SGD с cosine/WSD-шедулерами. Что ещё любопытнее — теоретические оценки выводятся для SGD, однако графики LLM-претрейна авторы запускали с Adam — использование адаптивных оптимизаторов ведёт к таким же результатам. Авторы также утверждают, что шедулер WSD удобнее для экспериментов со scaling law.<br><br><a href="https://arxiv.org/abs/2411.07061" rel="nofollow noopener noreferrer"><strong>General framework for online-to-nonconvex conversion: Schedule-free SGD is also effective for nonconvex optimization</strong></a><br><br>Статья берёт новый алгоритм оптимизации Schedule-Free из работы <a href="https://arxiv.org/abs/2405.15682" rel="nofollow noopener noreferrer">The Road Less Scheduled</a>, демонстрировавшей лучшие на момент публикации <a href="https://github.com/mlcommons/algorithmic-efficiency" rel="nofollow noopener noreferrer">результаты</a> в бенчмарке методов оптимизации, и разрабатывает инструменты для теоретического анализа в невыпуклых случаях. Идея Schedule-Free в том, что большинство шедулеров на самом деле похожи на усреднение итераций — алгоритм выглядит схожим образом, как хитрое усреднение параметров итераций. Достоинство подхода — алгоритму не нужно наперёд знать число итераций T для задания расписания шедулера. Текущая статья — теоретическая работа, показывающая, как алгоритм ведёт себя в невыпуклых сценариях. Значительное количество статей по методам оптимизации доказываются для задачи онлайн-обучения в терминах regret — в данной статье авторы также решили воспользоваться достоинствами regret-формулировок для анализа.<br><br><a href="https://arxiv.org/abs/2502.02561" rel="nofollow noopener noreferrer"><strong>Decision Theoretic Foundations for Conformal Prediction: Optimal Uncertainty Quantification for Risk-Averse Agents</strong></a><br><br>На конференции было как минимум 10 (а скорее — больше) статей на тему conformal prediction — её сейчас активно исследуют. Conformal prediction — когда модель предсказывает не один label, а множество, и нужно предсказать такое множество, которому с заданной уверенностью принадлежит правильный ответ. Мы пообщались с автором этой работы — он подробнее рассказал о подходе, математических идеях, а также о том, как эти идеи соотносятся с реальными задачами. Выглядит как хорошая точка входа, чтобы разобраться в теме.<br><br><em>Интересное отобрал </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Морозов</em><br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/154_480.webp" srcset="../assets/media/thumbs/154_480.webp 480w, ../assets/media/154.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="154" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/155_480.webp" srcset="../assets/media/thumbs/155_480.webp 480w, ../assets/media/155.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="154" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/156_480.webp" srcset="../assets/media/thumbs/156_480.webp 480w, ../assets/media/156.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="154" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 554 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/154" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/154.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="153" data-search="как в яндекс погоде использовали фичу для таргета, чтобы улучшить точность предсказания осадков яндекс запустил новые модели краткосрочного прогноза осадков — подробнее о них можно почитать на хабре. а специально для нашего канала ml-разработчик в яндекс погоде дмитрий стефеев рассказал о важной идее, которая позволила обойти возникшие в начале работы трудности. цель — создать модель регрессии для прогноза выпавших за час осадков (в мм). основными фичами являются прогнозы глобальных численных моделей (nwp). главная проблема заключалась в том, что радарные данные, которые мы используем в качестве таргета для обучения, значительно отличаются от прогноза численных моделей на входе. всё из-за сложности прогноза осадков. очень легко промазать как по времени, так и пространству. все функции потерь и их комбинации, которые мы пробовали, приводили к тому, что модель прогнозировала значительно меньше осадков, чем нужно, боясь промазать. можно было бы попробовать перейти от регрессии к классификации и подбирать пороги, но их, скорее всего, пришлось бы подбирать для каждого шага прогноза (как делали, к примеру, в metnet от google) и, вероятно, они бы отличались для разных мест на земле и разных сезонов. это значительно усложнило бы поддержку модели и увеличило вероятность непредсказуемого поведения. тогда нам пришла идея использовать прогноз осадков одной из численных моделей в фичах в качестве второго таргета, помимо радаров. получается, что модель начала частично обучаться к собственной фиче! сначала мы пробовали выставлять фиксированные веса лоссов к радарному таргету и прогнозу численной модели. стало значительно лучше, но местами проблема сохранялась. тогда мы сделали предположение, что веса должны зависеть от согласованности прогноза численных моделей на входе отдельно в каждой точке прогноза: чем более согласованный прогноз дают численные модели, тем меньший вес мы используем для радарного лосса и больший — для лосса численной модели. в ходе экспериментов мы пришли к простой формуле для весов. сначала считаем медиану для прогнозов осадков всех численных моделей в каждой точке прогноза. далее считаем среднюю абсолютную разность прогнозов численных моделей и медианы, а затем делим полученное значение на медиану, клипая результат по подобранным границам. полученное значение w мы применяем для лосса к радарам, а для лосса к прогнозу численной модели используем вес 1 - w. и такой трюк позволяет полностью устранить проблему с уменьшением объёма осадков — по сути это можно считать регуляризацией выхода модели к численному прогнозу, так как мы привязываем прогноз к физической модели, не давая ему отойти слишком далеко. далее мы добавили домножение полученных весов на дополнительные веса, которые линейно зависят от шага прогноза, чтобы по мере увеличения шага снижать вес лосса к радарам и увеличивать к численному прогнозу до подобранного граничного значения. на изображении выше в первой строке показан пример радарного таргета по первым 12 часам прогноза. ниже — прогноз нашей модели и прогноз численной модели в дополнительном таргете. для удобства отображения значения мм приведены к трём классам осадков. видно, что на первых шагах, прогноз модели больше похож на радары, а далее становится ближе к прогнозу численной модели, что позволяет улучшать качество прогноза на первых шагах за счёт исторических радарных данных, если они доступны на входе модели. ml underhood как в яндекс погоде использовали фичу для таргета, чтобы улучшить точность предсказания осадков яндекс запустил новые модели краткосрочного прогноза осадков — подробнее о них можно почитать на хабре . а специально для нашего канала ml-разработчик в яндекс погоде дмитрий стефеев рассказал о важной идее, которая позволила обойти возникшие в начале работы трудности. цель — создать модель регрессии для прогноза выпавших за час осадков (в мм). основными фичами являются прогнозы глобальных численных моделей (nwp). главная проблема заключалась в том, что радарные данные, которые мы используем в качестве таргета для обучения, значительно отличаются от прогноза численных моделей на входе. всё из-за сложности прогноза осадков. очень легко промазать как по времени, так и пространству. все функции потерь и их комбинации, которые мы пробовали, приводили к тому, что модель прогнозировала значительно меньше осадков, чем нужно, боясь промазать. можно было бы попробовать перейти от регрессии к классификации и подбирать пороги, но их, скорее всего, пришлось бы подбирать для каждого шага прогноза (как делали, к примеру, в metnet от google) и, вероятно, они бы отличались для разных мест на земле и разных сезонов. это значительно усложнило бы поддержку модели и увеличило вероятность непредсказуемого поведения. тогда нам пришла идея использовать прогноз осадков одной из численных моделей в фичах в качестве второго таргета, помимо радаров. получается, что модель начала частично обучаться к собственной фиче! сначала мы пробовали выставлять фиксированные веса лоссов к радарному таргету и прогнозу численной модели. стало значительно лучше, но местами проблема сохранялась. тогда мы сделали предположение, что веса должны зависеть от согласованности прогноза численных моделей на входе отдельно в каждой точке прогноза: чем более согласованный прогноз дают численные модели, тем меньший вес мы используем для радарного лосса и больший — для лосса численной модели. в ходе экспериментов мы пришли к простой формуле для весов. сначала считаем медиану для прогнозов осадков всех численных моделей в каждой точке прогноза. далее считаем среднюю абсолютную разность прогнозов численных моделей и медианы, а затем делим полученное значение на медиану, клипая результат по подобранным границам. полученное значение w мы применяем для лосса к радарам, а для лосса к прогнозу численной модели используем вес 1 - w. и такой трюк позволяет полностью устранить проблему с уменьшением объёма осадков — по сути это можно считать регуляризацией выхода модели к численному прогнозу, так как мы привязываем прогноз к физической модели, не давая ему отойти слишком далеко. далее мы добавили домножение полученных весов на дополнительные веса, которые линейно зависят от шага прогноза, чтобы по мере увеличения шага снижать вес лосса к радарам и увеличивать к численному прогнозу до подобранного граничного значения. на изображении выше в первой строке показан пример радарного таргета по первым 12 часам прогноза. ниже — прогноз нашей модели и прогноз численной модели в дополнительном таргете. для удобства отображения значения мм приведены к трём классам осадков. видно, что на первых шагах, прогноз модели больше похож на радары, а далее становится ближе к прогнозу численной модели, что позволяет улучшать качество прогноза на первых шагах за счёт исторических радарных данных, если они доступны на входе модели. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-08-06T14:08:21+00:00" href="./posts/153.html">2025-08-06 14:08 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как в Яндекс Погоде использовали фичу для таргета, чтобы улучшить точность предсказания осадков</strong><br><br>Яндекс запустил новые модели краткосрочного прогноза осадков — подробнее о них можно почитать <a href="https://habr.com/ru/companies/yandex/articles/929586/" rel="nofollow noopener noreferrer">на Хабре</a>. А специально для нашего канала ML-разработчик в Яндекс Погоде Дмитрий Стефеев рассказал о важной идее, которая позволила обойти возникшие в начале работы трудности. <br><br>Цель — создать модель регрессии для прогноза выпавших за час осадков (в мм). Основными фичами являются прогнозы <a href="https://ru.wikipedia.org/wiki/Численный_прогноз_погоды" rel="nofollow noopener noreferrer">глобальных численных моделей</a> (NWP). <br><br>Главная проблема заключалась в том, что радарные данные, которые мы используем в качестве таргета для обучения, значительно отличаются от прогноза численных моделей на входе. Всё из-за сложности прогноза осадков. Очень легко промазать как по времени, так и пространству. Все функции потерь и их комбинации, которые мы пробовали, приводили к тому, что модель прогнозировала значительно меньше осадков, чем нужно, боясь промазать. Можно было бы попробовать перейти от регрессии к классификации и подбирать пороги, но их, скорее всего, пришлось бы подбирать для каждого шага прогноза (как делали, к примеру, в <a href="https://research.google/blog/metnet-3-a-state-of-the-art-neural-weather-model-available-in-google-products/" rel="nofollow noopener noreferrer">Metnet</a> от Google) и, вероятно, они бы отличались для разных мест на Земле и разных сезонов. Это значительно усложнило бы поддержку модели и увеличило вероятность непредсказуемого поведения.<br><br>Тогда нам пришла идея использовать прогноз осадков одной из численных моделей в фичах в качестве второго таргета, помимо радаров. Получается, что модель начала частично обучаться к собственной фиче! Сначала мы пробовали выставлять фиксированные веса лоссов к радарному таргету и прогнозу численной модели. Стало значительно лучше, но местами проблема сохранялась. Тогда мы сделали предположение, что веса должны зависеть от согласованности прогноза численных моделей на входе отдельно в каждой точке прогноза: чем более согласованный прогноз дают численные модели, тем меньший вес мы используем для радарного лосса и больший — для лосса численной модели. <br><br>В ходе экспериментов мы пришли к простой формуле для весов. Сначала считаем медиану для прогнозов осадков всех численных моделей в каждой точке прогноза. Далее считаем среднюю абсолютную разность прогнозов численных моделей и медианы, а затем делим полученное значение на медиану, клипая результат по подобранным границам. Полученное значение W мы применяем для лосса к радарам, а для лосса к прогнозу численной модели используем вес 1 - W. И такой трюк позволяет полностью устранить проблему с уменьшением объёма осадков — по сути это можно считать регуляризацией выхода модели к численному прогнозу, так как мы привязываем прогноз к физической модели, не давая ему отойти слишком далеко. <br><br>Далее мы добавили домножение полученных весов на дополнительные веса, которые линейно зависят от шага прогноза, чтобы по мере увеличения шага снижать вес лосса к радарам и увеличивать к численному прогнозу до подобранного граничного значения. <br><br>На изображении выше в первой строке показан пример радарного таргета по первым 12 часам прогноза. Ниже — прогноз нашей модели и прогноз численной модели в дополнительном таргете. Для удобства отображения значения мм приведены к трём классам осадков. Видно, что на первых шагах, прогноз модели больше похож на радары, а далее становится ближе к прогнозу численной модели, что позволяет улучшать качество прогноза на первых шагах за счёт исторических радарных данных, если они доступны на входе модели.<br><br><a href="https://t.me/+hD03zlO0IS83YjYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/153_480.webp" srcset="../assets/media/thumbs/153_480.webp 480w, ../assets/media/153.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="153" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 020 просмотров · 15 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/153" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/153.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="146" data-search="как проходит acl 2025 👀 продолжаем рассказывать, что увидели и услышали на конференции: листайте фото и видео! в этом году acl состоялась в austria center vienna — конференц-зале в центре вены. красиво не только внутри, но и снаружи. иногда на докладах людно, иногда — не очень. поразило невероятное количество постеров: около 250 только в одном зале. работы очень разные, от «денег нет, но вы держитесь» до лаконичных постеров на а4. мы выбрали для вас самые интересные из них — о трендах и статьях читайте в душном nlp: в вене проходит 63-я ежегодная конференция ассоциации компьютерной лингвистики — acl 2025 интересное с конференции acl 2025 кадры для вас сделали и отобрали ❣ алексей березникер и александр николайчик #yaacl25 ml underhood как проходит acl 2025 👀 продолжаем рассказывать, что увидели и услышали на конференции: листайте фото и видео! в этом году acl состоялась в austria center vienna — конференц-зале в центре вены. красиво не только внутри, но и снаружи. иногда на докладах людно, иногда — не очень. поразило невероятное количество постеров: около 250 только в одном зале. работы очень разные, от «денег нет, но вы держитесь» до лаконичных постеров на а4. мы выбрали для вас самые интересные из них — о трендах и статьях читайте в душном nlp: в вене проходит 63-я ежегодная конференция ассоциации компьютерной лингвистики — acl 2025 интересное с конференции acl 2025 кадры для вас сделали и отобрали ❣ алексей березникер и александр николайчик #yaacl25 ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-30T13:11:42+00:00" href="./posts/146.html">2025-07-30 13:11 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как проходит ACL 2025 👀</strong><br><br>Продолжаем рассказывать, что увидели и услышали на конференции: листайте фото и видео!<br><br>В этом году ACL состоялась в Austria Center Vienna — конференц-зале в центре Вены. Красиво не только внутри, но и снаружи. Иногда на докладах людно, иногда — не очень. <br><br>Поразило невероятное количество постеров: около 250 только в одном зале. Работы очень разные, от «денег нет, но вы держитесь» до лаконичных постеров на А4. Мы выбрали для вас самые интересные из них — о трендах и статьях читайте в Душном NLP: <br><br><a href="https://t.me/stuffyNLP/163" rel="nofollow noopener noreferrer">В Вене проходит 63-я ежегодная конференция ассоциации компьютерной лингвистики — ACL 2025</a><br><br><a href="https://t.me/stuffyNLP/168" rel="nofollow noopener noreferrer">Интересное с конференции ACL 2025</a><br><br><em>Кадры для вас сделали и отобрали </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Березникер и Александр Николайчик</em><br><br>#YaACL25<br><br><a href="https://t.me/+x5iTado_ZZVmNGIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/146_480.webp" srcset="../assets/media/thumbs/146_480.webp 480w, ../assets/media/146.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="146" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/147_480.webp" srcset="../assets/media/thumbs/147_480.webp 480w, ../assets/media/147.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="146" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/148_480.webp" srcset="../assets/media/thumbs/148_480.webp 480w, ../assets/media/148.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="146" data-image-index="2" /><video controls preload="metadata" src="../assets/media/149_ACL-4.mp4"></video><video controls preload="metadata" src="../assets/media/150_ACL-6.mp4"></video><img class="media-img" loading="lazy" src="../assets/media/thumbs/151_480.webp" srcset="../assets/media/thumbs/151_480.webp 480w, ../assets/media/151.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="146" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/152_480.webp" srcset="../assets/media/thumbs/152_480.webp 480w, ../assets/media/152.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="146" data-image-index="4" /></div></div>
      <div class="actions">
        <span>1 875 просмотров · 18 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/146" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/146.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="144" data-search="начинаем новую неделю с новой конференцией в вене стартовала acl 2025. в ближайшие дни мы будем рассказывать обо всём самом интересном, что увидим на мероприятии, а сейчас поделимся занимательной статистикой. — всего на конференцию зарегистрировались около 20 тысяч авторов. — 51% авторов — из китая, ещё 18,6% — из сша. — у 67% работ, поданных на acl, в названии есть llm. — почти так же часто, как llm, в названиях встречается двоеточие — оно есть в 65% заголовков. рассказывайте в комментариях, о чём, связанном с acl, вам интересно было бы почитать. а, может быть, вы сами на конференции? тогда обязательно делитесь впечатлениями! #yaacl25 ml underhood начинаем новую неделю с новой конференцией в вене стартовала acl 2025. в ближайшие дни мы будем рассказывать обо всём самом интересном, что увидим на мероприятии, а сейчас поделимся занимательной статистикой. — всего на конференцию зарегистрировались около 20 тысяч авторов. — 51% авторов — из китая, ещё 18,6% — из сша. — у 67% работ, поданных на acl, в названии есть llm. — почти так же часто, как llm, в названиях встречается двоеточие — оно есть в 65% заголовков. рассказывайте в комментариях, о чём, связанном с acl, вам интересно было бы почитать. а, может быть, вы сами на конференции? тогда обязательно делитесь впечатлениями! #yaacl25 ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-28T08:05:16+00:00" href="./posts/144.html">2025-07-28 08:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Начинаем новую неделю с новой конференцией</strong><br><br>В Вене стартовала ACL 2025. В ближайшие дни мы будем рассказывать обо всём самом интересном, что увидим на мероприятии, а сейчас поделимся занимательной статистикой.<br><br>— Всего на конференцию зарегистрировались около 20 тысяч авторов.<br>— 51% авторов — из Китая, ещё 18,6% — из США. <br>— У 67% работ, поданных на ACL, в названии есть LLM.<br>— Почти так же часто, как LLM, в названиях встречается двоеточие — оно есть в 65% заголовков.<br><br>Рассказывайте в комментариях, о чём, связанном с ACL, вам интересно было бы почитать. А, может быть, вы сами на конференции? Тогда обязательно делитесь впечатлениями! <br><br>#YaACL25<br><br><a href="https://t.me/+QLZJ3hfW2Oc0MTRi" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/144_480.webp" srcset="../assets/media/thumbs/144_480.webp 480w, ../assets/media/144.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="144" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/145_480.webp" srcset="../assets/media/thumbs/145_480.webp 480w, ../assets/media/145.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="144" data-image-index="1" /></div></div>
      <div class="actions">
        <span>2 076 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/144" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/144.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="143" data-search="aqua-kv: адаптивная квантизация kv-кэша на icml 2025 команда yandex research представила шесть статей (каких именно — читайте в одном из предыдущих постов) — среди них есть работа, посвящённая методу адаптивной квантизации kv-кэша. один из авторов, исследователь yandex research алина шутова, рассказала нашему каналу, в чём суть предложенного в публикации способа. одна из ключевых проблем эксплуатации llm — экспоненциальный рост потребления памяти графических процессоров при обработке длинных контекстов. это связано с необходимостью хранения kv-кэша. для современных моделей, таких как llama 3.2 70b, и контекстов в 131 тысячу токенов, объём kv-кэша может достигать 42,9 гб на последовательность, что существенно ограничивает практическое применение и увеличивает стоимость вычислений. традиционные методы сжатия, такие как примитивное квантование или прунинг, демонстрируют значительную деградацию качества генерации при агрессивных режимах сжатия, особенно в области 2–3 бит на значение. предложенный авторами статьи метод aqua-kv (adaptive quantization for key-value) представляет принципиально новый подход, основанный на фундаментальном наблюдении: векторы ключей и значений в соседних слоях трансформера обладают высокой степенью корреляции. эта структурная избыточность позволяет прогнозировать значительную часть информации слоя k+1 на основе данных слоя k. вместо независимого квантования каждого слоя aqua-kv использует обученные линейные предикторы. один предиктор предсказывает ключи слоя k+1 на основе ключей слоя k, другой предсказывает значения слоя k+1 по комбинации предсказанных ключей этого слоя и значений слоя k. обучение этих компактных адаптеров проводится в ходе одноразовой калибровки на целевой модели. критический шаг метода — переход от квантования векторов целиком к квантованию только остаточной информации, то есть разности между фактическими векторами слоя и их предсказаниями. поскольку остаток содержит лишь ту информацию, которую нельзя получить из предыдущего слоя, его информационная энтропия существенно ниже. эта остаточная компонента подвергается экстремальному квантованию (до 2–2,5 бит на элемент) с применением векторного квантования без данных (data-free vq), адаптивно оптимизирующего распределение битов под дисперсию остатков. для восстановления kv-векторов во время инференса используются те же предикторы и деквантованный остаток. эксперименты демонстрируют эффективность aqua-kv. на моделях семейств llama 3.2 и qwen 2.5 применение метода с квантованием до 2 бит на значение привело к снижению объёма памяти kv-кэша в 16 раз (с ~43 гб до ~2,7 гб для контекста в 131k токенов) при сохранении практически исходного качества генерации. относительное увеличение перплексии составило менее 1%, а деградация точности на задачах длинного контекста из бенчмарка longbench не превысила 1%. aqua-kv совместим с любыми методами квантизации, и, как продемонстрировано в работе, заметно улучшает качество всех рассмотренных методов. метод демонстрирует совместимость с техниками прунинга, такими как h2o, обеспечивая дополнительную экономию памяти. код aqua-kv можно найти на github. ml underhood #yaicml25 aqua-kv: адаптивная квантизация kv-кэша на icml 2025 команда yandex research представила шесть статей (каких именно — читайте в одном из предыдущих постов ) — среди них есть работа, посвящённая методу адаптивной квантизации kv-кэша . один из авторов, исследователь yandex research алина шутова, рассказала нашему каналу, в чём суть предложенного в публикации способа. одна из ключевых проблем эксплуатации llm — экспоненциальный рост потребления памяти графических процессоров при обработке длинных контекстов. это связано с необходимостью хранения kv-кэша. для современных моделей, таких как llama 3.2 70b, и контекстов в 131 тысячу токенов, объём kv-кэша может достигать 42,9 гб на последовательность, что существенно ограничивает практическое применение и увеличивает стоимость вычислений. традиционные методы сжатия, такие как примитивное квантование или прунинг, демонстрируют значительную деградацию качества генерации при агрессивных режимах сжатия, особенно в области 2–3 бит на значение. предложенный авторами статьи метод aqua-kv (adaptive quantization for key-value) представляет принципиально новый подход, основанный на фундаментальном наблюдении: векторы ключей и значений в соседних слоях трансформера обладают высокой степенью корреляции. эта структурная избыточность позволяет прогнозировать значительную часть информации слоя k+1 на основе данных слоя k. вместо независимого квантования каждого слоя aqua-kv использует обученные линейные предикторы. один предиктор предсказывает ключи слоя k+1 на основе ключей слоя k, другой предсказывает значения слоя k+1 по комбинации предсказанных ключей этого слоя и значений слоя k. обучение этих компактных адаптеров проводится в ходе одноразовой калибровки на целевой модели. критический шаг метода — переход от квантования векторов целиком к квантованию только остаточной информации, то есть разности между фактическими векторами слоя и их предсказаниями. поскольку остаток содержит лишь ту информацию, которую нельзя получить из предыдущего слоя, его информационная энтропия существенно ниже. эта остаточная компонента подвергается экстремальному квантованию (до 2–2,5 бит на элемент) с применением векторного квантования без данных (data-free vq), адаптивно оптимизирующего распределение битов под дисперсию остатков. для восстановления kv-векторов во время инференса используются те же предикторы и деквантованный остаток. эксперименты демонстрируют эффективность aqua-kv. на моделях семейств llama 3.2 и qwen 2.5 применение метода с квантованием до 2 бит на значение привело к снижению объёма памяти kv-кэша в 16 раз (с ~43 гб до ~2,7 гб для контекста в 131k токенов) при сохранении практически исходного качества генерации. относительное увеличение перплексии составило менее 1%, а деградация точности на задачах длинного контекста из бенчмарка longbench не превысила 1%. aqua-kv совместим с любыми методами квантизации, и, как продемонстрировано в работе, заметно улучшает качество всех рассмотренных методов. метод демонстрирует совместимость с техниками прунинга, такими как h2o, обеспечивая дополнительную экономию памяти. код aqua-kv можно найти на github . ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-23T11:10:00+00:00" href="./posts/143.html">2025-07-23 11:10 UTC</a></div>
      </div>
      <div class="post-body"><strong>AQUA-KV: адаптивная квантизация KV-кэша</strong><br><br>На ICML 2025 команда Yandex Research представила шесть статей (каких именно — читайте <a href="https://t.me/MLunderhood/128" rel="nofollow noopener noreferrer">в одном из предыдущих постов</a>) — среди них есть работа, посвящённая <a href="https://arxiv.org/abs/2501.19392" rel="nofollow noopener noreferrer">методу адаптивной квантизации KV-кэша</a>. Один из авторов, исследователь Yandex Research Алина Шутова, рассказала нашему каналу, в чём суть предложенного в публикации способа. <br><br>Одна из ключевых проблем эксплуатации LLM — экспоненциальный рост потребления памяти графических процессоров при обработке длинных контекстов. Это связано с необходимостью хранения KV-кэша. Для современных моделей, таких как Llama 3.2 70B, и контекстов в 131 тысячу токенов, объём KV-кэша может достигать 42,9 ГБ на последовательность, что существенно ограничивает практическое применение и увеличивает стоимость вычислений. Традиционные методы сжатия, такие как примитивное квантование или прунинг, демонстрируют значительную деградацию качества генерации при агрессивных режимах сжатия, особенно в области 2–3 бит на значение.<br><br>Предложенный авторами статьи метод AQUA-KV (Adaptive QUAntization for Key-Value) представляет принципиально новый подход, основанный на фундаментальном наблюдении: векторы ключей и значений в соседних слоях трансформера обладают высокой степенью корреляции. Эта структурная избыточность позволяет прогнозировать значительную часть информации слоя k+1 на основе данных слоя k. <br><br>Вместо независимого квантования каждого слоя AQUA-KV использует обученные линейные предикторы. Один предиктор предсказывает ключи слоя k+1 на основе ключей слоя k, другой предсказывает значения слоя k+1 по комбинации предсказанных ключей этого слоя и значений слоя k. Обучение этих компактных адаптеров проводится в ходе одноразовой калибровки на целевой модели.<br><br>Критический шаг метода — переход от квантования векторов целиком к квантованию только остаточной информации, то есть разности между фактическими векторами слоя и их предсказаниями. Поскольку остаток содержит лишь ту информацию, которую нельзя получить из предыдущего слоя, его информационная энтропия существенно ниже. Эта остаточная компонента подвергается экстремальному квантованию (до 2–2,5 бит на элемент) с применением векторного квантования без данных (data-free VQ), адаптивно оптимизирующего распределение битов под дисперсию остатков. Для восстановления KV-векторов во время инференса используются те же предикторы и деквантованный остаток.<br><br>Эксперименты демонстрируют эффективность AQUA-KV. На моделях семейств Llama 3.2 и Qwen 2.5 применение метода с квантованием до 2 бит на значение привело к снижению объёма памяти KV-кэша в 16 раз (с ~43 ГБ до ~2,7 ГБ для контекста в 131K токенов) при сохранении практически исходного качества генерации. Относительное увеличение перплексии составило менее 1%, а деградация точности на задачах длинного контекста из бенчмарка LongBench не превысила 1%. AQUA-KV совместим с любыми методами квантизации, и, как продемонстрировано в работе, заметно улучшает качество всех рассмотренных методов. Метод демонстрирует совместимость с техниками прунинга, такими как H2O, обеспечивая дополнительную экономию памяти. Код AQUA-KV можно найти на <a href="https://github.com/goodevening13/aquakv" rel="nofollow noopener noreferrer">GitHub</a>.<br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/143_480.webp" srcset="../assets/media/thumbs/143_480.webp 480w, ../assets/media/143.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="143" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 182 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/143" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/143.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="138" data-search="пятничное: немного атмосферы icml 2025 — большие очереди на регистрацию и прекрасные виды снаружи vancouver convention centre. — арт-галерея с визуализацией кусочно-линейных нейросетей на одном из стендов. — пасека на крыше здания, в котором проходит конференция. — аутентичный корейский исследователь представляет свой постер. ml underhood #yaicml25 пятничное: немного атмосферы icml 2025 — большие очереди на регистрацию и прекрасные виды снаружи vancouver convention centre. — арт-галерея с визуализацией кусочно-линейных нейросетей на одном из стендов. — пасека на крыше здания, в котором проходит конференция. — аутентичный корейский исследователь представляет свой постер. ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-18T14:02:16+00:00" href="./posts/138.html">2025-07-18 14:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Пятничное: немного атмосферы ICML 2025<br></strong><br>— Большие очереди на регистрацию и прекрасные виды снаружи Vancouver Convention Centre.<br><br>— Арт-галерея с визуализацией кусочно-линейных нейросетей на одном из стендов. <br><br>— Пасека на крыше здания, в котором проходит конференция.<br><br>— Аутентичный корейский исследователь представляет свой постер.<br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25<div class="media"><video controls preload="metadata" src="../assets/media/138_2025-07-18_13.56.42.mp4"></video><img class="media-img" loading="lazy" src="../assets/media/thumbs/139_480.webp" srcset="../assets/media/thumbs/139_480.webp 480w, ../assets/media/139.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/140_480.webp" srcset="../assets/media/thumbs/140_480.webp 480w, ../assets/media/140.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/141_480.webp" srcset="../assets/media/thumbs/141_480.webp 480w, ../assets/media/141.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/142_480.webp" srcset="../assets/media/thumbs/142_480.webp 480w, ../assets/media/142.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="138" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 784 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/138" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/138.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="135" data-search="новая порция докладов с icml 2025 конференция в разгаре, а инженеры из яндекса продолжают отмечать и комментировать любопытные работы. делимся ими с вами. ai&#x27;s models of the world, and ours invited talk от джона кляйнберга — об отличиях в представлениях о мире у моделей и у человека. мотивация примерно такая: после наступления сингулярности человеческий труд во многих областях станет не нужен, и это как минимум обидно, если не сказать страшно. хочется, чтобы люди продолжали что-то делать. чтобы представить эту ситуацию, можно обратиться к задаче, где она уже произошла: к игре в шахматы. несмотря на то, что компьютеры давно играют в шахматы лучше людей, интерес к игре только вырос — в первую очередь, благодаря интернету. люди играют в шахматы больше, чем когда-либо. если сравнить шахматные партии с участием компьютера и партии между людьми, видно, что во вторых намного больше красивых комбинаций — эстетики, которая играла важную роль в шахматном образовании прошлого. проще говоря, в человеческих партиях есть то, что принято называть «красивыми идеями», благодаря которым эта игра и получила такое распространение во всем мире. кляйнберг рассказал о проблеме обучения моделей, играющих как человек с рейтингом, например, 1100, 1200, 1600, 1800 или 2300, в надежде воспроизвести красивые человеческие партии. это оказалось сложней, чем можно было ожидать. с человеческой точки зрения ходы моделей, которые пытаются имитировать игру человека, всё ещё выглядят неестественно, и лучшие попытки дают accuracy около 60%. но результат оказался востребован — «с компьютером намного интереснее играть, когда он проигрывает» (с). во второй части выступления кляйнберг упомянул старый результат: задача распознавания языка из счётно бесконечного множества неразрешима за конечное время, зато задача генерации предложений из неизвестного языка — решается. но решить её можно тривиально: выбрать и генерировать удлиняемую простую конструкцию из языка — неинтересно. результат группы кляйнберга этого года — возможность делать это с константной плотностью, то есть так, чтобы выход модели покрывал ⅛ языка. с другой стороны, несложно доказать, что больше половины неизвестного произвольного языка сгенерировать теоретически невозможно. generative ai&#x27;s collision with copyright law доклад о том, как использовать защищённые авторским правом данные для обучения моделей. ключевой вывод — ситуация сильно зависит от страны: — в израиле любое использование данных для обучения признаётся fair use. — в ес данные можно использовать, если к ним есть легальный доступ; при этом его нельзя ограничивать для образовательных и культурных учреждений. у авторов есть право исключать свои произведения из датасетов, используемых в обучении. в японии и сингапуре ситуация в целом такая же. — в сша всё сложнее из-за прецедентного права, многое решается индивидуально в суде. авторы (в отличие от правообладателей) не могут запретить использование своих работ для обучения. а если использование данных может повлиять на рынок правообладателя, скорее всего, это считается нарушением. riemannian diffusion adaptation for distributed optimization on manifolds отдельный лайк авторам за задачу оптимизации в римановых многообразиях. сюда входит задача глубокого обучения с ортогональными матрицами, а это то, что помогало стабилизировать асинхронное глубокое обучение в течение продолжительного времени. на древнем рекламном фреймворке глубокого обучения такие модели — с всегда ортогональными слоями — обучались стабильнее и показывали лучшее качество (при переходе на allreduce, к сожалению, ортогональные матрицы стали вести себя так же, как обычные, но медленнее). авторы приписывают к достоинствам метода решение задач на любых многообразиях, но при этом не сравнивают себя со специализированными методами для разных задач. вкладка экспериментов — скромная для метода, решающего любые задачи: в abstract — четыре примера, в экспериментах — всего два, и нет сравнения со специализированными под каждую задачу методами. работы заметили ❣ алексей поспелов и алексей морозов ml underhood #yaicml25 новая порция докладов с icml 2025 конференция в разгаре, а инженеры из яндекса продолжают отмечать и комментировать любопытные работы. делимся ими с вами. ai&amp;#x27;s models of the world, and ours invited talk от джона кляйнберга — об отличиях в представлениях о мире у моделей и у человека. мотивация примерно такая: после наступления сингулярности человеческий труд во многих областях станет не нужен, и это как минимум обидно, если не сказать страшно. хочется, чтобы люди продолжали что-то делать. чтобы представить эту ситуацию, можно обратиться к задаче, где она уже произошла: к игре в шахматы. несмотря на то, что компьютеры давно играют в шахматы лучше людей, интерес к игре только вырос — в первую очередь, благодаря интернету. люди играют в шахматы больше, чем когда-либо. если сравнить шахматные партии с участием компьютера и партии между людьми, видно, что во вторых намного больше красивых комбинаций — эстетики, которая играла важную роль в шахматном образовании прошлого. проще говоря, в человеческих партиях есть то, что принято называть «красивыми идеями», благодаря которым эта игра и получила такое распространение во всем мире. кляйнберг рассказал о проблеме обучения моделей, играющих как человек с рейтингом, например, 1100, 1200, 1600, 1800 или 2300, в надежде воспроизвести красивые человеческие партии. это оказалось сложней, чем можно было ожидать. с человеческой точки зрения ходы моделей, которые пытаются имитировать игру человека, всё ещё выглядят неестественно, и лучшие попытки дают accuracy около 60%. но результат оказался востребован — «с компьютером намного интереснее играть, когда он проигрывает» (с). во второй части выступления кляйнберг упомянул старый результат: задача распознавания языка из счётно бесконечного множества неразрешима за конечное время, зато задача генерации предложений из неизвестного языка — решается. но решить её можно тривиально: выбрать и генерировать удлиняемую простую конструкцию из языка — неинтересно. результат группы кляйнберга этого года — возможность делать это с константной плотностью, то есть так, чтобы выход модели покрывал ⅛ языка. с другой стороны, несложно доказать, что больше половины неизвестного произвольного языка сгенерировать теоретически невозможно. generative ai&amp;#x27;s collision with copyright law доклад о том, как использовать защищённые авторским правом данные для обучения моделей. ключевой вывод — ситуация сильно зависит от страны: — в израиле любое использование данных для обучения признаётся fair use. — в ес данные можно использовать, если к ним есть легальный доступ; при этом его нельзя ограничивать для образовательных и культурных учреждений. у авторов есть право исключать свои произведения из датасетов, используемых в обучении. в японии и сингапуре ситуация в целом такая же. — в сша всё сложнее из-за прецедентного права, многое решается индивидуально в суде. авторы (в отличие от правообладателей) не могут запретить использование своих работ для обучения. а если использование данных может повлиять на рынок правообладателя, скорее всего, это считается нарушением. riemannian diffusion adaptation for distributed optimization on manifolds отдельный лайк авторам за задачу оптимизации в римановых многообразиях. сюда входит задача глубокого обучения с ортогональными матрицами, а это то, что помогало стабилизировать асинхронное глубокое обучение в течение продолжительного времени. на древнем рекламном фреймворке глубокого обучения такие модели — с всегда ортогональными слоями — обучались стабильнее и показывали лучшее качество (при переходе на allreduce, к сожалению, ортогональные матрицы стали вести себя так же, как обычные, но медленнее). авторы приписывают к достоинствам метода решение задач на любых многообразиях, но при этом не сравнивают себя со специализированными методами для разных задач. вкладка экспериментов — скромная для метода, решающего любые задачи: в abstract — четыре примера, в экспериментах — всего два, и нет сравнения со специализированными под каждую задачу методами. работы заметили ❣ алексей поспелов и алексей морозов ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-18T09:15:01+00:00" href="./posts/135.html">2025-07-18 09:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>Новая порция докладов с ICML 2025</strong><br><br>Конференция в разгаре, а инженеры из Яндекса продолжают отмечать и комментировать любопытные работы. Делимся ими с вами.<br><br><a href="https://icml.cc/virtual/2025/invited-talk/39862?utm_source=chatgpt.com" rel="nofollow noopener noreferrer"><strong>AI&#x27;s Models of the World, and Ours</strong></a><br><br>Invited talk от Джона Кляйнберга — об отличиях в представлениях о мире у моделей и у человека. Мотивация примерно такая: после наступления сингулярности человеческий труд во многих областях станет не нужен, и это как минимум обидно, если не сказать страшно. Хочется, чтобы люди продолжали что-то делать. Чтобы представить эту ситуацию, можно обратиться к задаче, где она уже произошла: к игре в шахматы. Несмотря на то, что компьютеры давно играют в шахматы лучше людей, интерес к игре только вырос — в первую очередь, благодаря интернету. Люди играют в шахматы больше, чем когда-либо. Если сравнить шахматные партии с участием компьютера и партии между людьми, видно, что во вторых намного больше красивых комбинаций — эстетики, которая играла важную роль в шахматном образовании прошлого. Проще говоря, в человеческих партиях есть то, что принято называть «красивыми идеями», благодаря которым эта игра и получила такое распространение во всем мире.<br><br>Кляйнберг рассказал о проблеме обучения моделей, играющих как человек с рейтингом, например, 1100, 1200, 1600, 1800 или 2300, в надежде воспроизвести красивые человеческие партии. Это оказалось сложней, чем можно было ожидать. С человеческой точки зрения ходы моделей, которые пытаются имитировать игру человека, всё ещё выглядят неестественно, и лучшие попытки дают accuracy около 60%. Но результат оказался востребован — «с компьютером намного интереснее играть, когда он проигрывает» (с).<br><br>Во второй части выступления Кляйнберг упомянул старый результат: задача распознавания языка из счётно бесконечного множества неразрешима за конечное время, зато задача генерации предложений из неизвестного языка — решается. Но решить её можно тривиально: выбрать и генерировать удлиняемую простую конструкцию из языка — неинтересно. Результат группы Кляйнберга этого года — возможность делать это с константной плотностью, то есть так, чтобы выход модели покрывал ⅛ языка. С другой стороны, несложно доказать, что больше половины неизвестного произвольного языка сгенерировать теоретически невозможно.<br><br><a href="https://icml.cc/virtual/2025/invited-talk/39865" rel="nofollow noopener noreferrer"><strong>Generative AI&#x27;s Collision with Copyright Law<br></strong></a><br>Доклад о том, как использовать защищённые авторским правом данные для обучения моделей. Ключевой вывод — ситуация сильно зависит от страны:<br><br>— В Израиле любое использование данных для обучения признаётся fair use.<br><br>— В ЕС данные можно использовать, если к ним есть легальный доступ; при этом его нельзя ограничивать для образовательных и культурных учреждений. У авторов есть право исключать свои произведения из датасетов, используемых в обучении. В Японии и Сингапуре ситуация в целом такая же.<br><br>— В США всё сложнее из-за прецедентного права, многое решается индивидуально в суде. Авторы (в отличие от правообладателей) не могут запретить использование своих работ для обучения. А если использование данных может повлиять на рынок правообладателя, скорее всего, это считается нарушением.<br><br><a href="https://icml.cc/virtual/2025/poster/46403" rel="nofollow noopener noreferrer"><strong>Riemannian Diffusion Adaptation for Distributed Optimization on Manifolds</strong></a><br><br>Отдельный лайк авторам за задачу оптимизации в римановых многообразиях. Сюда входит задача глубокого обучения с ортогональными матрицами, а это то, что помогало стабилизировать асинхронное глубокое обучение в течение продолжительного времени. На древнем рекламном фреймворке глубокого обучения такие модели — с всегда ортогональными слоями — обучались стабильнее и показывали лучшее качество (при переходе на allreduce, к сожалению, ортогональные матрицы стали вести себя так же, как обычные, но медленнее). Авторы приписывают к достоинствам метода решение задач на любых многообразиях, но при этом не сравнивают себя со специализированными методами для разных задач. Вкладка экспериментов — скромная для метода, решающего любые задачи: в abstract — четыре примера, в экспериментах — всего два, и нет сравнения со специализированными под каждую задачу методами.<br><br><em>Работы заметили </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Поспелов и Алексей Морозов<br></em><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/135_480.webp" srcset="../assets/media/thumbs/135_480.webp 480w, ../assets/media/135.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="135" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/136_480.webp" srcset="../assets/media/thumbs/136_480.webp 480w, ../assets/media/136.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="135" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/137_480.webp" srcset="../assets/media/thumbs/137_480.webp 480w, ../assets/media/137.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="135" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 363 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/135" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/135.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="132" data-search="icml 2025: интересные доклады на тему ml — часть 2 when to retrain machine learning model в работе исследуют проблему регулярного переобучения моделей в продакшн-системах: то, как часто нужно полностью обучать модель с нуля на новых данных. приходят к выводу, что переобучать слишком часто — дорого и бесполезно, попробуют понять, в какие моменты времени это лучше делать. получается временной ряд, который они аппроксимируют своими методами. решение имеет смысл, только если есть возможность переобучать модель очень часто, но хочется делать это реже — без ущерба для качества. при этом, поскольку подход ориентирован именно на полное переобучение «с нуля», он не применяется к онлайн-обучению: там всегда предпочтительнее дообучать модель настолько часто, насколько это возможно. how to set adamw’s weight decay as you scale model and dataset size новый метод для подбора гиперпараметра регуляризации в adamw. авторы переписали формулы weight decay в виде, который начинает походить на экспоненциальное сглаживание (ewma). репараметризуют его новыми параметрами и говорят, что подбор одного нового параметра работает проще и сохраняет свойства при изменении размеров датасета, размера батча или размера архитектуры. то есть можно один раз подобрать и какое-то время о нём не вспоминать. формула очень простая и её будет легко попробовать в боевых моделях. efficient optimization with orthogonality constraint: a randomized riemannian submanifold method ещё одна статья на тему оптимизации на римановых многообразиях для ортогональных матриц. из интересного — оказывается, условия ортогональности используются сейчас не только в классических задачах вроде pca, но и в некоторых задачах файнтюна. к сожалению, автор не читал статью orthogonal weight normalization, где в 2017 году была предложена простая и вычислительно эффективная идея, хорошо зарекомендовавшая себя на практике. было бы круто сравнить эти подходы на одной задаче. интересное отобрал ❣ алексей морозов ml underhood #yaicml25 icml 2025: интересные доклады на тему ml — часть 2 when to retrain machine learning model в работе исследуют проблему регулярного переобучения моделей в продакшн-системах: то, как часто нужно полностью обучать модель с нуля на новых данных. приходят к выводу, что переобучать слишком часто — дорого и бесполезно, попробуют понять, в какие моменты времени это лучше делать. получается временной ряд, который они аппроксимируют своими методами. решение имеет смысл, только если есть возможность переобучать модель очень часто, но хочется делать это реже — без ущерба для качества. при этом, поскольку подход ориентирован именно на полное переобучение «с нуля», он не применяется к онлайн-обучению: там всегда предпочтительнее дообучать модель настолько часто, насколько это возможно. how to set adamw’s weight decay as you scale model and dataset size новый метод для подбора гиперпараметра регуляризации в adamw. авторы переписали формулы weight decay в виде, который начинает походить на экспоненциальное сглаживание (ewma). репараметризуют его новыми параметрами и говорят, что подбор одного нового параметра работает проще и сохраняет свойства при изменении размеров датасета, размера батча или размера архитектуры. то есть можно один раз подобрать и какое-то время о нём не вспоминать. формула очень простая и её будет легко попробовать в боевых моделях. efficient optimization with orthogonality constraint: a randomized riemannian submanifold method ещё одна статья на тему оптимизации на римановых многообразиях для ортогональных матриц. из интересного — оказывается, условия ортогональности используются сейчас не только в классических задачах вроде pca, но и в некоторых задачах файнтюна. к сожалению, автор не читал статью orthogonal weight normalization , где в 2017 году была предложена простая и вычислительно эффективная идея, хорошо зарекомендовавшая себя на практике. было бы круто сравнить эти подходы на одной задаче. интересное отобрал ❣ алексей морозов ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-17T13:03:35+00:00" href="./posts/132.html">2025-07-17 13:03 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICML 2025: интересные доклады на тему ML — часть 2</strong><br><br><a href="https://arxiv.org/abs/2505.14903" rel="nofollow noopener noreferrer"><strong>When to Retrain Machine Learning Model<br></strong></a><br>В работе исследуют проблему регулярного переобучения моделей в продакшн-системах: то, как часто нужно полностью обучать модель с нуля на новых данных. Приходят к выводу, что переобучать слишком часто — дорого и бесполезно, попробуют понять, в какие моменты времени это лучше делать. Получается временной ряд, который они аппроксимируют своими методами. Решение имеет смысл, только если есть возможность переобучать модель очень часто, но хочется делать это реже — без ущерба для качества. При этом, поскольку подход ориентирован именно на полное переобучение «с нуля», он не применяется к онлайн-обучению: там всегда предпочтительнее дообучать модель настолько часто, насколько это возможно.<br><br><a href="https://arxiv.org/abs/2405.13698" rel="nofollow noopener noreferrer"><strong>How to set AdamW’s weight decay as you scale model and dataset size</strong></a><br><br>Новый метод для подбора гиперпараметра регуляризации в AdamW. Авторы переписали формулы weight decay в виде, который начинает походить на экспоненциальное сглаживание (EWMA). Репараметризуют его новыми параметрами и говорят, что подбор одного нового параметра работает проще и сохраняет свойства при изменении размеров датасета, размера батча или размера архитектуры. То есть можно один раз подобрать и какое-то время о нём не вспоминать. Формула очень простая и её будет легко попробовать в боевых моделях.<br><br><a href="https://arxiv.org/abs/2505.12378" rel="nofollow noopener noreferrer"><strong>Efficient Optimization with Orthogonality Constraint: a Randomized Riemannian Submanifold Method</strong></a><br><br>Ещё одна статья на тему оптимизации на римановых многообразиях для ортогональных матриц. Из интересного — оказывается, условия ортогональности используются сейчас не только в классических задачах вроде PCA, но и в некоторых задачах файнтюна. К сожалению, автор не читал статью <a href="https://arxiv.org/abs/1709.06079" rel="nofollow noopener noreferrer">Orthogonal Weight Normalization</a>, где в 2017 году была предложена простая и вычислительно эффективная идея, хорошо зарекомендовавшая себя на практике. Было бы круто сравнить эти подходы на одной задаче.<br><br><em>Интересное отобрал </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Морозов</em><br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/132_480.webp" srcset="../assets/media/thumbs/132_480.webp 480w, ../assets/media/132.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="132" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/133_480.webp" srcset="../assets/media/thumbs/133_480.webp 480w, ../assets/media/133.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="132" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/134_480.webp" srcset="../assets/media/thumbs/134_480.webp 480w, ../assets/media/134.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="132" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 246 просмотров · 14 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/132" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/132.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="131" data-search="icml 2025: интересные доклады на тему ml — часть 1 в эти дни в ванкувере стартовала icml 2025. инженеры яндекса делятся первой порцией любопытных работ прямо с места событий. efficient distributed optimization under heavy-tailed noise авторы пытаются бороться с шумными апдейтами без дополнительной памяти. вводят два гиперпараметра: «верхний порог» и «нижний порог», но при этом не просто обрезают градиенты по порогам, а делают это необычным способом, получая более качественную оптимизацию. достоинство метода — в его stateless-сущности и экономии памяти, недостаток — в необходимость подбирать два новых гиперпараметра. существующие методы, вроде amsgrad, делают примерно то же самое: борются с взрывными апдейтами, но с использованием дополнительной памяти. огорчает, что нет сравнения с amsgrad — старый stateful-метод vs новый stateless-метод. online conformal prediction via online optimization несмотря на немного обескураживающее название, под капотом — онлайн-обучение квантильной регрессии (алгоритм оптимизации разработан специально для неё). на постере нет оценок на regret, однако авторы заверили, что их можно получить, поскольку это узкая задача из уже изученного более широкого семейства. lean and mean adaptive optimization via subset-norm and subspace-momentum with convergence guarantees сугубо теоретическая статья, практические применения которой уже можно было видеть. adagrad, adam, rmsprop — покоординатные адаптивные lr. есть другая крайность — один нормализатор на все параметры (что делает метод фактически sgd, только чуть более простым в подборе гиперпараметров). авторы исследуют нечто среднее: делят параметры на группы и для каждой вычисляют нормализатор из нормы вектора градиентов. во‑первых, авторы выписали оценки сходимости для ряда задач, во‑вторых — провели эксперименты с трансформерами для выбора оптимальных групп параметров. из личного разговора с исследователем удалось узнать, что лучше брать матрицы целиком — поколоночные и построчные группы работают хуже и покоординатного метода, и предложенного метода. global curvature for second-order optimization of neural networks метод второго порядка для оптимизации нейросетей. смысл такой же, как в классических подходах: давайте будем считать произведение обратного квадратного корня гессиана на градиент как-нибудь побыстрее. авторы статьи говорят: вычисление feed forward-архитектур устойчиво к некоторым перестановкам в матрицах весов линейных проекций — и некоторыми похожими свойствами обладает гессиан. из этого свойства они получают вычислительно более эффективный метод. разные методы оптимизации предлагают разные способы считать произведение обратного квадратного корня гессиана на градиент. самые известные методы для large scale-задач — bfgs и l-bfgs. пообщались с авторами статей — они заявляют, что их метод лучше для их архитектур, потому что он ищет среди точных решений (с учётом исследуемого ими свойства устойчивости к перестановкам), а семейства bfsg используют low-rank аппроксимацию, то есть не дают точного решения. формулы выписаны только для tanh-активации. пожелаем авторам удачи — хочется увидеть фундаментальный сдвиг в качестве методов оптимизации и асимптотике сходимости, а не очередной «adam с рюшечками». интересное отобрал ❣ алексей морозов ml underhood #yaicml25 icml 2025: интересные доклады на тему ml — часть 1 в эти дни в ванкувере стартовала icml 2025. инженеры яндекса делятся первой порцией любопытных работ прямо с места событий. efficient distributed optimization under heavy-tailed noise авторы пытаются бороться с шумными апдейтами без дополнительной памяти. вводят два гиперпараметра: «верхний порог» и «нижний порог», но при этом не просто обрезают градиенты по порогам, а делают это необычным способом, получая более качественную оптимизацию. достоинство метода — в его stateless-сущности и экономии памяти, недостаток — в необходимость подбирать два новых гиперпараметра. существующие методы, вроде amsgrad, делают примерно то же самое: борются с взрывными апдейтами, но с использованием дополнительной памяти. огорчает, что нет сравнения с amsgrad — старый stateful-метод vs новый stateless-метод. online conformal prediction via online optimization несмотря на немного обескураживающее название, под капотом — онлайн-обучение квантильной регрессии (алгоритм оптимизации разработан специально для неё). на постере нет оценок на regret, однако авторы заверили, что их можно получить, поскольку это узкая задача из уже изученного более широкого семейства. lean and mean adaptive optimization via subset-norm and subspace-momentum with convergence guarantees сугубо теоретическая статья, практические применения которой уже можно было видеть. adagrad, adam, rmsprop — покоординатные адаптивные lr. есть другая крайность — один нормализатор на все параметры (что делает метод фактически sgd, только чуть более простым в подборе гиперпараметров). авторы исследуют нечто среднее: делят параметры на группы и для каждой вычисляют нормализатор из нормы вектора градиентов. во‑первых, авторы выписали оценки сходимости для ряда задач, во‑вторых — провели эксперименты с трансформерами для выбора оптимальных групп параметров. из личного разговора с исследователем удалось узнать, что лучше брать матрицы целиком — поколоночные и построчные группы работают хуже и покоординатного метода, и предложенного метода. global curvature for second-order optimization of neural networks метод второго порядка для оптимизации нейросетей. смысл такой же, как в классических подходах: давайте будем считать произведение обратного квадратного корня гессиана на градиент как-нибудь побыстрее. авторы статьи говорят: вычисление feed forward-архитектур устойчиво к некоторым перестановкам в матрицах весов линейных проекций — и некоторыми похожими свойствами обладает гессиан. из этого свойства они получают вычислительно более эффективный метод. разные методы оптимизации предлагают разные способы считать произведение обратного квадратного корня гессиана на градиент. самые известные методы для large scale-задач — bfgs и l-bfgs. пообщались с авторами статей — они заявляют, что их метод лучше для их архитектур, потому что он ищет среди точных решений (с учётом исследуемого ими свойства устойчивости к перестановкам), а семейства bfsg используют low-rank аппроксимацию, то есть не дают точного решения. формулы выписаны только для tanh-активации. пожелаем авторам удачи — хочется увидеть фундаментальный сдвиг в качестве методов оптимизации и асимптотике сходимости, а не очередной «adam с рюшечками». интересное отобрал ❣ алексей морозов ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-17T08:51:15+00:00" href="./posts/131.html">2025-07-17 08:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>ICML 2025: интересные доклады на тему ML — часть 1</strong><br><br>В эти дни в Ванкувере стартовала ICML 2025. Инженеры Яндекса делятся первой порцией любопытных работ прямо с места событий. <br><br><a href="https://arxiv.org/abs/2502.04164" rel="nofollow noopener noreferrer"><strong>Efficient Distributed Optimization under Heavy-Tailed Noise</strong></a><strong><br></strong><br>Авторы пытаются бороться с шумными апдейтами без дополнительной памяти. Вводят два гиперпараметра: «верхний порог» и «нижний порог», но при этом не просто обрезают градиенты по порогам, а делают это необычным способом, получая более качественную оптимизацию. Достоинство метода — в его stateless-сущности и экономии памяти, недостаток — в необходимость подбирать два новых гиперпараметра. Существующие методы, вроде AMSgrad, делают примерно то же самое: борются с взрывными апдейтами, но с использованием дополнительной памяти. Огорчает, что нет сравнения с AMSgrad — старый stateful-метод VS новый stateless-метод.<br><br><a href="https://icml.cc/virtual/2025/poster/45619" rel="nofollow noopener noreferrer"><strong>Online Conformal Prediction via Online Optimization</strong></a><br><br>Несмотря на немного обескураживающее название, под капотом — онлайн-обучение квантильной регрессии (алгоритм оптимизации разработан специально для неё). На постере нет оценок на regret, однако авторы заверили, что их можно получить, поскольку это узкая задача из уже изученного более широкого семейства.<br><br><a href="https://arxiv.org/abs/2411.07120" rel="nofollow noopener noreferrer"><strong>Lean and Mean Adaptive Optimization via Subset-Norm and Subspace-Momentum with Convergence Guarantees</strong></a><br><br>Сугубо теоретическая статья, практические применения которой уже можно было видеть. AdaGrad, Adam, RMSprop — покоординатные адаптивные lr. Есть другая крайность — один нормализатор на все параметры (что делает метод фактически SGD, только чуть более простым в подборе гиперпараметров). Авторы исследуют нечто среднее: делят параметры на группы и для каждой вычисляют нормализатор из нормы вектора градиентов. Во‑первых, авторы выписали оценки сходимости для ряда задач, во‑вторых — провели эксперименты с трансформерами для выбора оптимальных групп параметров. Из личного разговора с исследователем удалось узнать, что лучше брать матрицы целиком — поколоночные и построчные группы работают хуже и покоординатного метода, и предложенного метода.<br><br><a href="https://icml.cc/virtual/2025/poster/44556" rel="nofollow noopener noreferrer"><strong>Global curvature for second-order optimization of neural networks</strong></a><br><br>Метод второго порядка для оптимизации нейросетей. Смысл такой же, как в классических подходах: давайте будем считать произведение обратного квадратного корня гессиана на градиент как-нибудь побыстрее. Авторы статьи говорят: вычисление feed forward-архитектур устойчиво к некоторым перестановкам в матрицах весов линейных проекций — и некоторыми похожими свойствами обладает гессиан. Из этого свойства они получают вычислительно более эффективный метод. Разные методы оптимизации предлагают разные способы считать произведение обратного квадратного корня гессиана на градиент. Самые известные методы для large scale-задач — BFGS и L-BFGS. Пообщались с авторами статей — они заявляют, что их метод лучше для их архитектур, потому что он ищет среди точных решений (с учётом исследуемого ими свойства устойчивости к перестановкам), а семейства BFSG используют low-rank аппроксимацию, то есть не дают точного решения. Формулы выписаны только для tanh-активации. Пожелаем авторам удачи — хочется увидеть фундаментальный сдвиг в качестве методов оптимизации и асимптотике сходимости, а не очередной «Adam с рюшечками».<br><br><em>Интересное отобрал </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Алексей Морозов</em><br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/131_480.webp" srcset="../assets/media/thumbs/131_480.webp 480w, ../assets/media/131.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="131" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 506 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/131" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/131.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="130" data-search="векторный поиск в ydb: зачем он нужен и как его используют в алисе сегодня команда yandex b2b tech представила новую версию системы управления базами данных ydb. главная фича — векторный поиск. с ним можно за миллисекунды находить информацию в разнородных данных и формировать персональные ответы на запросы пользователей. технология основана на поиске семантически похожих данных в больших коллекциях. разные типы данных — текст, изображения, аудио и видео — представляются в виде эмбеддингов, которые затем сохраняются в базу данных. после этого можно находить не только точные совпадения, но и близкие по смыслу объекты — даже если они записаны по-разному или вообще без описаний. векторный поиск улучшает качество и увеличивает скорость работы продуктов на базе ии: рекомендательных и поисковых систем, виртуальных ассистентов. никита зубков, руководитель отдела разработки диалоговой системы алисы, рассказал, как технология помогает сделать общение пользователей с ассистентом более персонализированным: с помощью векторного поиска мы находим наиболее релевантные диалогу сессии в прошлом и подставляем их в контекст. благодаря этому ответы алисы становятся персональными: она больше не забывает, как зовут вашего котика, когда вы последний раз ходили в спортзал или какой фильм вы недавно обсуждали с друзьями. например, раньше алиса обнулялась и не помнила, есть ли у вас домашнее животное, какой оно породы и как его зовут. но теперь, если сообщить ей эту информацию, а затем задать вопрос: «как мне провести выходные?», она может предложить пойти в парк с собакой и даже напомнит взять любимый зелёный мячик питомца. в ydb есть две версии векторного поиска: точный и приближённый. первый гарантирует, что найденные результаты будут самыми похожими на использованный образец, но требует большой вычислительной сложности. приближённый — позволяет искать по коллекциям из сотен миллионов векторов за десятки-сотни миллисекунд, даже если все вектора не помещаются в оперативную память. база данных ydb доступна как опенсорс-проект и как коммерческая сборка с открытым ядром. обе версии можно развернуть на своих серверах или воспользоваться managed-решением в yandex cloud. больше технических деталей можно узнать из статьи на хабре. ml underhood векторный поиск в ydb: зачем он нужен и как его используют в алисе сегодня команда yandex b2b tech представила новую версию системы управления базами данных ydb . главная фича — векторный поиск. с ним можно за миллисекунды находить информацию в разнородных данных и формировать персональные ответы на запросы пользователей. технология основана на поиске семантически похожих данных в больших коллекциях. разные типы данных — текст, изображения, аудио и видео — представляются в виде эмбеддингов, которые затем сохраняются в базу данных. после этого можно находить не только точные совпадения, но и близкие по смыслу объекты — даже если они записаны по-разному или вообще без описаний. векторный поиск улучшает качество и увеличивает скорость работы продуктов на базе ии: рекомендательных и поисковых систем, виртуальных ассистентов. никита зубков, руководитель отдела разработки диалоговой системы алисы, рассказал, как технология помогает сделать общение пользователей с ассистентом более персонализированным: с помощью векторного поиска мы находим наиболее релевантные диалогу сессии в прошлом и подставляем их в контекст. благодаря этому ответы алисы становятся персональными: она больше не забывает, как зовут вашего котика, когда вы последний раз ходили в спортзал или какой фильм вы недавно обсуждали с друзьями. например, раньше алиса обнулялась и не помнила, есть ли у вас домашнее животное, какой оно породы и как его зовут. но теперь, если сообщить ей эту информацию, а затем задать вопрос: «как мне провести выходные?», она может предложить пойти в парк с собакой и даже напомнит взять любимый зелёный мячик питомца. в ydb есть две версии векторного поиска: точный и приближённый. первый гарантирует, что найденные результаты будут самыми похожими на использованный образец, но требует большой вычислительной сложности. приближённый — позволяет искать по коллекциям из сотен миллионов векторов за десятки-сотни миллисекунд, даже если все вектора не помещаются в оперативную память. база данных ydb доступна как опенсорс-проект и как коммерческая сборка с открытым ядром. обе версии можно развернуть на своих серверах или воспользоваться managed-решением в yandex cloud . больше технических деталей можно узнать из статьи на хабре . ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-15T06:05:30+00:00" href="./posts/130.html">2025-07-15 06:05 UTC</a></div>
      </div>
      <div class="post-body"><strong>Векторный поиск в YDB: зачем он нужен и как его используют в Алисе</strong><br><br>Сегодня команда Yandex B2B Tech представила новую версию системы управления базами данных <a href="https://ydb.yandex.ru/" rel="nofollow noopener noreferrer">YDB</a>. Главная фича — векторный поиск. С ним можно за миллисекунды находить информацию в разнородных данных и формировать персональные ответы на запросы пользователей. <br><br>Технология основана на поиске семантически похожих данных в больших коллекциях. Разные типы данных — текст, изображения, аудио и видео — представляются в виде эмбеддингов, которые затем сохраняются в базу данных. После этого можно находить не только точные совпадения, но и близкие по смыслу объекты — даже если они записаны по-разному или вообще без описаний.<br><br>Векторный поиск улучшает качество и увеличивает скорость работы продуктов на базе ИИ: рекомендательных и поисковых систем, виртуальных ассистентов. Никита Зубков, руководитель отдела разработки диалоговой системы Алисы, рассказал, как технология помогает сделать общение пользователей с ассистентом более персонализированным:<br><br><blockquote>С помощью векторного поиска мы находим наиболее релевантные диалогу сессии в прошлом и подставляем их в контекст. Благодаря этому ответы Алисы становятся персональными: она больше не забывает, как зовут вашего котика, когда вы последний раз ходили в спортзал или какой фильм вы недавно обсуждали с друзьями.<br><br>Например, раньше Алиса обнулялась и не помнила, есть ли у вас домашнее животное, какой оно породы и как его зовут. Но теперь, если сообщить ей эту информацию, а затем задать вопрос: «Как мне провести выходные?», она может предложить пойти в парк с собакой и даже напомнит взять любимый зелёный мячик питомца.</blockquote><br><br>В YDB есть две версии векторного поиска: точный и приближённый. Первый гарантирует, что найденные результаты будут самыми похожими на использованный образец, но требует большой вычислительной сложности. Приближённый — позволяет искать по коллекциям из сотен миллионов векторов за десятки-сотни миллисекунд, даже если все вектора не помещаются в оперативную память. <br><br>База данных YDB <a href="https://github.com/ydb-platform" rel="nofollow noopener noreferrer">доступна</a> как опенсорс-проект и как <a href="https://ydb.yandex.ru/" rel="nofollow noopener noreferrer">коммерческая сборка</a> с открытым ядром. Обе версии можно развернуть на своих серверах или воспользоваться <a href="https://yandex.cloud/ru/services/ydb" rel="nofollow noopener noreferrer">managed-решением в Yandex Cloud</a>. Больше технических деталей можно узнать из <a href="https://habr.com/ru/companies/yandex/articles/926724/" rel="nofollow noopener noreferrer">статьи на Хабре</a>.<br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/130_480.webp" srcset="../assets/media/thumbs/130_480.webp 480w, ../assets/media/130.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="130" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 913 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/130" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/130.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="128" data-search="yandex research везёт на icml 2025 шесть статей шесть работ российских исследователей из яндекса приняли на icml (international conference on machine learning) — одну из старейших и самых авторитетных в мире научных конференций по ии, которая входит в топ-3 согласно google scholar. статьи посвящены различным аспектам машинного обучения — от алгоритмического мышления нейронных сетей и измерения разнообразия до оптимизации использования памяти при работе с большими языковыми моделями. кратко рассказываем о каждой из них — подробнее можно почитать в блоге yandex research. discrete neural algorithmic reasoning авторы исследуют причины, по которым нейросетевые модели плохо обобщаются при обучении на алгоритмические задачи, и предлагают архитектурные изменения, решающие эту проблему. в частности, вводят ограничение на представление состояний вычислений, что обеспечивает точное соответствие исходным алгоритмам. этот подход позволил добиться чёткого выполнения нейросетью нескольких алгоритмов. кроме того, предложенная архитектура даёт возможность строго доказывать корректность работы обученных моделей на любых входных данных. measuring diversity: axioms and challenges в работе анализируют метрики разнообразия и выделяют три свойства, которым должна удовлетворять хорошая метрика: монотонность, уникальность и непрерывность. существующие метрики не удовлетворяют хотя бы одному из этих свойств. при этом в работе приведены примеры метрик, которые удовлетворяют всем, но их вычисление — np-трудная задача. вопрос о том, существуют ли эффективные метрики со всеми желаемыми свойствами, остаётся открытым. cache me if you must: adaptive key-value quantization for large language models llm хранят ключи (k) и значения (v) внимания для каждого токена, что быстро расходует память. авторы предлагают сжимать их не в исходном виде, а с учётом взаимной информации между слоями — кодировать только то, что нельзя предсказать по соседнему слою линейными предикторами. это позволяет сжимать kv-вектора почти без потерь качества даже при экстремальном 2-битном квантовании. frugal: memory-efficient optimization by reducing state overhead for scalable training при увеличении размеров обучаемой модели для хранения статистик оптимизатора требуется огромное количество памяти. предыдущие методы уменьшали эту нагрузку, проецируя градиент на малоранговое пространство, где и хранились статистики оптимизатора. однако такой подход не использует всю информацию из градиента. авторы frugal предлагают решить эту проблему, разделяя градиент на две части, одна из которых используется для обновления в малоранговом подпространстве через adam, а вторая — в оставшемся подпространстве с помощью оптимизатора без статистик, например sgd или signsgd. метод стабильно превосходит другие подходы при ограниченных ресурсах, достигая лучших результатов в предобучении и дообучении при той же экономии памяти. inverse bridge matching distillation авторы предлагают алгоритм дистилляции diffusion bridge-модели (dbm) для задачи image-to-image translation до одного шага. метод работает как для условных, так и безусловных моделей, может применяться для широкого класса задач реконструкции и генерации изображений, а также ускоряет работу моделей в 4–100 раз. в некоторых задачах модель-ученик даёт результат лучше, чем модель-учитель. evopress: towards optimal dynamic model compression via evolutionary search evopress — метод оптимального динамического сжатия больших языковых моделей, основанный на применении эволюционного алгоритма. он учитывает сложную нелинейную взаимосвязь между разными слоями нейронной сети. подход валидируют на семействах моделей llama, mistral и phi, где evopress достигает более высокого качества по сравнению с однородным сжатием и конкурентными динамическими методами. в этом году конференция будет проходить с 13 по 19 июля в ванкувере, и её по традиции посетят ml-инженеры из яндекса. ну а мы будем рассказывать о самых интересных статьях и докладах. ml underhood #yaicml25 yandex research везёт на icml 2025 шесть статей шесть работ российских исследователей из яндекса приняли на icml (international conference on machine learning) — одну из старейших и самых авторитетных в мире научных конференций по ии, которая входит в топ-3 согласно google scholar. статьи посвящены различным аспектам машинного обучения — от алгоритмического мышления нейронных сетей и измерения разнообразия до оптимизации использования памяти при работе с большими языковыми моделями. кратко рассказываем о каждой из них — подробнее можно почитать в блоге yandex research. discrete neural algorithmic reasoning авторы исследуют причины, по которым нейросетевые модели плохо обобщаются при обучении на алгоритмические задачи, и предлагают архитектурные изменения, решающие эту проблему. в частности, вводят ограничение на представление состояний вычислений, что обеспечивает точное соответствие исходным алгоритмам. этот подход позволил добиться чёткого выполнения нейросетью нескольких алгоритмов. кроме того, предложенная архитектура даёт возможность строго доказывать корректность работы обученных моделей на любых входных данных. measuring diversity: axioms and challenges в работе анализируют метрики разнообразия и выделяют три свойства, которым должна удовлетворять хорошая метрика: монотонность, уникальность и непрерывность. существующие метрики не удовлетворяют хотя бы одному из этих свойств. при этом в работе приведены примеры метрик, которые удовлетворяют всем, но их вычисление — np-трудная задача. вопрос о том, существуют ли эффективные метрики со всеми желаемыми свойствами, остаётся открытым. cache me if you must: adaptive key-value quantization for large language models llm хранят ключи (k) и значения (v) внимания для каждого токена, что быстро расходует память. авторы предлагают сжимать их не в исходном виде, а с учётом взаимной информации между слоями — кодировать только то, что нельзя предсказать по соседнему слою линейными предикторами. это позволяет сжимать kv-вектора почти без потерь качества даже при экстремальном 2-битном квантовании. frugal: memory-efficient optimization by reducing state overhead for scalable training при увеличении размеров обучаемой модели для хранения статистик оптимизатора требуется огромное количество памяти. предыдущие методы уменьшали эту нагрузку, проецируя градиент на малоранговое пространство, где и хранились статистики оптимизатора. однако такой подход не использует всю информацию из градиента. авторы frugal предлагают решить эту проблему, разделяя градиент на две части, одна из которых используется для обновления в малоранговом подпространстве через adam, а вторая — в оставшемся подпространстве с помощью оптимизатора без статистик, например sgd или signsgd. метод стабильно превосходит другие подходы при ограниченных ресурсах, достигая лучших результатов в предобучении и дообучении при той же экономии памяти. inverse bridge matching distillation авторы предлагают алгоритм дистилляции diffusion bridge-модели (dbm) для задачи image-to-image translation до одного шага. метод работает как для условных, так и безусловных моделей, может применяться для широкого класса задач реконструкции и генерации изображений, а также ускоряет работу моделей в 4–100 раз. в некоторых задачах модель-ученик даёт результат лучше, чем модель-учитель. evopress: towards optimal dynamic model compression via evolutionary search evopress — метод оптимального динамического сжатия больших языковых моделей, основанный на применении эволюционного алгоритма. он учитывает сложную нелинейную взаимосвязь между разными слоями нейронной сети. подход валидируют на семействах моделей llama, mistral и phi, где evopress достигает более высокого качества по сравнению с однородным сжатием и конкурентными динамическими методами. в этом году конференция будет проходить с 13 по 19 июля в ванкувере, и её по традиции посетят ml-инженеры из яндекса. ну а мы будем рассказывать о самых интересных статьях и докладах. ml underhood #yaicml25">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-07-10T13:25:01+00:00" href="./posts/128.html">2025-07-10 13:25 UTC</a></div>
      </div>
      <div class="post-body"><strong>Yandex Research везёт на ICML 2025 шесть статей</strong><br><br>Шесть работ российских исследователей из Яндекса приняли на <a href="https://icml.cc/" rel="nofollow noopener noreferrer">ICML</a> (International Conference on Machine Learning) — одну из старейших и самых авторитетных в мире научных конференций по ИИ, которая входит в топ-3 согласно Google Scholar. Статьи посвящены различным аспектам машинного обучения — от алгоритмического мышления нейронных сетей и измерения разнообразия до оптимизации использования памяти при работе с большими языковыми моделями. Кратко рассказываем о каждой из них — подробнее можно почитать <a href="https://research.yandex.com/blog/papers-accepted-to-icml-2025" rel="nofollow noopener noreferrer">в блоге</a> Yandex Research.<br><br><a href="https://arxiv.org/abs/2402.11628" rel="nofollow noopener noreferrer"><strong>Discrete Neural Algorithmic Reasoning<br></strong></a>Авторы исследуют причины, по которым нейросетевые модели плохо обобщаются при обучении на алгоритмические задачи, и предлагают архитектурные изменения, решающие эту проблему. В частности, вводят ограничение на представление состояний вычислений, что обеспечивает точное соответствие исходным алгоритмам. Этот подход позволил добиться чёткого выполнения нейросетью нескольких алгоритмов. Кроме того, предложенная архитектура даёт возможность строго доказывать корректность работы обученных моделей на любых входных данных.<br><br><a href="https://arxiv.org/abs/2410.14556" rel="nofollow noopener noreferrer"><strong>Measuring Diversity: Axioms and Challenges</strong></a><br>В работе анализируют метрики разнообразия и выделяют три свойства, которым должна удовлетворять хорошая метрика: монотонность, уникальность и непрерывность. Существующие метрики не удовлетворяют хотя бы одному из этих свойств. При этом в работе приведены примеры метрик, которые удовлетворяют всем, но их вычисление — NP-трудная задача. Вопрос о том, существуют ли эффективные метрики со всеми желаемыми свойствами, остаётся открытым.<br><br><a href="https://arxiv.org/abs/2501.19392" rel="nofollow noopener noreferrer"><strong>Cache Me If You Must: Adaptive Key-Value Quantization for Large Language Models</strong></a> <br>LLM хранят ключи (K) и значения (V) внимания для каждого токена, что быстро расходует память. Авторы предлагают сжимать их не в исходном виде, а с учётом взаимной информации между слоями — кодировать только то, что нельзя предсказать по соседнему слою линейными предикторами. Это позволяет сжимать KV-вектора почти без потерь качества даже при экстремальном 2-битном квантовании.<br><br><a href="https://arxiv.org/abs/2411.07837" rel="nofollow noopener noreferrer"><strong>FRUGAL: Memory-Efficient Optimization by Reducing State Overhead for Scalable Training</strong></a> <br>При увеличении размеров обучаемой модели для хранения статистик оптимизатора требуется огромное количество памяти. Предыдущие методы уменьшали эту нагрузку, проецируя градиент на малоранговое пространство, где и хранились статистики оптимизатора. Однако такой подход не использует всю информацию из градиента. Авторы FRUGAL предлагают решить эту проблему, разделяя градиент на две части, одна из которых используется для обновления в малоранговом подпространстве через Adam, а вторая — в оставшемся подпространстве с помощью оптимизатора без статистик, например SGD или signSGD. Метод стабильно превосходит другие подходы при ограниченных ресурсах, достигая лучших результатов в предобучении и дообучении при той же экономии памяти.<br><br><a href="https://arxiv.org/abs/2502.01362" rel="nofollow noopener noreferrer"><strong>Inverse Bridge Matching Distillation <br></strong></a>Авторы предлагают алгоритм дистилляции diffusion bridge-модели (DBM) для задачи image-to-image translation до одного шага. Метод работает как для условных, так и безусловных моделей, может применяться для широкого класса задач реконструкции и генерации изображений, а также ускоряет работу моделей в 4–100 раз. В некоторых задачах модель-ученик даёт результат лучше, чем модель-учитель.<br><br><a href="https://arxiv.org/abs/2410.14649" rel="nofollow noopener noreferrer"><strong>EvoPress: Towards Optimal Dynamic Model Compression via Evolutionary Search</strong></a> <br>EvoPress — метод оптимального динамического сжатия больших языковых моделей, основанный на применении эволюционного алгоритма. Он учитывает сложную нелинейную взаимосвязь между разными слоями нейронной сети. Подход валидируют на семействах моделей Llama, Mistral и Phi, где EvoPress достигает более высокого качества по сравнению с однородным сжатием и конкурентными динамическими методами.<br><br>В этом году конференция будет проходить с 13 по 19 июля в Ванкувере, и её по традиции посетят ML-инженеры из Яндекса. Ну а мы будем рассказывать о самых интересных статьях и докладах.<br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaICML25</div>
      <div class="actions">
        <span>2 134 просмотров · 50 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/128" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/128.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="124" data-search="впечатления от iclr 2025 подводим итоги конференции вместе с инженерами яндекса. сегодня о своих впечатлениях от iclr в этих карточках и одной секретной поделится руководитель cloud ai/ml services yandex cloud василий ершов. ml underhood впечатления от iclr 2025 подводим итоги конференции вместе с инженерами яндекса. сегодня о своих впечатлениях от iclr в этих карточках и одной секретной поделится руководитель cloud ai/ml services yandex cloud василий ершов. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-30T11:33:56+00:00" href="./posts/124.html">2025-06-30 11:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Впечатления от ICLR 2025</strong><br><br>Подводим итоги конференции вместе с инженерами Яндекса. Сегодня о своих впечатлениях от ICLR в этих карточках и <a href="https://telegra.ph/sfs-06-30-3" rel="nofollow noopener noreferrer">одной секретной</a> поделится руководитель Cloud AI/ML Services Yandex Cloud Василий Ершов. <br><br><a href="https://t.me/+6-c8oKONtxdlMjZi" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/124_480.webp" srcset="../assets/media/thumbs/124_480.webp 480w, ../assets/media/124.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="124" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/125_480.webp" srcset="../assets/media/thumbs/125_480.webp 480w, ../assets/media/125.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="124" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/126_480.webp" srcset="../assets/media/thumbs/126_480.webp 480w, ../assets/media/126.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="124" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/127_480.webp" srcset="../assets/media/thumbs/127_480.webp 480w, ../assets/media/127.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="124" data-image-index="3" /></div></div>
      <div class="actions">
        <span>5 632 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/124" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/124.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="119" data-search="заметки с icra — главной конференции по робототехнике в конце мая в сша прошла icra — топ-1 по цитируемости конференция в области робототехники. на мероприятии побывала руководитель службы исследования алгоритмов нового поколения мария голицына. она делится статьями на тему автономного транспорта, а ещё — фотографиями самых разных роботов. в разборе уместилось лишь несколько работ — полный список отобранных марией статей можно увидеть по ссылке. mitigating covariate shift in imitation learning for autonomous vehicles using latent space generative world models первым, с кем удалось поговорить на воркшопах, был александр попов — один из авторов статьи. он работает в nvidia, где несколько сотен человек занимаются разработкой беспилотных автомобилей. в частности, команда развивает подход perception-to-trajectory: на входе — изображение с камеры, на выходе — траектория, по которой едет машина. в работе обсуждается проблема covariate shift — отклонений от распределения, на котором модель обучалась. это частая проблема в imitation learning: агент может оказаться в состояниях, которых не было в демонстрациях эксперта, и начинает совершать ошибки. авторы предлагают решение — использовать замкнутый цикл (closed-loop training) с генеративной моделью мира, работающей в латентном пространстве. в этой схеме берутся исторические данные с камер, затем система «закрывает глаза» и делает последовательность шагов вперёд в латентном пространстве — в статье это 12 шагов. на каждом шаге сравнивается действие эксперта и действие, предсказанное, чтобы выровнять распределения. дополнительно используется лосс, который приближает распределения латентных переходов модели к тем, что наблюдаются в обучающих данных. это помогает агенту научиться возвращению к траектории даже в ситуациях, когда он ушёл далеко. completing explicit 3d reconstruction via view extrapolation with diffusion priors другая работа — о 3d-реконструкции с помощью дополнения недостающих видов. это идея, которая сейчас витает в воздухе: если у нас есть всего несколько ракурсов объекта, и их не хватает для точной реконструкции, можно дополнить недостающие изображения сгенерированными диффузионной моделью. авторы используют diffusion priors, чтобы «достроить» недостающие виды (view extrapolation), а затем делают реконструкцию по расширенному набору. базовая модель — foundation-модель mvdream, которая работает с несколькими изображениями на входе. если подавать больше сгенерированных видов (например, 3 + 3, 3 + 6), качество итоговой реконструкции улучшается. правда, есть нюанс: диффузионка может выдать неконсистентные виды, и в этом случае качество ухудшается. но по мере того как сами модели улучшаются, подход начинает работать всё стабильнее. в этом исследовании как раз демонстрируется, что сгенерированные виды действительно помогают улучшить результат. таких работ на конференции было много — идея активно развивается. uncertainty-guided enhancement on driving perception system via foundation models одна из немногих работ на icra, где llm используется в контексте вождения. идея простая: если perception-система не уверена в своём предсказании, можно подстраховаться с помощью foundation-модели. то есть модель делает предсказание и оценивает его надёжность. если уверенность высокая — используем результат. если низкая — подключаем llm, которая даёт своё предсказание, и берём то, что надёжнее. llm тут не участвует в обучении и не делает инференс постоянно — её подключают только по необходимости. это скорее механизм уверенного доуточнения, чем полноценный модуль восприятия. подводя итог, можно сказать, что icra, как и многие крупные конференции, — ещё и отличная площадка для нетворкинга. удалось поговорить и обменяться идеями с инженерами из zoox, waymo, nuro, motional, loxo и других компаний, которые занимаются автономным транспортом. ml underhood заметки с icra — главной конференции по робототехнике в конце мая в сша прошла icra — топ-1 по цитируемости конференция в области робототехники. на мероприятии побывала руководитель службы исследования алгоритмов нового поколения мария голицына. она делится статьями на тему автономного транспорта, а ещё — фотографиями самых разных роботов. в разборе уместилось лишь несколько работ — полный список отобранных марией статей можно увидеть по ссылке . mitigating covariate shift in imitation learning for autonomous vehicles using latent space generative world models первым, с кем удалось поговорить на воркшопах, был александр попов — один из авторов статьи. он работает в nvidia, где несколько сотен человек занимаются разработкой беспилотных автомобилей. в частности, команда развивает подход perception-to-trajectory: на входе — изображение с камеры, на выходе — траектория, по которой едет машина. в работе обсуждается проблема covariate shift — отклонений от распределения, на котором модель обучалась. это частая проблема в imitation learning: агент может оказаться в состояниях, которых не было в демонстрациях эксперта, и начинает совершать ошибки. авторы предлагают решение — использовать замкнутый цикл (closed-loop training) с генеративной моделью мира, работающей в латентном пространстве. в этой схеме берутся исторические данные с камер, затем система «закрывает глаза» и делает последовательность шагов вперёд в латентном пространстве — в статье это 12 шагов. на каждом шаге сравнивается действие эксперта и действие, предсказанное, чтобы выровнять распределения. дополнительно используется лосс, который приближает распределения латентных переходов модели к тем, что наблюдаются в обучающих данных. это помогает агенту научиться возвращению к траектории даже в ситуациях, когда он ушёл далеко. completing explicit 3d reconstruction via view extrapolation with diffusion priors другая работа — о 3d-реконструкции с помощью дополнения недостающих видов. это идея, которая сейчас витает в воздухе: если у нас есть всего несколько ракурсов объекта, и их не хватает для точной реконструкции, можно дополнить недостающие изображения сгенерированными диффузионной моделью. авторы используют diffusion priors, чтобы «достроить» недостающие виды (view extrapolation), а затем делают реконструкцию по расширенному набору. базовая модель — foundation-модель mvdream, которая работает с несколькими изображениями на входе. если подавать больше сгенерированных видов (например, 3 + 3, 3 + 6), качество итоговой реконструкции улучшается. правда, есть нюанс: диффузионка может выдать неконсистентные виды, и в этом случае качество ухудшается. но по мере того как сами модели улучшаются, подход начинает работать всё стабильнее. в этом исследовании как раз демонстрируется, что сгенерированные виды действительно помогают улучшить результат. таких работ на конференции было много — идея активно развивается. uncertainty-guided enhancement on driving perception system via foundation models одна из немногих работ на icra, где llm используется в контексте вождения. идея простая: если perception-система не уверена в своём предсказании, можно подстраховаться с помощью foundation-модели. то есть модель делает предсказание и оценивает его надёжность. если уверенность высокая — используем результат. если низкая — подключаем llm, которая даёт своё предсказание, и берём то, что надёжнее. llm тут не участвует в обучении и не делает инференс постоянно — её подключают только по необходимости. это скорее механизм уверенного доуточнения, чем полноценный модуль восприятия. подводя итог, можно сказать, что icra, как и многие крупные конференции, — ещё и отличная площадка для нетворкинга. удалось поговорить и обменяться идеями с инженерами из zoox, waymo, nuro, motional, loxo и других компаний, которые занимаются автономным транспортом. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-26T11:18:48+00:00" href="./posts/119.html">2025-06-26 11:18 UTC</a></div>
      </div>
      <div class="post-body"><strong>Заметки с ICRA — главной конференции по робототехнике</strong><br><br>В конце мая в США прошла ICRA — топ-1 по цитируемости конференция в области робототехники. На мероприятии побывала руководитель службы исследования алгоритмов нового поколения Мария Голицына. Она делится статьями на тему автономного транспорта, а ещё — фотографиями самых разных роботов. В разборе уместилось лишь несколько работ — полный список отобранных Марией статей можно увидеть <a href="https://graph.org/Spisok-statej-s-ICRA-06-25" rel="nofollow noopener noreferrer">по ссылке</a>.<br><br><a href="https://arxiv.org/pdf/2409.16663" rel="nofollow noopener noreferrer"><strong>Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models</strong></a><br><br>Первым, с кем удалось поговорить на воркшопах, был Александр Попов — один из авторов статьи. Он работает в NVIDIA, где несколько сотен человек занимаются разработкой беспилотных автомобилей. В частности, команда развивает подход perception-to-trajectory: на входе — изображение с камеры, на выходе — траектория, по которой едет машина.<br><br>В работе обсуждается проблема covariate shift — отклонений от распределения, на котором модель обучалась. Это частая проблема в imitation learning: агент может оказаться в состояниях, которых не было в демонстрациях эксперта, и начинает совершать ошибки.<br><br>Авторы предлагают решение — использовать замкнутый цикл (closed-loop training) с генеративной моделью мира, работающей в латентном пространстве. В этой схеме берутся исторические данные с камер, затем система «закрывает глаза» и делает последовательность шагов вперёд в латентном пространстве — в статье это 12 шагов. На каждом шаге сравнивается действие эксперта и действие, предсказанное, чтобы выровнять распределения.<br><br>Дополнительно используется лосс, который приближает распределения латентных переходов модели к тем, что наблюдаются в обучающих данных. Это помогает агенту научиться возвращению к траектории даже в ситуациях, когда он ушёл далеко.<br><br><a href="https://openreview.net/pdf?id=imIYm1ILm9" rel="nofollow noopener noreferrer"><strong>Completing Explicit 3D Reconstruction via View Extrapolation with Diffusion Priors</strong></a><br><br>Другая работа — о 3D-реконструкции с помощью дополнения недостающих видов. Это идея, которая сейчас витает в воздухе: если у нас есть всего несколько ракурсов объекта, и их не хватает для точной реконструкции, можно дополнить недостающие изображения сгенерированными диффузионной моделью.<br><br>Авторы используют diffusion priors, чтобы «достроить» недостающие виды (view extrapolation), а затем делают реконструкцию по расширенному набору. Базовая модель — Foundation-модель MVDream, которая работает с несколькими изображениями на входе. Если подавать больше сгенерированных видов (например, 3 + 3, 3 + 6), качество итоговой реконструкции улучшается.<br><br>Правда, есть нюанс: диффузионка может выдать неконсистентные виды, и в этом случае качество ухудшается. Но по мере того как сами модели улучшаются, подход начинает работать всё стабильнее. В этом исследовании как раз демонстрируется, что сгенерированные виды действительно помогают улучшить результат. Таких работ на конференции было много — идея активно развивается.<br><br><a href="https://arxiv.org/pdf/2410.01144" rel="nofollow noopener noreferrer"><strong>Uncertainty-Guided Enhancement on Driving Perception System via Foundation Models</strong></a><br><br>Одна из немногих работ на ICRA, где LLM используется в контексте вождения. Идея простая: если perception-система не уверена в своём предсказании, можно подстраховаться с помощью foundation-модели. То есть модель делает предсказание и оценивает его надёжность. Если уверенность высокая — используем результат. Если низкая — подключаем LLM, которая даёт своё предсказание, и берём то, что надёжнее.<br><br>LLM тут не участвует в обучении и не делает инференс постоянно — её подключают только по необходимости. Это скорее механизм уверенного доуточнения, чем полноценный модуль восприятия.<br><br>Подводя итог, можно сказать, что ICRA, как и многие крупные конференции, — ещё и отличная площадка для нетворкинга. Удалось поговорить и обменяться идеями с инженерами из Zoox, Waymo, Nuro, Motional, Loxo и других компаний, которые занимаются автономным транспортом.<br><br><a href="https://t.me/+zFlM7fTaTgo0YWQy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><video controls preload="metadata" src="../assets/media/120_333.mp4"></video><img class="media-img" loading="lazy" src="../assets/media/thumbs/121_480.webp" srcset="../assets/media/thumbs/121_480.webp 480w, ../assets/media/121.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/122_480.webp" srcset="../assets/media/thumbs/122_480.webp 480w, ../assets/media/122.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/123_480.webp" srcset="../assets/media/thumbs/123_480.webp 480w, ../assets/media/123.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="119" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 986 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/119" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/119.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="116" data-search="как в яндекс картах находят редкие дорожные знаки среди миллионов изображений важная задача отдела картопроизводства — улучшать качество детекции дорожных знаков. один из способов — находить знаки на панорамах, снятых с помощью телефонов или камер безопасности в такси. на момент старта проекта детектор знаков уже был, но он находил не все знаки и с недостаточной полнотой и точностью. сами по себе знаки можно взять хоть из википедии — там есть список всех 300+ штук. но модель не тренируется на списке — нужны десятки тысяч изображений с примерами каждого знака в реальных условиях. вот тут и начинается основная работа. пайплайн, описанный ниже, служит для сужения огромного набора снимков (сотни миллионов) до относительно небольшого (сотни тысяч), на которых присутствует искомый нами знак. 1. находим всё, что может быть знаком на имеющемся датасете обучили rt-detr на единственный класс «дорожный знак». он «выкручен в полноту»: то есть в трейдофе полнота &lt;-&gt; точность, выбрана именно полнота. так сделано, потому что предсказания этого детектора — кандидаты на проверку. на выходе из этого этапа получается много «кропов-кандидатов» — кусочков исходного снимка, на котором представлено что-то похожее на знак. 2. классификация кропов дальше в дело вступает few-shot-классификатор на основе нашей картиночной тушки — большой свёрточной сети, которую разрабатывает служба компьютерного зрения. из неё взяты «эмбединги похожих» — представление изображений в векторном пространстве, где похожие изображения переходят в близкие вектора. поверх этих эмбедингов обучены несколько линейных слоёв. в качестве примеров позитивного класса используются 20–30 примеров нужного нам знака. примеров негативного класса в избытке — их берут из текущего датасета дорожных знаков. в результате получается блок, который умеет отвечать: похоже ли входное изображение (кроп-кандидат) на искомый нами знак. 3. классификация снимков если хотя бы один кроп на снимке прошёл классификацию, мы сохраняем весь снимок. так из сотен миллионов остаётся от сотни тысяч до миллиона в зависимости от знака. из полученных кандидатов мы отбираем на разметку лишь малое количество: 2–5–10 тысяч картинок. дальше подключается трёхступенчатый пайплайн разметки с помощью людей в яндекс заданиях. 1. проверка наличия знака асессор отвечает, есть ли на изображении нужный знак («да», «нет», «не загрузилась картинка»). это быстрый и дешёвый способ отсеять ошибки предыдущих этапов, не тратя ресурсы на полную разметку ненужных картинок. чтобы на выходе получить, например, 1200 знаков, на вход подаём с запасом — 2000–3000 изображений, иногда больше, если знак редкий. 2. разметка всех знаков на оставшихся изображениях люди размечают прямоугольники вокруг всех дорожных знаков — не только искомого. это важно для обучения детектора: нужны как положительные примеры, так и фоны с другими знаками, чтобы избежать ложных срабатываний. 3. классификация каждого знака каждый размеченный знак показывается отдельно, и асессор выбирает, что это за знак — из палетки с 300+ вариантов. пробовали упрощать интерфейс (группировка по цвету, форме и прочему), но это всё равно остаётся самым трудоёмким этапом. что в итоге сейчас весь пайплайн уже работает в проде. для некоторых знаков, вроде «железнодорожный переезд», удалось собрать 5000 размеченных примеров — больше, чем требовалось. а вот со знаками поворота всё сложнее: классификатор часто путает «влево» и «вправо», из-за чего нужные картинки отсеиваются, и на выходе остаётся по 700–800 примеров. в ближайший месяц планируем дособрать все основные знаки по россии и двинуться в сторону подготовки датасетов в межнаре. ml underhood как в яндекс картах находят редкие дорожные знаки среди миллионов изображений важная задача отдела картопроизводства — улучшать качество детекции дорожных знаков. один из способов — находить знаки на панорамах, снятых с помощью телефонов или камер безопасности в такси. на момент старта проекта детектор знаков уже был, но он находил не все знаки и с недостаточной полнотой и точностью. сами по себе знаки можно взять хоть из википедии — там есть список всех 300+ штук. но модель не тренируется на списке — нужны десятки тысяч изображений с примерами каждого знака в реальных условиях. вот тут и начинается основная работа. пайплайн, описанный ниже, служит для сужения огромного набора снимков (сотни миллионов) до относительно небольшого (сотни тысяч), на которых присутствует искомый нами знак. 1. находим всё, что может быть знаком на имеющемся датасете обучили rt-detr на единственный класс «дорожный знак». он «выкручен в полноту»: то есть в трейдофе полнота &amp;lt;-&amp;gt; точность, выбрана именно полнота. так сделано, потому что предсказания этого детектора — кандидаты на проверку. на выходе из этого этапа получается много «кропов-кандидатов» — кусочков исходного снимка, на котором представлено что-то похожее на знак. 2. классификация кропов дальше в дело вступает few-shot-классификатор на основе нашей картиночной тушки — большой свёрточной сети, которую разрабатывает служба компьютерного зрения. из неё взяты «эмбединги похожих» — представление изображений в векторном пространстве, где похожие изображения переходят в близкие вектора. поверх этих эмбедингов обучены несколько линейных слоёв. в качестве примеров позитивного класса используются 20–30 примеров нужного нам знака. примеров негативного класса в избытке — их берут из текущего датасета дорожных знаков. в результате получается блок, который умеет отвечать: похоже ли входное изображение (кроп-кандидат) на искомый нами знак. 3. классификация снимков если хотя бы один кроп на снимке прошёл классификацию, мы сохраняем весь снимок. так из сотен миллионов остаётся от сотни тысяч до миллиона в зависимости от знака. из полученных кандидатов мы отбираем на разметку лишь малое количество: 2–5–10 тысяч картинок. дальше подключается трёхступенчатый пайплайн разметки с помощью людей в яндекс заданиях. 1. проверка наличия знака асессор отвечает, есть ли на изображении нужный знак («да», «нет», «не загрузилась картинка»). это быстрый и дешёвый способ отсеять ошибки предыдущих этапов, не тратя ресурсы на полную разметку ненужных картинок. чтобы на выходе получить, например, 1200 знаков, на вход подаём с запасом — 2000–3000 изображений, иногда больше, если знак редкий. 2. разметка всех знаков на оставшихся изображениях люди размечают прямоугольники вокруг всех дорожных знаков — не только искомого. это важно для обучения детектора: нужны как положительные примеры, так и фоны с другими знаками, чтобы избежать ложных срабатываний. 3. классификация каждого знака каждый размеченный знак показывается отдельно, и асессор выбирает, что это за знак — из палетки с 300+ вариантов. пробовали упрощать интерфейс (группировка по цвету, форме и прочему), но это всё равно остаётся самым трудоёмким этапом. что в итоге сейчас весь пайплайн уже работает в проде. для некоторых знаков, вроде «железнодорожный переезд», удалось собрать 5000 размеченных примеров — больше, чем требовалось. а вот со знаками поворота всё сложнее: классификатор часто путает «влево» и «вправо», из-за чего нужные картинки отсеиваются, и на выходе остаётся по 700–800 примеров. в ближайший месяц планируем дособрать все основные знаки по россии и двинуться в сторону подготовки датасетов в межнаре. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-06T10:24:46+00:00" href="./posts/116.html">2025-06-06 10:24 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как в Яндекс Картах находят редкие дорожные знаки среди миллионов изображений</strong><br><br>Важная задача отдела картопроизводства — улучшать качество детекции дорожных знаков. Один из способов — находить знаки на панорамах, снятых с помощью телефонов или камер безопасности в такси. На момент старта проекта детектор знаков уже был, но он находил не все знаки и с недостаточной полнотой и точностью. <br><br>Сами по себе знаки можно взять хоть из Википедии — там есть список всех 300+ штук. Но модель не тренируется на списке — нужны десятки тысяч изображений с примерами каждого знака в реальных условиях. Вот тут и начинается основная работа.<br><br>Пайплайн, описанный ниже, служит для сужения огромного набора снимков (сотни миллионов) до относительно небольшого (сотни тысяч), на которых присутствует искомый нами знак.<br><br><strong>1.  Находим всё, что может быть знаком<br></strong>На имеющемся датасете обучили RT-DETR на единственный класс «дорожный знак». Он «выкручен в полноту»: то есть в трейдофе полнота &lt;-&gt; точность, выбрана именно полнота. Так сделано, потому что предсказания этого детектора — кандидаты на проверку. На выходе из этого этапа получается много «кропов-кандидатов» — кусочков исходного снимка, на котором представлено что-то похожее на знак. <br><br><strong>2. Классификация кропов<br></strong>Дальше в дело вступает few-shot-классификатор на основе нашей картиночной тушки — большой свёрточной сети, которую разрабатывает Служба компьютерного зрения. Из неё взяты «эмбединги похожих» — представление изображений в векторном пространстве, где похожие изображения переходят в близкие вектора. Поверх этих эмбедингов обучены несколько линейных слоёв. В качестве примеров позитивного класса используются 20–30 примеров нужного нам знака. Примеров негативного класса в избытке — их берут из текущего датасета дорожных знаков. В результате получается блок, который умеет отвечать: похоже ли входное изображение (кроп-кандидат) на искомый нами знак. <br><br><strong>3. Классификация снимков<br></strong>Если хотя бы один кроп на снимке прошёл классификацию, мы сохраняем весь снимок. Так из сотен миллионов остаётся от сотни тысяч до миллиона в зависимости от знака. Из полученных кандидатов мы отбираем на разметку лишь малое количество: 2–5–10 тысяч картинок.<br><br>Дальше подключается трёхступенчатый пайплайн разметки с помощью людей в Яндекс Заданиях.<br><br><strong>1. Проверка наличия знака<br></strong>Асессор отвечает, есть ли на изображении нужный знак («да», «нет», «не загрузилась картинка»). Это быстрый и дешёвый способ отсеять ошибки предыдущих этапов, не тратя ресурсы на полную разметку ненужных картинок. Чтобы на выходе получить, например, 1200 знаков, на вход подаём с запасом — 2000–3000 изображений, иногда больше, если знак редкий.<br><br><strong>2. Разметка всех знаков<br></strong>На оставшихся изображениях люди размечают прямоугольники вокруг всех дорожных знаков — не только искомого. Это важно для обучения детектора: нужны как положительные примеры, так и фоны с другими знаками, чтобы избежать ложных срабатываний.<br><br><strong>3. Классификация каждого знака</strong><br>Каждый размеченный знак показывается отдельно, и асессор выбирает, что это за знак — из палетки с 300+ вариантов. Пробовали упрощать интерфейс (группировка по цвету, форме и прочему), но это всё равно остаётся самым трудоёмким этапом.<br><br><strong>Что в итоге</strong><br>Сейчас весь пайплайн уже работает в проде. Для некоторых знаков, вроде «железнодорожный переезд», удалось собрать 5000 размеченных примеров — больше, чем требовалось. А вот со знаками поворота всё сложнее: классификатор часто путает «влево» и «вправо», из-за чего нужные картинки отсеиваются, и на выходе остаётся по 700–800 примеров. В ближайший месяц планируем дособрать все основные знаки по России и двинуться в сторону подготовки датасетов в межнаре.<br><br><a href="https://t.me/+zFlM7fTaTgo0YWQy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/116_480.webp" srcset="../assets/media/thumbs/116_480.webp 480w, ../assets/media/116.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/117_480.webp" srcset="../assets/media/thumbs/117_480.webp 480w, ../assets/media/117.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="116" data-image-index="1" /></div></div>
      <div class="actions">
        <span>2 534 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/116" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/116.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="115" data-search="как яндекс браузер извлекает контент веб-страниц для пересказа? часть ii продолжаем рассказ о суммаризации в яндекс браузере. в первой части речь шла об основной идее и её реализации, а во второй — заключительной — старший ml-разработчик в яндекс браузере михаил катунькин раскроет, как обучали модель. для сбора датасета мы пользовались двумя техниками: асессорскими разметками и синтетическими метками, полученными при помощи yandexgpt. асессору показывали веб-страницу, на которой он мог мышкой выделить блоки, соответствующие основному контенту. таким образом собрали около 7 тысяч размеченных веб-страниц. размеченные данные мы разделили на две части. 2 тысячи примеров использовали в качестве тестового датасета. оставшиеся 5 тысяч применили для дообучения yandexgpt для разметки веб-страниц. при помощи yandexgpt разметили ещё 100 тысяч страниц, и уже на этих данных обучили catboost. последние 100 деревьев в catboost обучались на 5 тысячах примеров, собранных асессорами. чтобы оценить качество извлечения контента, для каждой страницы считалась точность и полнота извлечения текста, а затем проводилось макроусреднение по всему датасету. вариант без доразметки данных при помощи yandexgpt давал точность 88,8% и полноту в 96,3%. доразметка подняла точность до 95,0% при той же полноте. наборы страниц для датасетов получали по следующему принципу: 50% — случайные страницы из интернета, прошедшие классификатор «суммаризируемости»; ещё 50% — случайный сэмпл страниц, на которых пользователи активировали пересказ в браузере. в каждой из выборок важно ограничить число страниц с одного домена, чтобы датасет был достаточно разнообразным. для того, чтобы размечать страницы при помощи yandexgpt, применили следующую технику. html-дерево делится на несколько пересекающихся деревьев меньшего размера, чтобы каждое из них попадало в контекст из 8192 токенов модели. затем к выходным эмбеддингам yandexgpt, соответствующим определённому блоку текста, применяется бинарный классификатор. для тех блоков, которые классифицировали несколько раз из-за перекрытия деревьев, берётся средняя метка. бинарный классификатор, а также lora-адаптер к модели учатся на 5 тысячах страниц, размеченных асессорами. этот подход применим не только для суммаризации страниц. так можно обучать классификаторы и детекторы и для других функций браузера, используя то же самое пространство фичей. ml underhood как яндекс браузер извлекает контент веб-страниц для пересказа? часть ii продолжаем рассказ о суммаризации в яндекс браузере. в первой части речь шла об основной идее и её реализации, а во второй — заключительной — старший ml-разработчик в яндекс браузере михаил катунькин раскроет, как обучали модель. для сбора датасета мы пользовались двумя техниками: асессорскими разметками и синтетическими метками, полученными при помощи yandexgpt. асессору показывали веб-страницу, на которой он мог мышкой выделить блоки, соответствующие основному контенту. таким образом собрали около 7 тысяч размеченных веб-страниц. размеченные данные мы разделили на две части. 2 тысячи примеров использовали в качестве тестового датасета. оставшиеся 5 тысяч применили для дообучения yandexgpt для разметки веб-страниц. при помощи yandexgpt разметили ещё 100 тысяч страниц, и уже на этих данных обучили catboost. последние 100 деревьев в catboost обучались на 5 тысячах примеров, собранных асессорами. чтобы оценить качество извлечения контента, для каждой страницы считалась точность и полнота извлечения текста, а затем проводилось макроусреднение по всему датасету. вариант без доразметки данных при помощи yandexgpt давал точность 88,8% и полноту в 96,3%. доразметка подняла точность до 95,0% при той же полноте. наборы страниц для датасетов получали по следующему принципу: 50% — случайные страницы из интернета, прошедшие классификатор «суммаризируемости»; ещё 50% — случайный сэмпл страниц, на которых пользователи активировали пересказ в браузере. в каждой из выборок важно ограничить число страниц с одного домена, чтобы датасет был достаточно разнообразным. для того, чтобы размечать страницы при помощи yandexgpt, применили следующую технику. html-дерево делится на несколько пересекающихся деревьев меньшего размера, чтобы каждое из них попадало в контекст из 8192 токенов модели. затем к выходным эмбеддингам yandexgpt, соответствующим определённому блоку текста, применяется бинарный классификатор. для тех блоков, которые классифицировали несколько раз из-за перекрытия деревьев, берётся средняя метка. бинарный классификатор, а также lora-адаптер к модели учатся на 5 тысячах страниц, размеченных асессорами. этот подход применим не только для суммаризации страниц. так можно обучать классификаторы и детекторы и для других функций браузера, используя то же самое пространство фичей. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-06-03T11:57:26+00:00" href="./posts/115.html">2025-06-03 11:57 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как Яндекс Браузер извлекает контент веб-страниц для пересказа? Часть II</strong><br><br>Продолжаем рассказ о суммаризации в Яндекс Браузере. В <a href="https://t.me/MLunderhood/112" rel="nofollow noopener noreferrer">первой части</a> речь шла об основной идее и её реализации, а во второй — заключительной — старший ML-разработчик в Яндекс Браузере Михаил Катунькин раскроет, как обучали модель. <br><br>Для сбора датасета мы пользовались двумя техниками: асессорскими разметками и синтетическими метками, полученными при помощи YandexGPT.<br><br>Асессору показывали веб-страницу, на которой он мог мышкой выделить блоки, соответствующие основному контенту. Таким образом собрали около 7 тысяч размеченных веб-страниц.<br><br>Размеченные данные мы разделили на две части. 2 тысячи примеров использовали в качестве тестового датасета. Оставшиеся 5 тысяч применили для дообучения YandexGPT для разметки веб-страниц.<br><br>При помощи YandexGPT разметили ещё 100 тысяч страниц, и уже на этих данных обучили Catboost. Последние 100 деревьев в Catboost обучались на 5 тысячах примеров, собранных асессорами.<br><br>Чтобы оценить качество извлечения контента, для каждой страницы считалась точность и полнота извлечения текста, а затем проводилось макроусреднение по всему датасету. Вариант без доразметки данных при помощи YandexGPT давал точность 88,8% и полноту в 96,3%. Доразметка подняла точность до 95,0% при той же полноте.<br><br>Наборы страниц для датасетов получали по следующему принципу: 50% — случайные страницы из интернета, прошедшие классификатор «суммаризируемости»; ещё 50% — случайный сэмпл страниц, на которых пользователи активировали пересказ в браузере. В каждой из выборок важно ограничить число страниц с одного домена, чтобы датасет был достаточно разнообразным.<br><br>Для того, чтобы размечать страницы при помощи YandexGPT, применили следующую технику. HTML-дерево делится на несколько пересекающихся деревьев меньшего размера, чтобы каждое из них попадало в контекст из 8192 токенов модели. Затем к выходным эмбеддингам YandexGPT, соответствующим определённому блоку текста, применяется бинарный классификатор. Для тех блоков, которые классифицировали несколько раз из-за перекрытия деревьев, берётся средняя метка. Бинарный классификатор, а также LoRA-адаптер к модели учатся на 5 тысячах страниц, размеченных асессорами.<br><br>Этот подход применим не только для суммаризации страниц. Так можно обучать классификаторы и детекторы и для других функций Браузера, используя то же самое пространство фичей. <br><br><a href="https://t.me/+5hJqA2HLfLZkODEy" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 136 просмотров · 16 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/115" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/115.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="114" data-search="исследователи яндекса выложили в опенсорс yambda — датасет на 5 млрд событий в открытом доступе появился yandex music billion-interactions dataset (yambda) — один из крупнейших в мире датасетов в области рекомендательных систем. в этом посте рассказываем, зачем он нужен и какие у него ключевые особенности. в последние годы рекомендации вышли на плато по сравнению с более быстро развивающимся областями, такими как llm. исследователям недоступны терабайты данных, которые нужны для развития рекомендательных систем, а коммерческие платформы редко делятся данными. поэтому приходится использовать устаревшие и маленькие наборы. модели, обученные на таких данных, теряют эффективность при масштабировании. существующие доступные датасеты, такие как movielens, netflix prize dataset, amazon reviews, music4all-onion, steam и несколько других имеют ряд недостатков. например, сравнительно небольшой размер делает их нерепрезентативным для коммерческих масштабов, а фокус на явных сигналах ограничивает полезность для моделирования реальных последовательных взаимодействий. чтобы решить эти проблемы и дать исследователям больше возможностей для разработки и тестирования новых гипотез в рекомендациях, исследователи яндекса выложили в опенсорс свой датасет yambda. ключевые особенности yambda: — содержит 4,79 млрд обезличенных взаимодействий пользователей с музыкальными треками в яндекс музыке. — есть три версии: полная (5 млрд событий) и уменьшенные (500 млн и 50 млн событий). — включает два основных типа взаимодействий: неявную обратную связь (прослушивания) и явную обратную связь (лайки, дизлайки, анлайки и андизлайки). — для большинства треков есть нейросетевые вектора, сгенерированные с помощью свёрточной нейронной сети (cnn), что позволяет учитывать некоторые характеристики музыкальных треков. — включены анонимизированные признаки метаданных треков, такие как длительность, содержание вложений, исполнитель и альбом. — каждое событие помечено флагом is_organic, который позволяет различать органические действия пользователей и действия, вызванные рекомендациями алгоритма. — все события имеют временные метки, что позволяет проводить анализ временных последовательностей и оценивать алгоритмы в условиях, приближённых к реальным. — данные распределены в формате apache parquet, что обеспечивает совместимость с распределёнными системами обработки данных (например, hadoop, spark) и современными аналитическими инструментами (например, polars, pandas). методы оценки в отличие от метода leave-one-out (loo), который исключает последнее положительное взаимодействие пользователя из обучающей выборки для предсказания, yambda-5b использует глобальный временной сплит (global temporal split, gts). преимущество gts в том, что он сохраняет временную последовательность событий, предотвращая нарушение временных зависимостей между тренировочным и тестовым наборами данных. это позволяет более точно оценить, как модель будет работать в реальных условиях, когда доступ к будущим данным ограничен или невозможен. вместе с датасетом представлены baseline-алгоритмы (mostpop, decaypop, itemknn, ials, bpr, sansa, sasrec). они служат отправной точкой для сравнения эффективности новых подходов в области рекомендательных систем. используются следующие метрики: — ndcg@k (normalized discounted cumulative gain) — оценивает качество ранжирования рекомендаций. — recall@k — измеряет способность алгоритма генерировать релевантные рекомендации из общего набора возможных рекомендаций. — coverage@k — показывает, насколько широко представлен каталог элементов в рекомендации. датасет и код для оценочных бейзлайнов уже доступны на hugging face, а статья — на arxiv. статью подготовили ❣ александр плошкин, владислав тыцкий, алексей письменный, владимир байкалов, евгений тайчинов, артём пермяков, даниил бурлаков, евгений крофто, николай савушкин @recsyschannel исследователи яндекса выложили в опенсорс yambda — датасет на 5 млрд событий в открытом доступе появился yandex music billion-interactions dataset (yambda) — один из крупнейших в мире датасетов в области рекомендательных систем. в этом посте рассказываем, зачем он нужен и какие у него ключевые особенности. в последние годы рекомендации вышли на плато по сравнению с более быстро развивающимся областями, такими как llm. исследователям недоступны терабайты данных, которые нужны для развития рекомендательных систем, а коммерческие платформы редко делятся данными. поэтому приходится использовать устаревшие и маленькие наборы. модели, обученные на таких данных, теряют эффективность при масштабировании. существующие доступные датасеты, такие как movielens, netflix prize dataset, amazon reviews, music4all-onion, steam и несколько других имеют ряд недостатков. например, сравнительно небольшой размер делает их нерепрезентативным для коммерческих масштабов, а фокус на явных сигналах ограничивает полезность для моделирования реальных последовательных взаимодействий. чтобы решить эти проблемы и дать исследователям больше возможностей для разработки и тестирования новых гипотез в рекомендациях, исследователи яндекса выложили в опенсорс свой датасет yambda. ключевые особенности yambda: — содержит 4,79 млрд обезличенных взаимодействий пользователей с музыкальными треками в яндекс музыке. — есть три версии: полная (5 млрд событий) и уменьшенные (500 млн и 50 млн событий). — включает два основных типа взаимодействий: неявную обратную связь (прослушивания) и явную обратную связь (лайки, дизлайки, анлайки и андизлайки). — для большинства треков есть нейросетевые вектора, сгенерированные с помощью свёрточной нейронной сети (cnn), что позволяет учитывать некоторые характеристики музыкальных треков. — включены анонимизированные признаки метаданных треков, такие как длительность, содержание вложений, исполнитель и альбом. — каждое событие помечено флагом is_organic, который позволяет различать органические действия пользователей и действия, вызванные рекомендациями алгоритма. — все события имеют временные метки, что позволяет проводить анализ временных последовательностей и оценивать алгоритмы в условиях, приближённых к реальным. — данные распределены в формате apache parquet, что обеспечивает совместимость с распределёнными системами обработки данных (например, hadoop, spark) и современными аналитическими инструментами (например, polars, pandas). методы оценки в отличие от метода leave-one-out (loo), который исключает последнее положительное взаимодействие пользователя из обучающей выборки для предсказания, yambda-5b использует глобальный временной сплит (global temporal split, gts). преимущество gts в том, что он сохраняет временную последовательность событий, предотвращая нарушение временных зависимостей между тренировочным и тестовым наборами данных. это позволяет более точно оценить, как модель будет работать в реальных условиях, когда доступ к будущим данным ограничен или невозможен. вместе с датасетом представлены baseline-алгоритмы (mostpop, decaypop, itemknn, ials, bpr, sansa, sasrec). они служат отправной точкой для сравнения эффективности новых подходов в области рекомендательных систем. используются следующие метрики: — ndcg@k (normalized discounted cumulative gain) — оценивает качество ранжирования рекомендаций. — recall@k — измеряет способность алгоритма генерировать релевантные рекомендации из общего набора возможных рекомендаций. — coverage@k — показывает, насколько широко представлен каталог элементов в рекомендации. датасет и код для оценочных бейзлайнов уже доступны на hugging face , а статья — на arxiv . статью подготовили ❣ александр плошкин, владислав тыцкий, алексей письменный, владимир байкалов, евгений тайчинов, артём пермяков, даниил бурлаков, евгений крофто, николай савушкин @recsyschannel">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-29T08:15:22+00:00" href="./posts/114.html">2025-05-29 08:15 UTC</a></div>
      </div>
      <div class="post-body"><strong>Исследователи Яндекса выложили в опенсорс Yambda — датасет на 5 млрд событий</strong><br><br>В открытом доступе <a href="https://huggingface.co/datasets/yandex/yambda" rel="nofollow noopener noreferrer">появился</a> Yandex Music Billion-Interactions Dataset (Yambda) — один из крупнейших в мире датасетов в области рекомендательных систем. В этом посте рассказываем, зачем он нужен и какие у него ключевые особенности. <br><br>В последние годы рекомендации вышли на плато по сравнению с более быстро развивающимся областями, такими как LLM. Исследователям недоступны терабайты данных, которые нужны для развития рекомендательных систем, а коммерческие платформы редко делятся данными. Поэтому приходится использовать устаревшие и маленькие наборы. Модели, обученные на таких данных, теряют эффективность при масштабировании.<br><br>Существующие доступные датасеты, такие как MovieLens, Netflix Prize dataset, Amazon Reviews, Music4All-Onion, Steam и несколько других имеют ряд недостатков. Например, сравнительно небольшой размер делает их нерепрезентативным для коммерческих масштабов, а фокус на явных сигналах ограничивает полезность для моделирования реальных последовательных взаимодействий.<br><br>Чтобы решить эти проблемы и дать исследователям больше возможностей для разработки и тестирования новых гипотез в рекомендациях, исследователи  Яндекса выложили в опенсорс свой датасет Yambda. <br><br><strong>Ключевые особенности Yambda: </strong><br><br>— Содержит 4,79 млрд обезличенных взаимодействий пользователей с музыкальными треками в Яндекс Музыке.<br>— Есть три версии: полная (5 млрд событий) и уменьшенные (500 млн и 50 млн<br>событий).<br>— Включает два основных типа взаимодействий: неявную обратную связь (прослушивания) и явную обратную связь (лайки, дизлайки, анлайки и андизлайки).<br>— Для большинства треков есть нейросетевые вектора, сгенерированные с помощью свёрточной нейронной сети (CNN), что позволяет учитывать некоторые характеристики музыкальных треков.<br>— Включены анонимизированные признаки метаданных треков, такие как длительность, содержание вложений, исполнитель и альбом.<br>— Каждое событие помечено флагом is_organic, который позволяет различать органические действия пользователей и действия, вызванные рекомендациями алгоритма.<br>— Все события имеют временные метки, что позволяет проводить анализ временных последовательностей и оценивать алгоритмы в условиях, приближённых к реальным.<br>— Данные распределены в формате Apache Parquet, что обеспечивает совместимость с распределёнными системами обработки данных (например, Hadoop, Spark) и современными аналитическими инструментами (например, Polars, Pandas).<br><br><strong>Методы оценки</strong><br><br>В отличие от метода Leave-One-Out (LOO), который исключает последнее положительное взаимодействие пользователя из обучающей выборки для предсказания, Yambda-5B использует глобальный временной сплит (Global Temporal Split, GTS). Преимущество GTS в том, что он сохраняет временную последовательность событий, предотвращая нарушение временных зависимостей между тренировочным и тестовым наборами данных. Это позволяет более точно оценить, как модель будет работать в реальных условиях, когда доступ к будущим данным ограничен или невозможен.<br><br>Вместе с датасетом представлены baseline-алгоритмы (MostPop, DecayPop, ItemKNN, iALS, BPR, SANSA, SASRec). Они служат отправной точкой для сравнения эффективности новых подходов в области рекомендательных систем.<br><br>Используются следующие метрики:<br><br>— NDCG@k (Normalized Discounted Cumulative Gain) — оценивает качество ранжирования рекомендаций.<br>— Recall@k — измеряет способность алгоритма генерировать релевантные рекомендации из общего набора возможных рекомендаций.<br>— Coverage@k — показывает, насколько широко представлен каталог элементов в рекомендации.<br><br>Датасет и код для оценочных бейзлайнов уже доступны на <a href="https://huggingface.co/datasets/yandex/yambda" rel="nofollow noopener noreferrer">Hugging Face</a>, а статья — <a href="https://arxiv.org/abs/2505.22238" rel="nofollow noopener noreferrer">на arXiv</a>. <br><br>Статью подготовили <em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> </em>Александр Плошкин, Владислав Тыцкий, Алексей Письменный, Владимир Байкалов, Евгений Тайчинов, Артём Пермяков, Даниил Бурлаков, Евгений Крофто, Николай Савушкин<br><br>@RecSysChannel<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/114_480.webp" srcset="../assets/media/thumbs/114_480.webp 480w, ../assets/media/114.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="114" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 982 просмотров · 35 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/114" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/114.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="113" data-search="в опенсорсе появился датасет yambda, который поможет исследователям и разработчикам по всему миру тестировать и совершенствовать новые алгоритмы рекомендаций. все подробности читайте в посте. в опенсорсе появился датасет yambda, который поможет исследователям и разработчикам по всему миру тестировать и совершенствовать новые алгоритмы рекомендаций. все подробности читайте в посте.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-29T08:15:21+00:00" href="./posts/113.html">2025-05-29 08:15 UTC</a></div>
      </div>
      <div class="post-body">В опенсорсе появился датасет Yambda, который поможет исследователям и разработчикам по всему миру тестировать и совершенствовать новые алгоритмы рекомендаций. Все подробности читайте в посте.</div>
      <div class="actions">
        <span>1 688 просмотров · 10 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/113" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/113.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="112" data-search="как яндекс браузер извлекает контент веб-страниц для пересказа? часть i представьте, что вам нужно быстро проанализировать текст с десятка веб-страниц. на помощь придет функция краткого пересказа в яндекс браузере. здесь — и не только здесь — работает суммаризация текста. но возникает вопрос: что попадает в качестве входных данных в llm, занимающуюся суммаризацией? на него в двух постах ответит михаил катунькин, старший ml-разработчик в яндекс браузере. в первой части речь пойдёт об общей идее и реализации, а во второй — об обучении модели. общая идея самое очевидное решение — взять html-код страницы. его проблема в том, что верстка содержит в себе много лишней текстовой информации, которая «засорит» и сильно увеличит контекст модели: теги, скрипты, стили. всё это приведёт к ухудшению качества пересказа, большему времени работы модели и удорожанию инференса. значит, следующий шаг — извлечь только текстовую информацию. это улучшит ситуацию, но в контекст модели по-прежнему будет проникать много лишнего: реклама, меню, комментарии. а из-за того, что модель потеряет информацию о структуре страницы, в пересказ начнёт попадать, например, содержание статей из блока рекомендаций похожего контента. можно использовать разные эвристики, чтобы извлекать не весь текст страницы, а только полезный. такой подход, например, используется в режиме чтения в браузере однако из-за разнообразия верстки сайтов эта техника не является универсальной, нуждается в переподборе эвристик с течением времени, даёт плохое качество извлечения текста на определённых доменах. мы решили извлекать основной контент страницы при помощи ml-модели. а затем использовать информацию из html-кода для задания структуры текста: заголовков, подзаголовков, ссылок, выделений. важный нюанс: пересказ должен работать быстро даже при медленном интернете. передавать тяжелые html-страницы целиком на серверы яндекса было бы плохим решением. поэтому модель следовало сделать легковесной, чтобы она работала непосредственно на устройстве пользователей. реализация html представляет собой дерево тегов. блоки с текстом на странице — это листья в дереве. будем для каждого листа принимать решение: брать его или нет в конечный текст. в качестве бинарного классификатора возьмем catboost. классификация осуществляется на основе множества статистик, посчитанных при обходе дерева: по тегам, атрибутам тегов, текстам. существенное улучшение качества даёт следующий трюк: блоки с текстом классифицируется не независимо, а группами с одинаковым путём из тегов и атрибутов от блока до корня. при этом для хорошего качества извлечения текста модели достаточно информации о структуре разметки и базовых статистик по текстам. использование семантических вектор-признаков, посчитанных по текстам, заметного улучшения качества суммаризации не даёт. поэтому в продакшен-версии от них решили отказаться ради скорости работы модели. было важно оптимизировать код подсчёта признаков для модели, из-за того, что деревья тегов могут содержать сотни тысяч узлов. в итоговой версии код реализовали на c++. он работает 40 мс на одно извлечение в 90 перцентили. для сравнения, код на js с эвристиками из режима чтения в mozilla работает 1,125 с в 90 перцентили. для достижения этого результата, в частности, мы оптимизировали динамические выделения памяти, а также удалили из набора признаков сложные в вычислении, но не столь значимые. из html-дерева с проклассифицироваными листьями уже можно получить очищенный текст в формате markdown. для этого по дереву идёт обход в глубину, собирающий текст из листьев, прошедших порог классификации, и учитывающий наличие тегов с заголовками, ссылками, выделениями и прочим. ml underhood как яндекс браузер извлекает контент веб-страниц для пересказа? часть i представьте, что вам нужно быстро проанализировать текст с десятка веб-страниц. на помощь придет функция краткого пересказа в яндекс браузере. здесь — и не только здесь — работает суммаризация текста. но возникает вопрос: что попадает в качестве входных данных в llm, занимающуюся суммаризацией? на него в двух постах ответит михаил катунькин, старший ml-разработчик в яндекс браузере. в первой части речь пойдёт об общей идее и реализации, а во второй — об обучении модели. общая идея самое очевидное решение — взять html-код страницы. его проблема в том, что верстка содержит в себе много лишней текстовой информации, которая «засорит» и сильно увеличит контекст модели: теги, скрипты, стили. всё это приведёт к ухудшению качества пересказа, большему времени работы модели и удорожанию инференса. значит, следующий шаг — извлечь только текстовую информацию. это улучшит ситуацию, но в контекст модели по-прежнему будет проникать много лишнего: реклама, меню, комментарии. а из-за того, что модель потеряет информацию о структуре страницы, в пересказ начнёт попадать, например, содержание статей из блока рекомендаций похожего контента. можно использовать разные эвристики, чтобы извлекать не весь текст страницы, а только полезный. такой подход, например, используется в режиме чтения в браузере однако из-за разнообразия верстки сайтов эта техника не является универсальной, нуждается в переподборе эвристик с течением времени, даёт плохое качество извлечения текста на определённых доменах. мы решили извлекать основной контент страницы при помощи ml-модели. а затем использовать информацию из html-кода для задания структуры текста: заголовков, подзаголовков, ссылок, выделений. важный нюанс: пересказ должен работать быстро даже при медленном интернете. передавать тяжелые html-страницы целиком на серверы яндекса было бы плохим решением. поэтому модель следовало сделать легковесной, чтобы она работала непосредственно на устройстве пользователей. реализация html представляет собой дерево тегов. блоки с текстом на странице — это листья в дереве. будем для каждого листа принимать решение: брать его или нет в конечный текст. в качестве бинарного классификатора возьмем catboost. классификация осуществляется на основе множества статистик, посчитанных при обходе дерева: по тегам, атрибутам тегов, текстам. существенное улучшение качества даёт следующий трюк: блоки с текстом классифицируется не независимо, а группами с одинаковым путём из тегов и атрибутов от блока до корня. при этом для хорошего качества извлечения текста модели достаточно информации о структуре разметки и базовых статистик по текстам. использование семантических вектор-признаков, посчитанных по текстам, заметного улучшения качества суммаризации не даёт. поэтому в продакшен-версии от них решили отказаться ради скорости работы модели. было важно оптимизировать код подсчёта признаков для модели, из-за того, что деревья тегов могут содержать сотни тысяч узлов. в итоговой версии код реализовали на c++. он работает 40 мс на одно извлечение в 90 перцентили. для сравнения, код на js с эвристиками из режима чтения в mozilla работает 1,125 с в 90 перцентили. для достижения этого результата, в частности, мы оптимизировали динамические выделения памяти, а также удалили из набора признаков сложные в вычислении, но не столь значимые. из html-дерева с проклассифицироваными листьями уже можно получить очищенный текст в формате markdown. для этого по дереву идёт обход в глубину, собирающий текст из листьев, прошедших порог классификации, и учитывающий наличие тегов с заголовками, ссылками, выделениями и прочим. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-28T12:56:24+00:00" href="./posts/112.html">2025-05-28 12:56 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как Яндекс Браузер извлекает контент веб-страниц для пересказа? Часть I</strong><br><br>Представьте, что вам нужно быстро проанализировать текст с десятка веб-страниц. На помощь придет функция <a href="https://300.ya.ru/" rel="nofollow noopener noreferrer">краткого пересказа</a> в Яндекс Браузере. Здесь — и не только здесь — работает суммаризация текста. Но возникает вопрос: что попадает в качестве входных данных в LLM, занимающуюся суммаризацией? На него в двух постах ответит Михаил Катунькин, старший ML-разработчик в Яндекс Браузере. В первой части речь пойдёт об общей идее и реализации, а во второй — об обучении модели. <br><br><strong>Общая идея</strong><br><br>Самое очевидное решение — взять HTML-код страницы. Его проблема в том, что верстка содержит в себе много лишней текстовой информации, которая «засорит» и сильно увеличит контекст модели: теги, скрипты, стили. Всё это приведёт к ухудшению качества пересказа, большему времени работы модели и удорожанию инференса.<br><br>Значит, следующий шаг — извлечь только текстовую информацию. Это улучшит ситуацию, но в контекст модели по-прежнему будет проникать много лишнего: реклама, меню, комментарии. А из-за того, что модель потеряет информацию о структуре страницы, в пересказ начнёт попадать, например, содержание статей из блока рекомендаций похожего контента.<br><br>Можно использовать разные эвристики, чтобы извлекать не весь текст страницы, а только полезный. Такой подход, например, используется в <a href="https://github.com/mozilla/readability" rel="nofollow noopener noreferrer">режиме чтения в браузере</a> Однако из-за разнообразия верстки сайтов эта техника не является универсальной, нуждается в переподборе эвристик с течением времени, даёт плохое качество извлечения текста на определённых доменах.<br><br>Мы решили извлекать основной контент страницы при помощи ML-модели. А затем использовать информацию из HTML-кода для задания структуры текста: заголовков, подзаголовков, ссылок, выделений.<br><br>Важный нюанс: пересказ должен работать быстро даже при медленном интернете. Передавать тяжелые HTML-страницы целиком на серверы Яндекса было бы плохим решением. Поэтому модель следовало сделать легковесной, чтобы она работала непосредственно на устройстве пользователей.<br><br><strong>Реализация</strong><br><br>HTML представляет собой дерево тегов. Блоки с текстом на странице — это листья в дереве. Будем для каждого листа принимать решение: брать его или нет в конечный текст. В качестве бинарного классификатора возьмем Catboost. <br><br>Классификация осуществляется на основе множества статистик, посчитанных при обходе дерева: по тегам, атрибутам тегов, текстам. Существенное улучшение качества даёт следующий трюк: блоки с текстом классифицируется не независимо, а группами с одинаковым путём из тегов и атрибутов от блока до корня. При этом для хорошего качества извлечения текста модели достаточно информации о структуре разметки и базовых статистик по текстам. <br><br>Использование семантических вектор-признаков, посчитанных по текстам, заметного улучшения качества суммаризации не даёт. Поэтому в продакшен-версии от них решили отказаться ради скорости работы модели.<br><br>Было важно оптимизировать код подсчёта признаков для модели, из-за того, что деревья тегов могут содержать сотни тысяч узлов. В итоговой версии код реализовали на C++. Он работает 40 мс на одно извлечение в 90 перцентили. Для сравнения, код на JS с эвристиками из режима чтения в Mozilla работает 1,125 с в 90 перцентили. Для достижения этого результата, в частности, мы оптимизировали динамические выделения памяти, а также удалили из набора признаков сложные в вычислении, но не столь значимые.<br><br>Из HTML-дерева с проклассифицироваными листьями уже можно получить очищенный текст в формате markdown. Для этого по дереву идёт обход в глубину, собирающий текст из листьев, прошедших порог классификации, и учитывающий наличие тегов с заголовками, ссылками, выделениями и прочим.<br><br><a href="https://t.me/+wYt6RVTtZTc2MWJi" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 118 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/112" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/112.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="111" data-search="как llm помогают анализировать ответы в опросах вы когда-нибудь задумывались, как исследователи анализируют результаты опросов? с закрытыми вопросами, у которых есть несколько вариантов ответа, всё достаточно просто — алгоритмы суммаризации существуют давно. но что насчёт анализа открытых вопросов, на которые респонденты отвечают в свободной форме? это кропотливый и изнурительный труд, ведь приходится вручную обрабатывать сотни, а то и тысячи ответов. к счастью, llm может помочь и здесь. ведущий исследователь интеграции ии и ux в поиске яндекса алексей шипулин рассказал нашему каналу о созданном им телеграм-боте, который помогает анализировать ответы на открытые вопросы и экономит массу времени. «под капотом» у бота сразу несколько моделей. первая — помощнее — читает все ответы, ищет близкие по смыслу и составляет некоторое количество категорий. дальше в дело вступает модель послабее, которая тоже знакомится с ответами и распределяет их по созданным ранее категориям. чтобы процесс был прозрачнее для исследователя, модель комментирует каждый ответ, объясняя, почему он попал в ту или иную группу. тут важно ещё и то, что llm знают не только ответы, но и вопросы — это положительно сказывается на категоризации. казалось бы, на этом можно и заканчивать, но нет. вторая модель может совершать ошибки, относя ответ не к той категории, где ему следует быть. проверкой занимается третья llm — она оценивает ответы на соответствие категории по трёхбалльной шкале. те ответы, которые получили оценку «два» или ниже, снова проходят через второй этап и распределяются по другим категориям. потом опять проверка и опять перераспределение, если нужно. на финальном этапе ответы распределяются по частотности. самые редкие алгоритм предлагает не относить в отдельные категории, чтобы не размывать статистику. а делать это или нет — решает исследователь. весь процесс, на который человек мог бы убить целый день, занимает не более трёх минут — и на выходе получается наглядный график и таблица с ответами. всего же с момента запуска в октябре телеграм-бот сэкономил исследователям яндекса уже 12 лет! ml underhood как llm помогают анализировать ответы в опросах вы когда-нибудь задумывались, как исследователи анализируют результаты опросов? с закрытыми вопросами, у которых есть несколько вариантов ответа, всё достаточно просто — алгоритмы суммаризации существуют давно. но что насчёт анализа открытых вопросов, на которые респонденты отвечают в свободной форме? это кропотливый и изнурительный труд, ведь приходится вручную обрабатывать сотни, а то и тысячи ответов. к счастью, llm может помочь и здесь. ведущий исследователь интеграции ии и ux в поиске яндекса алексей шипулин рассказал нашему каналу о созданном им телеграм-боте, который помогает анализировать ответы на открытые вопросы и экономит массу времени. «под капотом» у бота сразу несколько моделей. первая — помощнее — читает все ответы, ищет близкие по смыслу и составляет некоторое количество категорий. дальше в дело вступает модель послабее, которая тоже знакомится с ответами и распределяет их по созданным ранее категориям. чтобы процесс был прозрачнее для исследователя, модель комментирует каждый ответ, объясняя, почему он попал в ту или иную группу. тут важно ещё и то, что llm знают не только ответы, но и вопросы — это положительно сказывается на категоризации. казалось бы, на этом можно и заканчивать, но нет. вторая модель может совершать ошибки, относя ответ не к той категории, где ему следует быть. проверкой занимается третья llm — она оценивает ответы на соответствие категории по трёхбалльной шкале. те ответы, которые получили оценку «два» или ниже, снова проходят через второй этап и распределяются по другим категориям. потом опять проверка и опять перераспределение, если нужно. на финальном этапе ответы распределяются по частотности. самые редкие алгоритм предлагает не относить в отдельные категории, чтобы не размывать статистику. а делать это или нет — решает исследователь. весь процесс, на который человек мог бы убить целый день, занимает не более трёх минут — и на выходе получается наглядный график и таблица с ответами. всего же с момента запуска в октябре телеграм-бот сэкономил исследователям яндекса уже 12 лет! ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-20T11:08:29+00:00" href="./posts/111.html">2025-05-20 11:08 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как LLM помогают анализировать ответы в опросах </strong><br><br>Вы когда-нибудь задумывались, как исследователи анализируют результаты опросов? С закрытыми вопросами, у которых есть несколько вариантов ответа, всё достаточно просто — алгоритмы суммаризации существуют давно. Но что насчёт анализа открытых вопросов, на которые респонденты отвечают в свободной форме? Это кропотливый и изнурительный труд, ведь приходится вручную обрабатывать сотни, а то и тысячи ответов.<br><br>К счастью, LLM может помочь и здесь. Ведущий исследователь интеграции ИИ и UX в Поиске Яндекса Алексей Шипулин рассказал нашему каналу о созданном им телеграм-боте, который помогает анализировать ответы на открытые вопросы и экономит массу времени.  <br><br>«Под капотом» у бота сразу несколько моделей. Первая — помощнее — читает все ответы, ищет близкие по смыслу и составляет некоторое количество категорий. Дальше в дело вступает модель послабее, которая тоже знакомится с ответами и распределяет их по созданным ранее категориям. Чтобы процесс был прозрачнее для исследователя, модель комментирует каждый ответ, объясняя, почему он попал в ту или иную группу. Тут важно ещё и то, что LLM знают не только ответы, но и вопросы — это положительно сказывается на категоризации. <br><br>Казалось бы, на этом можно и заканчивать, но нет. Вторая модель может совершать ошибки, относя ответ не к той категории, где ему следует быть. Проверкой занимается третья LLM — она оценивает ответы на соответствие категории по трёхбалльной шкале. Те ответы, которые получили оценку «два» или ниже, снова проходят через второй этап и распределяются по другим категориям. Потом опять проверка и опять перераспределение, если нужно. <br><br>На финальном этапе ответы распределяются по частотности. Самые редкие алгоритм предлагает не относить в отдельные категории, чтобы не размывать статистику. А делать это или нет — решает исследователь. <br><br>Весь процесс, на который человек мог бы убить целый день, занимает не более трёх минут — и на выходе получается наглядный график и таблица с ответами. Всего же с момента запуска в октябре телеграм-бот сэкономил исследователям Яндекса уже 12 лет!<br><br><a href="https://t.me/+zFlM7fTaTgo0YWQy" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>3 205 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/111" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/111.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="110" data-search="как llm помогает быстрее находить товары на складе маркета часто на складских полках рядом оказываются очень похожие товары. это приводит к тому, что при сборке заказов товары приходится долго искать: сборщик берёт не то, смотрит, кладёт обратно, продолжает искать нужное. на таких «плохих» полках, где лежат визуально похожие товары (например, шторки одного цвета, но разных моделей) шанс вытащить искомое с первого раза не самый большой. в среднем на поиск нужной позиции теряется 4 секунды. а если таких операций полмиллиона в день, масштабы становятся внушительными. екатерина трофимова, менеджер продуктов в команде логистики маркета, рассказала, как с помощью llm получилось оптимизировать хранение товаров на складе с учётом их похожести для более быстрого поиска. до начала эксперимента у команды не было системного признака схожести товаров — их просто складывали, как придётся. попробовали сформулировать фановую гипотезу: если системно понимать, какие товары похожи, можно оптимизировать процесс. мы применили технологию векторизации текста к названиям товаров, чтобы получить уникальный вектор, описывающий каждый товар. с её помощью считается и сравнивается похожесть того, что кладётся на полку, и того, что уже лежит на ней. все наименования и метаданные товаров прогнали через yandexgpt и получили вектора, которые легко сравнить и посмотреть косинусную близость между ними. если значение выше порогового — считаем, что товары слишком похожи и не даём класть их рядом. если ниже — можно класть на одну полку. пример на картинке. органайзеры для специй разных производителей мы не дадим положить вместе, потому что их вектора очень близки друг к другу. сменный блок и тетрадь можно положить рядом, потому что у них безопасное пороговое значение, хоть эти вектора тоже находятся относительно близко друг к другу. каша с черникой могут отправиться как к органайзерам, так и к бумажной продукции. помимо llm, пробовали использовать другие решения, например расстояние левенштейна (метрика для измерения различий между двумя строками) — но это не сработало. в нашем случае названия товаров могут сильно различаться по форме, даже если они описывают один и тот же продукт, поэтому расстояние левенштейна будет большим для схожих товаров. gpt справляется с задачей лучше: вектора для таких товаров близки, даже если названия выглядят совсем по-разному. в среднем, новый подход даёт выигрыш в 2,5 секунды на поиск товара, что приводит к экономии около 2 миллионов рублей в месяц на масштабе всех фулфилментов маркета. точность применения метода мы субъективно оцениваем в 7 из 10 — можно было выиграть ещё 1,5 секунды, за счет более агрессивного значения схожести товаров, но чтобы не ловить упячки в духе «кастрюля схожа с вилкой, не клади их вместе», мы ограничились безопасным значением. проект работает в проде с июня 2024 года. мы внедрили его на бэкенде wms (системы управления складом), и в течение 30 дней после внедрения увидели, что эффект стабильно сохраняется. вся реализация решения от проверки гипотезы до внедрения в прод заняла один человеко-месяц разработки. это подтверждает гипотезу, что не всегда нужно строить космолет — иногда можно сделать быстро, просто и полезно. можно сделать вывод, что llm — это не только генерация, классификация и другие привычные задачи. иногда стоит взглянуть на модель под другим углом — например, в этом случае её использовали как числовой преобразователь и нашли решение бизнес-задачи. ml underhood как llm помогает быстрее находить товары на складе маркета часто на складских полках рядом оказываются очень похожие товары. это приводит к тому, что при сборке заказов товары приходится долго искать: сборщик берёт не то, смотрит, кладёт обратно, продолжает искать нужное. на таких «плохих» полках, где лежат визуально похожие товары (например, шторки одного цвета, но разных моделей) шанс вытащить искомое с первого раза не самый большой. в среднем на поиск нужной позиции теряется 4 секунды. а если таких операций полмиллиона в день, масштабы становятся внушительными. екатерина трофимова, менеджер продуктов в команде логистики маркета, рассказала, как с помощью llm получилось оптимизировать хранение товаров на складе с учётом их похожести для более быстрого поиска. до начала эксперимента у команды не было системного признака схожести товаров — их просто складывали, как придётся. попробовали сформулировать фановую гипотезу: если системно понимать, какие товары похожи, можно оптимизировать процесс. мы применили технологию векторизации текста к названиям товаров, чтобы получить уникальный вектор, описывающий каждый товар. с её помощью считается и сравнивается похожесть того, что кладётся на полку, и того, что уже лежит на ней. все наименования и метаданные товаров прогнали через yandexgpt и получили вектора, которые легко сравнить и посмотреть косинусную близость между ними. если значение выше порогового — считаем, что товары слишком похожи и не даём класть их рядом. если ниже — можно класть на одну полку. пример на картинке. органайзеры для специй разных производителей мы не дадим положить вместе, потому что их вектора очень близки друг к другу. сменный блок и тетрадь можно положить рядом, потому что у них безопасное пороговое значение, хоть эти вектора тоже находятся относительно близко друг к другу. каша с черникой могут отправиться как к органайзерам, так и к бумажной продукции. помимо llm, пробовали использовать другие решения, например расстояние левенштейна (метрика для измерения различий между двумя строками) — но это не сработало. в нашем случае названия товаров могут сильно различаться по форме, даже если они описывают один и тот же продукт, поэтому расстояние левенштейна будет большим для схожих товаров. gpt справляется с задачей лучше: вектора для таких товаров близки, даже если названия выглядят совсем по-разному. в среднем, новый подход даёт выигрыш в 2,5 секунды на поиск товара, что приводит к экономии около 2 миллионов рублей в месяц на масштабе всех фулфилментов маркета. точность применения метода мы субъективно оцениваем в 7 из 10 — можно было выиграть ещё 1,5 секунды, за счет более агрессивного значения схожести товаров, но чтобы не ловить упячки в духе «кастрюля схожа с вилкой, не клади их вместе», мы ограничились безопасным значением. проект работает в проде с июня 2024 года. мы внедрили его на бэкенде wms (системы управления складом), и в течение 30 дней после внедрения увидели, что эффект стабильно сохраняется. вся реализация решения от проверки гипотезы до внедрения в прод заняла один человеко-месяц разработки. это подтверждает гипотезу, что не всегда нужно строить космолет — иногда можно сделать быстро, просто и полезно. можно сделать вывод, что llm — это не только генерация, классификация и другие привычные задачи. иногда стоит взглянуть на модель под другим углом — например, в этом случае её использовали как числовой преобразователь и нашли решение бизнес-задачи. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-05-12T11:28:06+00:00" href="./posts/110.html">2025-05-12 11:28 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как LLM помогает быстрее находить товары на складе Маркета</strong><br><br>Часто на складских полках рядом оказываются очень похожие товары. Это приводит к тому, что при сборке заказов товары приходится долго искать: сборщик берёт не то, смотрит, кладёт обратно, продолжает искать нужное. На таких «плохих» полках, где лежат визуально похожие товары (например, шторки одного цвета, но разных моделей) шанс вытащить искомое с первого раза не самый большой. В среднем на поиск нужной позиции теряется 4 секунды. А если таких операций полмиллиона в день, масштабы становятся внушительными.<br><br>Екатерина Трофимова, менеджер продуктов в команде логистики Маркета, рассказала, как с помощью LLM получилось оптимизировать хранение товаров на складе с учётом их похожести для более быстрого поиска. <br><br>До начала эксперимента у команды не было системного признака схожести товаров — их просто складывали, как придётся. Попробовали сформулировать фановую гипотезу: если системно понимать, какие товары похожи, можно оптимизировать процесс. <br><br>Мы применили технологию векторизации текста к названиям товаров, чтобы получить уникальный вектор, описывающий каждый товар. С её помощью считается и сравнивается похожесть того, что кладётся на полку, и того, что уже лежит на ней. Все наименования и метаданные товаров прогнали через YandexGPT и получили вектора, которые легко сравнить и посмотреть косинусную близость между ними. Если значение выше порогового — считаем, что товары слишком похожи и не даём класть их рядом. Если ниже — можно класть на одну полку.<br><br>Пример на картинке. Органайзеры для специй разных производителей мы не дадим положить вместе, потому что их вектора очень близки друг к другу. Сменный блок и тетрадь можно положить рядом, потому что у них безопасное пороговое значение, хоть эти вектора тоже находятся относительно близко друг к другу. Каша с черникой могут отправиться как к органайзерам, так и к бумажной продукции.<br><br>Помимо LLM, пробовали использовать другие решения, например расстояние Левенштейна (метрика для измерения различий между двумя строками) — но это не сработало. В нашем случае названия товаров могут сильно различаться по форме, даже если они описывают один и тот же продукт, поэтому расстояние Левенштейна будет большим для схожих товаров. GPT справляется с задачей лучше: вектора для таких товаров близки, даже если названия выглядят совсем по-разному. <br><br>В среднем, новый подход даёт выигрыш в 2,5 секунды на поиск товара, что приводит к экономии около 2 миллионов рублей в месяц на масштабе всех фулфилментов Маркета. Точность применения метода мы субъективно оцениваем в 7 из 10 — можно было выиграть ещё 1,5 секунды, за счет более агрессивного значения схожести товаров, но чтобы не ловить упячки в духе «кастрюля схожа с вилкой, не клади их вместе», мы ограничились безопасным значением.<br><br>Проект работает в проде с июня 2024 года. Мы внедрили его на бэкенде WMS (системы управления складом), и в течение 30 дней после внедрения увидели, что эффект стабильно сохраняется.<br><br>Вся реализация решения от проверки гипотезы до внедрения в прод заняла один человеко-месяц разработки. Это подтверждает гипотезу, что не всегда нужно строить космолет — иногда можно сделать быстро, просто и полезно. <br><br>Можно сделать вывод, что LLM — это не только генерация, классификация и другие привычные задачи. Иногда стоит взглянуть на модель под другим углом — например, в этом случае её использовали как числовой преобразователь и нашли решение бизнес-задачи.<br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/110_480.webp" srcset="../assets/media/thumbs/110_480.webp 480w, ../assets/media/110.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="110" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 632 просмотров · 36 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/110" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/110.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="109" data-search="как алиса видит мир недавно алиса научилась распознавать объекты, показанные через камеру смартфона. в основе этой фичи лежит мультимодальная нейросеть (visual language model, vlm), которая уже используется в поиске по картинкам, умной камере и нейроэксперте. на хабре вышла большая статья о том, как создавали эту модель, а здесь мы кратко расскажем главное. vlm основана на семействе yandexgpt 5. она состоит из llm и картиночного энкодера. vlm получает на вход изображение и произвольную текстовую инструкцию и предсказывает текст — ответ на пользовательский запрос. датасет для претрейна мультимодальной модели состоял из документов, содержащих изображения, текстовых документов, пар «картинка-текст» и ocr-данных. далее в обучении шла стадия sft, а за ней — dpo. vlm адаптировали в алисы. её зрение работает в двух режимах: можно загрузить изображение в чат, а можно включить камеру и показывать ассистенту то, что вы видите. когда алиса получает изображение и запрос, последний отправляется в рефразер, который адаптирует вопрос для поиска в интернете. например, если пользователь просто показывает алисе булгур и спрашивает «сколько варить?», рефразер превращает вопрос в «сколько варить булгур». далее запрос отправляется в интернет. модель собирает всю нужную информацию и выдаёт пользователю ответ (15 минут, если что). а более подробно о том, как устроена vlm, а также об экспериментах и трудностях, которые возникали по ходу обучения, читайте на хабре. ml underhood как алиса видит мир недавно алиса научилась распознавать объекты, показанные через камеру смартфона. в основе этой фичи лежит мультимодальная нейросеть (visual language model, vlm), которая уже используется в поиске по картинкам, умной камере и нейроэксперте. на хабре вышла большая статья о том, как создавали эту модель, а здесь мы кратко расскажем главное. vlm основана на семействе yandexgpt 5. она состоит из llm и картиночного энкодера. vlm получает на вход изображение и произвольную текстовую инструкцию и предсказывает текст — ответ на пользовательский запрос. датасет для претрейна мультимодальной модели состоял из документов, содержащих изображения, текстовых документов, пар «картинка-текст» и ocr-данных. далее в обучении шла стадия sft, а за ней — dpo. vlm адаптировали в алисы. её зрение работает в двух режимах: можно загрузить изображение в чат, а можно включить камеру и показывать ассистенту то, что вы видите. когда алиса получает изображение и запрос, последний отправляется в рефразер, который адаптирует вопрос для поиска в интернете. например, если пользователь просто показывает алисе булгур и спрашивает «сколько варить?», рефразер превращает вопрос в «сколько варить булгур». далее запрос отправляется в интернет. модель собирает всю нужную информацию и выдаёт пользователю ответ (15 минут, если что). а более подробно о том, как устроена vlm, а также об экспериментах и трудностях, которые возникали по ходу обучения, читайте на хабре. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-29T11:23:42+00:00" href="./posts/109.html">2025-04-29 11:23 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как Алиса видит мир</strong><br><br>Недавно Алиса научилась распознавать объекты, показанные через камеру смартфона. В основе этой фичи лежит мультимодальная нейросеть (Visual Language Model, VLM), которая уже используется в Поиске по картинкам, Умной камере и Нейроэксперте. На Хабре <a href="https://habr.com/ru/companies/yandex/articles/904584/" rel="nofollow noopener noreferrer">вышла большая статья</a> о том, как создавали эту модель, а здесь мы кратко расскажем главное. <br><br>VLM основана на семействе YandexGPT 5. Она состоит из LLM и картиночного энкодера. VLM получает на вход изображение и произвольную текстовую инструкцию и предсказывает текст — ответ на пользовательский запрос.<br><br>Датасет для претрейна мультимодальной модели состоял из документов, содержащих изображения, текстовых документов, пар «картинка-текст» и OCR-данных. Далее в обучении шла стадия SFT, а за ней — DPO. <br><br>VLM адаптировали в Алисы. Её зрение работает в двух режимах: можно загрузить изображение в чат, а можно включить камеру и показывать ассистенту то, что вы видите. Когда Алиса получает изображение и запрос, последний отправляется в рефразер, который адаптирует вопрос для поиска в интернете. Например, если пользователь просто показывает Алисе булгур и спрашивает «Сколько варить?», рефразер превращает вопрос в «сколько варить булгур».<br><br>Далее запрос отправляется в интернет. Модель собирает всю нужную информацию и выдаёт пользователю ответ (15 минут, если что).  <br><br>А более подробно о том, как устроена VLM, а также об экспериментах и трудностях, которые возникали по ходу обучения, <a href="https://habr.com/ru/companies/yandex/articles/904584/" rel="nofollow noopener noreferrer">читайте на Хабре. </a><br><br><a href="https://t.me/+yaGEbJ4stulhZmYy" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 587 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/109" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/109.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="108" data-search="синхронный перевод видео в яндекс браузере перевод видео в яндекс браузере появился ещё в 2021 году. сегодня компания представляет новую версию этой технологии, способную сохранять тембр и интонации оригинального голоса. а сам перевод стал точнее благодаря yandexgpt. в статье на хабре вы можете почитать все подробности о том, как устроен инструмент, а здесь расскажем коротко. в основе технологии синтеза речи лежит модифицированная опенсорс-модель tortoise-tts. сама по себе она выдаёт результаты хорошего качества, почти неотличимые от человеческой речи. однако есть несколько проблем, которые не позволяют использовать модель в продакшене. одна из них связана с качеством zero-shot-синтеза, то есть генерации аудио тем же голосом, что и в аудиопромпте. результат может быть не похожим на исходник, а при переносе тембра с английского на русский появляется акцент. чтобы исправить это, в яндексе использовали фонемное представление текста и создали общий алфавит для английских и русских фонем. благодаря этому произношение модели стало более правильным. для моделирования тембра голоса внедрили биометрические эмбеддинги и контролировали качество речи с помощью метрики utmos. а проблему акцента при переводе с английского на русский решили с помощью синтетического датасета, где голос одного и того же человека представлен на двух языках. ещё один недостаток tortoise-tts — низкая скорость инференса, из-за которой модель и получила своё название. в яндексе оптимизировали её архитектуру, уменьшили количество итераций в диффузионной модели и применили технику дистилляции знаний. благодаря этому, генерация ответа происходит в реальном времени. sbs-тестирование показало, что новый перевод видео в яндекс браузере значительно превосходит решение elevenlabs: 62% побед против 34%. что касается исключительно озвучивания, то есть превращения текста в речь, то здесь система яндекса также впереди: 46% против 42%. speech info синхронный перевод видео в яндекс браузере перевод видео в яндекс браузере появился ещё в 2021 году. сегодня компания представляет новую версию этой технологии, способную сохранять тембр и интонации оригинального голоса. а сам перевод стал точнее благодаря yandexgpt. в статье на хабре вы можете почитать все подробности о том, как устроен инструмент, а здесь расскажем коротко. в основе технологии синтеза речи лежит модифицированная опенсорс-модель tortoise-tts. сама по себе она выдаёт результаты хорошего качества, почти неотличимые от человеческой речи. однако есть несколько проблем, которые не позволяют использовать модель в продакшене. одна из них связана с качеством zero-shot-синтеза, то есть генерации аудио тем же голосом, что и в аудиопромпте. результат может быть не похожим на исходник, а при переносе тембра с английского на русский появляется акцент. чтобы исправить это, в яндексе использовали фонемное представление текста и создали общий алфавит для английских и русских фонем. благодаря этому произношение модели стало более правильным. для моделирования тембра голоса внедрили биометрические эмбеддинги и контролировали качество речи с помощью метрики utmos. а проблему акцента при переводе с английского на русский решили с помощью синтетического датасета, где голос одного и того же человека представлен на двух языках. ещё один недостаток tortoise-tts — низкая скорость инференса, из-за которой модель и получила своё название. в яндексе оптимизировали её архитектуру, уменьшили количество итераций в диффузионной модели и применили технику дистилляции знаний. благодаря этому, генерация ответа происходит в реальном времени. sbs-тестирование показало, что новый перевод видео в яндекс браузере значительно превосходит решение elevenlabs: 62% побед против 34%. что касается исключительно озвучивания, то есть превращения текста в речь, то здесь система яндекса также впереди: 46% против 42%. speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-28T10:01:35+00:00" href="./posts/108.html">2025-04-28 10:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Синхронный перевод видео в Яндекс Браузере<br></strong><br>Перевод видео в Яндекс Браузере появился ещё в 2021 году. Сегодня компания представляет новую версию этой технологии, способную сохранять тембр и интонации оригинального голоса. А сам перевод стал точнее благодаря YandexGPT. <a href="https://habr.com/ru/companies/yandex/articles/902086/" rel="nofollow noopener noreferrer">В статье на Хабре</a> вы можете почитать все подробности о том, как устроен инструмент, а здесь расскажем коротко.<br><br>В основе технологии синтеза речи лежит модифицированная опенсорс-модель Tortoise-TTS. Сама по себе она выдаёт результаты хорошего качества, почти неотличимые от человеческой речи. Однако есть несколько проблем, которые не позволяют использовать модель в продакшене. <br><br>Одна из них связана с качеством zero-shot-синтеза, то есть генерации аудио тем же голосом, что и в аудиопромпте. Результат может быть не похожим на исходник, а при переносе тембра с английского на русский появляется акцент. <br><br>Чтобы исправить это, в Яндексе использовали фонемное представление текста и создали общий алфавит для английских и русских фонем. Благодаря этому произношение модели стало более правильным. Для моделирования тембра голоса внедрили биометрические эмбеддинги и контролировали качество речи с помощью метрики UTMOS. А проблему акцента при переводе с английского на русский решили с помощью синтетического датасета, где голос одного и того же человека представлен на двух языках.<br><br>Ещё один недостаток Tortoise-TTS — низкая скорость инференса, из-за которой модель и получила своё название. В Яндексе оптимизировали её архитектуру, уменьшили количество итераций в диффузионной модели и применили технику дистилляции знаний. Благодаря этому, генерация ответа происходит в реальном времени. <br><br>SBS-тестирование показало, что новый перевод видео в Яндекс Браузере значительно превосходит решение ElevenLabs:  62% побед против 34%. Что касается исключительно озвучивания, то есть превращения текста в речь, то здесь система Яндекса также впереди:  46% против 42%. <br><br><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a></div>
      <div class="actions">
        <span>2 092 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/108" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/108.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="107" data-search="в яндекс браузере запустилась новая версия синхронного перевода видео. а мы в канале speech info кратко рассказали, как она работает. в яндекс браузере запустилась новая версия синхронного перевода видео. а мы в канале speech info кратко рассказали, как она работает.">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-28T10:01:35+00:00" href="./posts/107.html">2025-04-28 10:01 UTC</a></div>
      </div>
      <div class="post-body">В Яндекс Браузере запустилась новая версия синхронного перевода видео. А мы в канале Speech Info кратко рассказали, как она работает.</div>
      <div class="actions">
        <span>1 705 просмотров · 0 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/107" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/107.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="103" data-search="что-то кончается, что-то начинается так писал анджей сапковский. у нас заканчивается конференция iclr и начинается — череда подробных обзоров по следам мероприятия. а сегодня — несколько вайбовых видео и все материалы, которые мы писали об iclr 2025: — постеры, на которые стоит обратить внимание. часть i. — постеры, на которые стоит обратить внимание. часть ii. — лучший постер второго дня с котиками. — пляшущие роботы и статьи от команды yandex research. оставайтесь с нами! а ещё больше материалов с iclr вы найдёте в других наших каналах: — душный nlp — speech info — рекомендательная — cv time #yaiclr ml underhood что-то кончается, что-то начинается так писал анджей сапковский. у нас заканчивается конференция iclr и начинается — череда подробных обзоров по следам мероприятия. а сегодня — несколько вайбовых видео и все материалы, которые мы писали об iclr 2025: — постеры, на которые стоит обратить внимание. часть i. — постеры, на которые стоит обратить внимание. часть ii. — лучший постер второго дня с котиками. — пляшущие роботы и статьи от команды yandex research. оставайтесь с нами! а ещё больше материалов с iclr вы найдёте в других наших каналах: — душный nlp — speech info — рекомендательная — cv time #yaiclr ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-27T09:12:33+00:00" href="./posts/103.html">2025-04-27 09:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Что-то кончается, что-то начинается</strong><br><br>Так писал Анджей Сапковский. У нас заканчивается конференция ICLR и начинается — череда подробных обзоров по следам мероприятия. А сегодня — несколько вайбовых видео и все материалы, которые мы писали об ICLR 2025:<br><br>— <a href="https://t.me/MLunderhood/94?single" rel="nofollow noopener noreferrer">Постеры, на которые стоит обратить внимание. Часть I.</a><br>— <a href="https://t.me/MLunderhood/99" rel="nofollow noopener noreferrer">Постеры, на которые стоит обратить внимание. Часть II.</a><br>— <a href="https://t.me/MLunderhood/97" rel="nofollow noopener noreferrer">Лучший постер второго дня</a> с котиками.<br>— <a href="https://t.me/MLunderhood/98" rel="nofollow noopener noreferrer">Пляшущие роботы и статьи от команды Yandex Research.</a><br><br>Оставайтесь с нами! А ещё больше материалов с ICLR вы найдёте в других наших каналах:<br><br>— <a href="https://t.me/stuffyNLP" rel="nofollow noopener noreferrer">Душный NLP</a><br>— <a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer">Speech Info</a><br>— <a href="https://t.me/RecSysChannel" rel="nofollow noopener noreferrer">Рекомендательная</a><br>— <a href="https://t.me/timeforcv" rel="nofollow noopener noreferrer">CV Time</a><br><br>#YaICLR<br><br><a href="https://t.me/+bdN10jra9vxkZjhi" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><video controls preload="metadata" src="../assets/media/103_IMG_8344.MOV.mov"></video><video controls preload="metadata" src="../assets/media/104_IMG_8348.MOV.mov"></video><video controls preload="metadata" src="../assets/media/105_IMG_8354.MOV.mov"></video><video controls preload="metadata" src="../assets/media/106_IMG_8361.MOV.mov"></video></div></div>
      <div class="actions">
        <span>1 939 просмотров · 8 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/103" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/103.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="99" data-search="подборка постеров с iclr 2025 продолжаем рассказывать о самых интересных статьях с конференции и показываем один весьма экстравагантный стенд. revisiting nearest neighbor for tabular data: a deep tabular baseline two decades later авторы улучшили nca (непараметрический метод на основе nearest neighbour) простыми нейросетями и обогнали бустинги и tabulardl на многих задачах. результаты: — на наборе задач мульти-классификации их метод оказался лучшим на 20% задач, что на 7 процентных пункта больше, чем с топ-2 подходом (у tabr 13%) — для бинарной классификации и регрессии результаты, скорее, сравнимы с текущими sota. — применялись на задачах с сотнями (но не тысячами) фичей. приёмы: — отказ от lbfgs в nca в пользу sgd для обучения проектора. — стохастика и по батчам, и по соседям — сэмплируются случайные группы соседей одного класса/ — заменили линейную проекцию из nca на нелинейную. используют простую нейросеть (2-3 слоя, bn, relu)/ — от предсказания жёстких меток класса перешли к вероятностям за счёт softmax, чтобы сгладить задачу оптимизации. anollm: large language models for tabular anomaly detection ищем аномалии в табличных данных. — составляем из данных корпус текстов вида «фича х равна y, ...». — файнтюним. — оцениваем вероятность встретить значение фичи при условии значений других фичей, считаем nll. достаточно маленьких моделей (130м — 1,7b). fredf: learning to forecast in frequency domain для прогноза временных рядов авторы предлагают дополнительно к предсказанной и gt-последовательностям применять fft и считать ещё один лосс между ними. говорят, что получается неплохо. а на последнем изображении тот самый экстравагантный стенд. выглядит душевно! постеры заметили ❣ кирилл никоров, пётр вытовтов, константин бабалян #yaiclr ml underhood подборка постеров с iclr 2025 продолжаем рассказывать о самых интересных статьях с конференции и показываем один весьма экстравагантный стенд. revisiting nearest neighbor for tabular data: a deep tabular baseline two decades later авторы улучшили nca (непараметрический метод на основе nearest neighbour) простыми нейросетями и обогнали бустинги и tabulardl на многих задачах. результаты: — на наборе задач мульти-классификации их метод оказался лучшим на 20% задач, что на 7 процентных пункта больше, чем с топ-2 подходом (у tabr 13%) — для бинарной классификации и регрессии результаты, скорее, сравнимы с текущими sota. — применялись на задачах с сотнями (но не тысячами) фичей. приёмы: — отказ от lbfgs в nca в пользу sgd для обучения проектора. — стохастика и по батчам, и по соседям — сэмплируются случайные группы соседей одного класса/ — заменили линейную проекцию из nca на нелинейную. используют простую нейросеть (2-3 слоя, bn, relu)/ — от предсказания жёстких меток класса перешли к вероятностям за счёт softmax, чтобы сгладить задачу оптимизации. anollm: large language models for tabular anomaly detection ищем аномалии в табличных данных. — составляем из данных корпус текстов вида «фича х равна y, ...». — файнтюним. — оцениваем вероятность встретить значение фичи при условии значений других фичей, считаем nll. достаточно маленьких моделей (130м — 1,7b). fredf: learning to forecast in frequency domain для прогноза временных рядов авторы предлагают дополнительно к предсказанной и gt-последовательностям применять fft и считать ещё один лосс между ними. говорят, что получается неплохо. а на последнем изображении тот самый экстравагантный стенд. выглядит душевно! постеры заметили ❣ кирилл никоров, пётр вытовтов, константин бабалян #yaiclr ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-26T12:27:18+00:00" href="./posts/99.html">2025-04-26 12:27 UTC</a></div>
      </div>
      <div class="post-body"><strong>Подборка постеров с ICLR 2025</strong><br><br>Продолжаем рассказывать о самых интересных статьях с конференции и показываем один весьма экстравагантный стенд. <br><br><a href="https://arxiv.org/abs/2407.03257" rel="nofollow noopener noreferrer"><strong>Revisiting Nearest Neighbor for Tabular Data: A Deep Tabular Baseline Two Decades Later</strong></a><br><br>Авторы улучшили NCA (непараметрический метод на основе nearest neighbour) простыми нейросетями и обогнали бустинги и TabularDL на многих задачах.<br><br>Результаты:<br>— На наборе задач мульти-классификации их метод оказался лучшим на 20% задач, что на 7 процентных пункта больше, чем с Топ-2 подходом (у TabR 13%)<br>— Для бинарной классификации и регрессии результаты, скорее, сравнимы с текущими SOTA.<br>— Применялись на задачах с сотнями (но не тысячами) фичей. <br><br>Приёмы:<br>— Отказ от LBFGS в NCA в пользу SGD для обучения проектора.<br>— Стохастика и по батчам, и по соседям — сэмплируются случайные группы соседей одного класса/<br>— Заменили линейную проекцию из NCA на нелинейную. Используют простую нейросеть (2-3 слоя, BN, ReLU)/<br>— От предсказания жёстких меток класса перешли к вероятностям за счёт softmax, чтобы сгладить задачу оптимизации.<br><br><a href="https://openreview.net/forum?id=7VkHffT5X2" rel="nofollow noopener noreferrer"><strong>AnoLLM: Large Language Models for Tabular Anomaly Detection</strong></a><br><br>Ищем аномалии в табличных данных. <br><br>— Составляем из данных корпус текстов вида «фича Х равна Y, ...».<br>— Файнтюним.<br>— Оцениваем вероятность встретить значение фичи при условии значений других фичей, считаем NLL.<br><br>Достаточно маленьких моделей (130М — 1,7B).<br><br><a href="https://arxiv.org/abs/2402.02399" rel="nofollow noopener noreferrer"><strong>FreDF: Learning to Forecast in Frequency Domain</strong></a><br><br>Для прогноза временных рядов авторы предлагают дополнительно к предсказанной и GT-последовательностям применять FFT и считать ещё один лосс между ними. Говорят, что получается неплохо.<br><br><em>А на последнем изображении тот самый экстравагантный стенд. Выглядит душевно! </em><br><br><em>Постеры заметили </em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji><em> Кирилл Никоров, Пётр Вытовтов, Константин Бабалян</em><br><br>#YaICLR<br><br><a href="https://t.me/+Bf5IAWB55xEwYzYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/99_480.webp" srcset="../assets/media/thumbs/99_480.webp 480w, ../assets/media/99.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="99" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/100_480.webp" srcset="../assets/media/thumbs/100_480.webp 480w, ../assets/media/100.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="99" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/101_480.webp" srcset="../assets/media/thumbs/101_480.webp 480w, ../assets/media/101.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="99" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/102_480.webp" srcset="../assets/media/thumbs/102_480.webp 480w, ../assets/media/102.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="99" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 574 просмотров · 13 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/99" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/99.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="98" data-search="танцуем в предвкушении, как этот милый робот-пёс уже завтра на полях iclr два постера от команды yandex research! c 10:00 по сингапурскому времени можно будет ознакомиться со статьёй tabred (hall 3 + hall 2b #348). с 15:00 — пообщаться с авторами tabm (hall 3 + hall 2b #323). приходите посмотреть и познакомиться! танцуем в предвкушении, как этот милый робот-пёс уже завтра на полях iclr два постера от команды yandex research! c 10:00 по сингапурскому времени можно будет ознакомиться со статьёй tabred (hall 3 + hall 2b #348). с 15:00 — пообщаться с авторами tabm (hall 3 + hall 2b #323). приходите посмотреть и познакомиться!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T15:01:04+00:00" href="./posts/98.html">2025-04-25 15:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Танцуем в предвкушении, как этот милый робот-пёс</strong><br><strong><br></strong>Уже завтра на полях ICLR два постера от команды Yandex Research!<br><br>C 10:00 по Сингапурскому времени можно будет ознакомиться со статьёй <a href="https://arxiv.org/abs/2406.19380" rel="nofollow noopener noreferrer">TabReD</a> (Hall 3 + Hall 2B #348).<br><br>С 15:00 — пообщаться с авторами <a href="https://arxiv.org/abs/2410.24210" rel="nofollow noopener noreferrer">TabM</a> (Hall 3 + Hall 2B #323). <br><br>Приходите посмотреть и познакомиться!</div>
      <div class="actions">
        <span>1 938 просмотров · 11 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/98" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/98.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="index.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link current" href="page-2.html">2</a> <a class="page-link" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-3.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 165, "media": [{"kind": "photo", "path": "../assets/media/165.jpg", "thumb": "../assets/media/thumbs/165_480.webp", "size": 147539, "mime": "image/jpeg", "name": null}]}, {"id": 164, "media": [{"kind": "photo", "path": "../assets/media/164.jpg", "thumb": "../assets/media/thumbs/164_480.webp", "size": 33029, "mime": "image/jpeg", "name": null}]}, {"id": 158, "media": [{"kind": "photo", "path": "../assets/media/158.jpg", "thumb": "../assets/media/thumbs/158_480.webp", "size": 144420, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/159.jpg", "thumb": "../assets/media/thumbs/159_480.webp", "size": 198485, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/160.jpg", "thumb": "../assets/media/thumbs/160_480.webp", "size": 280822, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/161.jpg", "thumb": "../assets/media/thumbs/161_480.webp", "size": 186436, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/162.jpg", "thumb": "../assets/media/thumbs/162_480.webp", "size": 68190, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/163.jpg", "thumb": "../assets/media/thumbs/163_480.webp", "size": 185110, "mime": "image/jpeg", "name": null}]}, {"id": 157, "media": []}, {"id": 154, "media": [{"kind": "photo", "path": "../assets/media/154.jpg", "thumb": "../assets/media/thumbs/154_480.webp", "size": 165210, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/155.jpg", "thumb": "../assets/media/thumbs/155_480.webp", "size": 157815, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/156.jpg", "thumb": "../assets/media/thumbs/156_480.webp", "size": 160208, "mime": "image/jpeg", "name": null}]}, {"id": 153, "media": [{"kind": "photo", "path": "../assets/media/153.jpg", "thumb": "../assets/media/thumbs/153_480.webp", "size": 96218, "mime": "image/jpeg", "name": null}]}, {"id": 146, "media": [{"kind": "photo", "path": "../assets/media/146.jpg", "thumb": "../assets/media/thumbs/146_480.webp", "size": 159350, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/147.jpg", "thumb": "../assets/media/thumbs/147_480.webp", "size": 231630, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/148.jpg", "thumb": "../assets/media/thumbs/148_480.webp", "size": 129607, "mime": "image/jpeg", "name": null}, {"kind": "video", "path": "../assets/media/149_ACL-4.mp4", "thumb": null, "size": 4213689, "mime": "video/mp4", "name": "ACL-4.mp4"}, {"kind": "video", "path": "../assets/media/150_ACL-6.mp4", "thumb": null, "size": 1135134, "mime": "video/mp4", "name": "ACL-6.mp4"}, {"kind": "photo", "path": "../assets/media/151.jpg", "thumb": "../assets/media/thumbs/151_480.webp", "size": 145761, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/152.jpg", "thumb": "../assets/media/thumbs/152_480.webp", "size": 41577, "mime": "image/jpeg", "name": null}]}, {"id": 144, "media": [{"kind": "photo", "path": "../assets/media/144.jpg", "thumb": "../assets/media/thumbs/144_480.webp", "size": 125706, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/145.jpg", "thumb": "../assets/media/thumbs/145_480.webp", "size": 104578, "mime": "image/jpeg", "name": null}]}, {"id": 143, "media": [{"kind": "photo", "path": "../assets/media/143.jpg", "thumb": "../assets/media/thumbs/143_480.webp", "size": 108960, "mime": "image/jpeg", "name": null}]}, {"id": 138, "media": [{"kind": "video", "path": "../assets/media/138_2025-07-18_13.56.42.mp4", "thumb": null, "size": 4764075, "mime": "video/mp4", "name": "2025-07-18_13.56.42.mp4"}, {"kind": "photo", "path": "../assets/media/139.jpg", "thumb": "../assets/media/thumbs/139_480.webp", "size": 238458, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/140.jpg", "thumb": "../assets/media/thumbs/140_480.webp", "size": 168320, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/141.jpg", "thumb": "../assets/media/thumbs/141_480.webp", "size": 352600, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/142.jpg", "thumb": "../assets/media/thumbs/142_480.webp", "size": 173592, "mime": "image/jpeg", "name": null}]}, {"id": 135, "media": [{"kind": "photo", "path": "../assets/media/135.jpg", "thumb": "../assets/media/thumbs/135_480.webp", "size": 90749, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/136.jpg", "thumb": "../assets/media/thumbs/136_480.webp", "size": 96894, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/137.jpg", "thumb": "../assets/media/thumbs/137_480.webp", "size": 182135, "mime": "image/jpeg", "name": null}]}, {"id": 132, "media": [{"kind": "photo", "path": "../assets/media/132.jpg", "thumb": "../assets/media/thumbs/132_480.webp", "size": 173721, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/133.jpg", "thumb": "../assets/media/thumbs/133_480.webp", "size": 171346, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/134.jpg", "thumb": "../assets/media/thumbs/134_480.webp", "size": 162856, "mime": "image/jpeg", "name": null}]}, {"id": 131, "media": [{"kind": "photo", "path": "../assets/media/131.jpg", "thumb": "../assets/media/thumbs/131_480.webp", "size": 148299, "mime": "image/jpeg", "name": null}]}, {"id": 130, "media": [{"kind": "photo", "path": "../assets/media/130.jpg", "thumb": "../assets/media/thumbs/130_480.webp", "size": 51437, "mime": "image/jpeg", "name": null}]}, {"id": 128, "media": []}, {"id": 124, "media": [{"kind": "photo", "path": "../assets/media/124.jpg", "thumb": "../assets/media/thumbs/124_480.webp", "size": 82508, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/125.jpg", "thumb": "../assets/media/thumbs/125_480.webp", "size": 202185, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/126.jpg", "thumb": "../assets/media/thumbs/126_480.webp", "size": 170937, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/127.jpg", "thumb": "../assets/media/thumbs/127_480.webp", "size": 146766, "mime": "image/jpeg", "name": null}]}, {"id": 119, "media": [{"kind": "video", "path": "../assets/media/120_333.mp4", "thumb": null, "size": 2210767, "mime": "video/mp4", "name": "333.mp4"}, {"kind": "photo", "path": "../assets/media/121.jpg", "thumb": "../assets/media/thumbs/121_480.webp", "size": 266696, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/122.jpg", "thumb": "../assets/media/thumbs/122_480.webp", "size": 261147, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/123.jpg", "thumb": "../assets/media/thumbs/123_480.webp", "size": 234052, "mime": "image/jpeg", "name": null}]}, {"id": 116, "media": [{"kind": "photo", "path": "../assets/media/116.jpg", "thumb": "../assets/media/thumbs/116_480.webp", "size": 54807, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/117.jpg", "thumb": "../assets/media/thumbs/117_480.webp", "size": 51443, "mime": "image/jpeg", "name": null}]}, {"id": 115, "media": []}, {"id": 114, "media": [{"kind": "photo", "path": "../assets/media/114.jpg", "thumb": "../assets/media/thumbs/114_480.webp", "size": 39506, "mime": "image/jpeg", "name": null}]}, {"id": 113, "media": []}, {"id": 112, "media": []}, {"id": 111, "media": []}, {"id": 110, "media": [{"kind": "photo", "path": "../assets/media/110.jpg", "thumb": "../assets/media/thumbs/110_480.webp", "size": 51554, "mime": "image/jpeg", "name": null}]}, {"id": 109, "media": []}, {"id": 108, "media": []}, {"id": 107, "media": []}, {"id": 103, "media": [{"kind": "video", "path": "../assets/media/103_IMG_8344.MOV.mov", "thumb": null, "size": 1312974, "mime": "video/quicktime", "name": "IMG_8344.MOV"}, {"kind": "video", "path": "../assets/media/104_IMG_8348.MOV.mov", "thumb": null, "size": 5363548, "mime": "video/quicktime", "name": "IMG_8348.MOV"}, {"kind": "video", "path": "../assets/media/105_IMG_8354.MOV.mov", "thumb": null, "size": 3704672, "mime": "video/quicktime", "name": "IMG_8354.MOV"}, {"kind": "video", "path": "../assets/media/106_IMG_8361.MOV.mov", "thumb": null, "size": 2883943, "mime": "video/quicktime", "name": "IMG_8361.MOV"}]}, {"id": 99, "media": [{"kind": "photo", "path": "../assets/media/99.jpg", "thumb": "../assets/media/thumbs/99_480.webp", "size": 173659, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/100.jpg", "thumb": "../assets/media/thumbs/100_480.webp", "size": 247903, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/101.jpg", "thumb": "../assets/media/thumbs/101_480.webp", "size": 185885, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/102.jpg", "thumb": "../assets/media/thumbs/102_480.webp", "size": 156972, "mime": "image/jpeg", "name": null}]}, {"id": 98, "media": []}];
    window.__STATIC_META = {"title": "ML Underhood", "username": "MLunderhood", "channel": "MLunderhood", "last_sync_utc": "2026-02-11T19:43:43Z", "posts_count": 111, "last_seen_message_id": 283, "stats": {"new": 161, "updated": 4, "media_downloaded": 161}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
