<!doctype html>
<html lang="ru">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>ML Underhood — статическая версия (стр. 3/4)</title>
  <meta name="description" content="Статическая версия зеркала Telegram-канала" />
  <link rel="icon" href="../favicon.ico?v=2026-02-10T01%3A04%3A03Z" sizes="any" />
  <link rel="icon" type="image/png" sizes="32x32" href="../favicon-32.png?v=2026-02-10T01%3A04%3A03Z" />
  <link rel="apple-touch-icon" href="../apple-touch-icon.png?v=2026-02-10T01%3A04%3A03Z" />

  <link rel="stylesheet" href="../style.css" />
  <script src="../metrika.js"></script>
</head>
<body>
  <header class="header">
    <div class="container">
      <div class="title-grid">
        <a class="grid-avatar" href="#" target="_blank" rel="noopener">
          <img id="channelAvatar" class="channel-avatar" src="../assets/channel_avatar.jpg" alt="Аватар канала"  />
        </a>
        <div class="grid-main">
          <div class="title-head">
            <div class="title-left">
              <a class="badge-chip" id="siteTitleWrap" href="#" target="_blank" rel="noopener"><h1 id="siteTitle">ML Underhood</h1></a>
            </div>
            <div class="hero-actions">
              <a id="subscribeBtn" class="subscribe-btn" href="https://t.me/+pMU3hEMtRO5jMzQy" target="_blank" rel="noopener" >Подписаться</a>
              <a class="icon-btn" href="../" aria-label="Перейти к динамической версии">↺</a>
              <button id="themeToggle" class="icon-btn" type="button" aria-label="Переключить тему"></button>
            </div>
          </div>
        </div>
        <div class="controls"></div>
      </div>
    </div>
  </header>

  
  <div id="promoBanner" class="promo-banner" hidden>
    <div class="container promo-inner">
      <span class="promo-text"><a href="https://t.me/addlist/5NH3RoVejEI1MGEy">Подпишись на все наши ML каналы. Они классные, отвечаем!</a></span>
      <button id="promoClose" class="promo-close" type="button" aria-label="Скрыть плашку">×</button>
    </div>
  </div>
  

  <main class="container">
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
    <div id="posts" class="posts">
      
    <article class="post" data-post-id="97" data-search="⚡️внимание! обнаружен лучший постер второго дня iclr 2025! ⚡️ внимание! обнаружен лучший постер второго дня iclr 2025!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T13:38:02+00:00" href="./posts/97.html">2025-04-25 13:38 UTC</a></div>
      </div>
      <div class="post-body">⚡️<strong>Внимание! </strong><br><br>Обнаружен лучший постер второго дня ICLR 2025!<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/97_480.webp" srcset="../assets/media/thumbs/97_480.webp 480w, ../assets/media/97.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="97" data-image-index="0" /></div></div>
      <div class="actions">
        <span>1 942 просмотров · 42 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/97" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/97.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="94" data-search="крутые постеры с конференции iclr 2025 наши инженеры вовсю изучают постеры на мероприятии и делятся самыми любопытными статьями. tempme: video temporal token merging for efficient text-video retrieval авторы предлагают хитро дообучить clip для ускорения поиска по видео. результаты: — в 1,5-3 раза снижается количество вычислений для инференса, в зависимости от базового метода; — качество ранжирования в сером плюсе приёмы: — используется lora для дообучения энкодера. — применяется специальная процедура усреднения похожих токенов, как по временной, так и по пространственной размерностям. — для улучшения такого усреднения используются дополнительные позишн-эмбеды. — за счёт этого снижается количество обрабатываемых токенов и возникают более явные зависимости между кадрами по времени. leanvec: searching vectors faster by making them fit авторы предлагают решение для ускорения процедуры поиска. идея очень понятная и, возможно, много где реализована. собираем выборку запрос-документ, вычисляем матрицы a и b, преобразующие данные в меньшую размерность. 2. на этапе построения базы вычисляем bx — получаем базу документов меньшей размерности и строим ann (quant). в процессе поиска делаем aq, на основе которой из графа ищем ближайшие документы, а после уточняем кандидатов на этапе реранкинга по оригинальным векторам. в статье приводят результаты экспериментов показывающие, что меньшая размерность может быть в 3-4 раза меньше исходной без значимой потери качества поиска. плюс, полученное преобразование устойчиво к ood. странно, что авторы не сравнили своё решение с подходом, использующимся при обучении многих sota-эмбеддингов: matryoshka representation learning. в таком случае в модель уже встроены низкие размерности и не нужно ничего дополнительно обучать. по словам авторов, sota-библиотека от intel, в которую они встроились, всё еще имеет всего 150 звезд на github, так что теоретически идеи хорошие, а вот использовать ли их на практике — об этом стоит 10 раз подумать и самому оценить. dellma: decision making under uncertainty with large language models авторы учат llm принимать решения в условиях неопределённости. они предлагают ввести лист состояний мира, который можно вывести из контекста и к которому, попарно для каждого state-action выводится функция полезности. постеры заметили ❣ кирилл никоров, алексей спасёнов, александр воронцов #yaiclr ml underhood крутые постеры с конференции iclr 2025 наши инженеры вовсю изучают постеры на мероприятии и делятся самыми любопытными статьями. tempme: video temporal token merging for efficient text-video retrieval авторы предлагают хитро дообучить clip для ускорения поиска по видео. результаты: — в 1,5-3 раза снижается количество вычислений для инференса, в зависимости от базового метода; — качество ранжирования в сером плюсе приёмы: — используется lora для дообучения энкодера. — применяется специальная процедура усреднения похожих токенов, как по временной, так и по пространственной размерностям. — для улучшения такого усреднения используются дополнительные позишн-эмбеды. — за счёт этого снижается количество обрабатываемых токенов и возникают более явные зависимости между кадрами по времени. leanvec: searching vectors faster by making them fit авторы предлагают решение для ускорения процедуры поиска. идея очень понятная и, возможно, много где реализована. собираем выборку запрос-документ, вычисляем матрицы a и b, преобразующие данные в меньшую размерность. 2. на этапе построения базы вычисляем bx — получаем базу документов меньшей размерности и строим ann (quant). в процессе поиска делаем aq, на основе которой из графа ищем ближайшие документы, а после уточняем кандидатов на этапе реранкинга по оригинальным векторам. в статье приводят результаты экспериментов показывающие, что меньшая размерность может быть в 3-4 раза меньше исходной без значимой потери качества поиска. плюс, полученное преобразование устойчиво к ood. странно, что авторы не сравнили своё решение с подходом, использующимся при обучении многих sota-эмбеддингов: matryoshka representation learning. в таком случае в модель уже встроены низкие размерности и не нужно ничего дополнительно обучать. по словам авторов, sota-библиотека от intel, в которую они встроились, всё еще имеет всего 150 звезд на github, так что теоретически идеи хорошие, а вот использовать ли их на практике — об этом стоит 10 раз подумать и самому оценить. dellma: decision making under uncertainty with large language models авторы учат llm принимать решения в условиях неопределённости. они предлагают ввести лист состояний мира, который можно вывести из контекста и к которому, попарно для каждого state-action выводится функция полезности. постеры заметили ❣ кирилл никоров, алексей спасёнов, александр воронцов #yaiclr ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-25T11:11:42+00:00" href="./posts/94.html">2025-04-25 11:11 UTC</a></div>
      </div>
      <div class="post-body"><strong>Крутые постеры с конференции ICLR 2025</strong><br><br>Наши инженеры вовсю изучают постеры на мероприятии и делятся самыми любопытными статьями. <br><br><a href="https://arxiv.org/abs/2409.01156" rel="nofollow noopener noreferrer"><strong>TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval</strong></a><br><br>Авторы предлагают хитро дообучить Clip для ускорения поиска по видео. Результаты:<br><br>— в 1,5-3 раза снижается количество  вычислений для инференса, в зависимости от базового метода;<br>— качество ранжирования в сером плюсе<br><br>Приёмы:<br><br>— Используется LoRA для дообучения энкодера.<br>— Применяется специальная процедура усреднения похожих токенов, как по временной, так и по пространственной размерностям.<br>— Для улучшения такого усреднения используются дополнительные позишн-эмбеды.<br>— За счёт этого снижается количество обрабатываемых токенов и возникают более явные зависимости между кадрами по времени.<br><br><a href="https://arxiv.org/abs/2312.16335" rel="nofollow noopener noreferrer"><strong>LeanVec: Searching vectors faster by making them fit</strong></a><br><br>Авторы предлагают решение для ускорения процедуры поиска. Идея очень понятная и, возможно, много где реализована.<br><br>Собираем выборку запрос-документ, вычисляем матрицы A и B, преобразующие данные в меньшую размерность.<br>2. На этапе построения базы вычисляем Bx — получаем базу документов меньшей размерности и строим ANN (quant).<br>В процессе поиска делаем Aq, на основе которой из графа ищем ближайшие документы, а после уточняем кандидатов на этапе реранкинга по оригинальным векторам.<br><br>В статье приводят результаты экспериментов показывающие, что меньшая размерность может быть в 3-4 раза меньше исходной без значимой потери качества поиска. Плюс, полученное преобразование устойчиво к OOD.<br><br>Странно, что авторы не сравнили своё решение с подходом, использующимся при обучении многих SOTA-эмбеддингов: <a href="https://arxiv.org/abs/2312.16335" rel="nofollow noopener noreferrer">Matryoshka Representation Learning.</a> В таком случае в модель уже встроены низкие размерности и не нужно ничего дополнительно обучать. По словам авторов, SOTA-библиотека от Intel, в которую они встроились, всё еще имеет всего 150 звезд на Github, так что теоретически идеи хорошие, а вот использовать ли их на практике — об этом стоит 10 раз подумать и самому оценить.<br><br><a href="https://arxiv.org/abs/2402.02392" rel="nofollow noopener noreferrer"><strong>DeLLMa: Decision Making Under Uncertainty with Large Language Models</strong></a><br><br>Авторы учат LLM принимать решения в условиях неопределённости. Они предлагают ввести лист состояний мира, который можно вывести из контекста и к которому, попарно для каждого state-action выводится функция полезности.<br><br><em>Постеры заметили </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Кирилл Никоров, Алексей Спасёнов, Александр Воронцов</em><br><br>#YaICLR<br><br><a href="https://t.me/+Bf5IAWB55xEwYzYy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/94_480.webp" srcset="../assets/media/thumbs/94_480.webp 480w, ../assets/media/94.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="94" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/95_480.webp" srcset="../assets/media/thumbs/95_480.webp 480w, ../assets/media/95.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="94" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/96_480.webp" srcset="../assets/media/thumbs/96_480.webp 480w, ../assets/media/96.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="94" data-image-index="2" /></div></div>
      <div class="actions">
        <span>2 956 просмотров · 9 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/94" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/94.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="92" data-search="вот-вот стартует конференция iclr 2025, и ml-инженеры из яндекса, которые будут освещать мероприятие для вас, уже высадились в сингапуре. следите за новостями! вот-вот стартует конференция iclr 2025, и ml-инженеры из яндекса, которые будут освещать мероприятие для вас, уже высадились в сингапуре. следите за новостями!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-23T15:26:54+00:00" href="./posts/92.html">2025-04-23 15:26 UTC</a></div>
      </div>
      <div class="post-body">Вот-вот стартует конференция ICLR 2025, и ML-инженеры из Яндекса, которые будут освещать мероприятие для вас, уже высадились в Сингапуре. Следите за новостями!<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/92_480.webp" srcset="../assets/media/thumbs/92_480.webp 480w, ../assets/media/92.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="92" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/93_480.webp" srcset="../assets/media/thumbs/93_480.webp 480w, ../assets/media/93.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="92" data-image-index="1" /></div></div>
      <div class="actions">
        <span>1 802 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/92" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/92.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="90" data-search="от pytorch к monai: опыт команды yandex cloud и шада в медицинском ai разбираем интересный кейс из области медицины. в проекте по распознаванию редкой патологии spina bifida на узи команда ml-инженеров из школы анализа данных и yandex cloud приняла неожиданное решение. за неделю до релиза они полностью переписали пайплайн на monai — библиотеке для медицинского ai от nvidia. дмитрий сошников, выступивший ментором проектной команды, рассказал, почему стандартных инструментов pytorch оказалось недостаточно, как monai упростила работу и какие модели команда планирует выложить в опенсорс. для обучения нейросети инженеры использовали датасет из 6 тысяч обезличенных узи-снимков беременных женщин. данные собрали и разметили специалисты нмиц имени кулакова. команда yandex cloud и студенты шада построили архитектуру решения, включающую несколько нейросетей для поиска и классификации патологий. с помощью датасета студенты обучили модели и создали веб-интерфейс для врачей. проект реализовали на платформе yandex cloud с использованием инструмента машинного обучения полного цикла yandex datasphere. выше можно сравнить два снимка (слева — без патологии, справа — с вероятностью патологии 83%) и получить представление о том, как сложно увидеть различия невооружённым глазом. изначально проект написали на «голом» pytorch без специализированных медицинских библиотек. пайплайн состоял из стандартных этапов: — предобработки изображений; — детекции области интереса с помощью нейросети yolo; — фильтрации снимков по качеству; — поиска признаков патологии на хороших изображениях. подход работал, но оказался сложным для поддержки: разобраться в кастомных скриптах было непросто — особенно новым участникам команды или внешним специалистам. поэтому когда стало ясно, что проект будет опубликован в опенсорсе и получит развитие, решили перейти на monai. переписывание всех частей пайплайна заняло неделю: сначала перенесли загрузку данных, затем — аугментации, потом — обучение и валидацию моделей. особенно полезной оказалась аугментация для ухудшения качества снимков, которая имитировала реальные особенности узи-аппаратов. также пригодились готовые функции потерь для борьбы с дисбалансом классов и стандартные медицинские метрики. кроме того, в monai есть встроенные инструменты интерпретации моделей, такие как grad-cam, что особенно важно для медицины: сегодня интерпретируемость моделей обязательна по этическим нормам. переход дал прирост сразу по нескольким направлениям. в первую очередь, улучшилось качество моделей — за счёт более разнообразных и реалистичных аугментаций. то же ухудшение изображений дало прирост точности на 2–3 процентных пункта. также сократился объём кода и повысилась его читаемость: любые действия можно отследить через документацию monai, а не разбираться в кастомных скриптах. команда планирует выложить обученные модели в опенсорс в рамках monai model zoo — библиотеки предобученных моделей для медицины. сейчас в разделе нет решений для ультразвука, и команда хочет закрыть этот пробел. также разработчики готовят пайплайн для сбора новых данных, их разметки и дообучения моделей, чтобы специалисты нмиц кулакова могли сами обновлять решение в будущем. благодаря этому наработки можно будет использовать и в других медицинских задачах. в заключение ещё раз напомним, что проект реализовывали выпускники шада. набор в школу анализа данных яндекса открыт до 5 мая. если хочется своими руками создавать проекты, которые меняют индустрию и мир, — самое время подать заявку. в подготовке поста участвовали: главный разработчик проекта владимир корсунов и руководитель проекта со стороны yandex cloud евгений попов. ml underhood от pytorch к monai: опыт команды yandex cloud и шада в медицинском ai разбираем интересный кейс из области медицины. в проекте по распознаванию редкой патологии spina bifida на узи команда ml-инженеров из школы анализа данных и yandex cloud приняла неожиданное решение. за неделю до релиза они полностью переписали пайплайн на monai — библиотеке для медицинского ai от nvidia. дмитрий сошников , выступивший ментором проектной команды, рассказал, почему стандартных инструментов pytorch оказалось недостаточно, как monai упростила работу и какие модели команда планирует выложить в опенсорс. для обучения нейросети инженеры использовали датасет из 6 тысяч обезличенных узи-снимков беременных женщин. данные собрали и разметили специалисты нмиц имени кулакова. команда yandex cloud и студенты шада построили архитектуру решения, включающую несколько нейросетей для поиска и классификации патологий. с помощью датасета студенты обучили модели и создали веб-интерфейс для врачей. проект реализовали на платформе yandex cloud с использованием инструмента машинного обучения полного цикла yandex datasphere. выше можно сравнить два снимка (слева — без патологии, справа — с вероятностью патологии 83%) и получить представление о том, как сложно увидеть различия невооружённым глазом. изначально проект написали на «голом» pytorch без специализированных медицинских библиотек. пайплайн состоял из стандартных этапов: — предобработки изображений; — детекции области интереса с помощью нейросети yolo; — фильтрации снимков по качеству; — поиска признаков патологии на хороших изображениях. подход работал, но оказался сложным для поддержки: разобраться в кастомных скриптах было непросто — особенно новым участникам команды или внешним специалистам. поэтому когда стало ясно, что проект будет опубликован в опенсорсе и получит развитие, решили перейти на monai. переписывание всех частей пайплайна заняло неделю: сначала перенесли загрузку данных, затем — аугментации, потом — обучение и валидацию моделей. особенно полезной оказалась аугментация для ухудшения качества снимков, которая имитировала реальные особенности узи-аппаратов. также пригодились готовые функции потерь для борьбы с дисбалансом классов и стандартные медицинские метрики. кроме того, в monai есть встроенные инструменты интерпретации моделей, такие как grad-cam, что особенно важно для медицины: сегодня интерпретируемость моделей обязательна по этическим нормам. переход дал прирост сразу по нескольким направлениям. в первую очередь, улучшилось качество моделей — за счёт более разнообразных и реалистичных аугментаций. то же ухудшение изображений дало прирост точности на 2–3 процентных пункта. также сократился объём кода и повысилась его читаемость: любые действия можно отследить через документацию monai, а не разбираться в кастомных скриптах. команда планирует выложить обученные модели в опенсорс в рамках monai model zoo — библиотеки предобученных моделей для медицины. сейчас в разделе нет решений для ультразвука, и команда хочет закрыть этот пробел. также разработчики готовят пайплайн для сбора новых данных, их разметки и дообучения моделей, чтобы специалисты нмиц кулакова могли сами обновлять решение в будущем. благодаря этому наработки можно будет использовать и в других медицинских задачах. в заключение ещё раз напомним, что проект реализовывали выпускники шада. набор в школу анализа данных яндекса открыт до 5 мая. если хочется своими руками создавать проекты, которые меняют индустрию и мир, — самое время подать заявку. в подготовке поста участвовали: главный разработчик проекта владимир корсунов и руководитель проекта со стороны yandex cloud евгений попов. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-22T14:02:23+00:00" href="./posts/90.html">2025-04-22 14:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>От PyTorch к MONAI: опыт команды Yandex Cloud и ШАДа в медицинском AI</strong><br><br>Разбираем интересный кейс из области медицины. В <a href="https://spinabifida.cloudtechport.com/" rel="nofollow noopener noreferrer">проекте</a> по распознаванию редкой патологии spina bifida на УЗИ команда ML-инженеров из Школы анализа данных и Yandex Cloud приняла неожиданное решение. За неделю до релиза они полностью переписали пайплайн на <a href="https://monai.io/?utm_source=chatgpt.com" rel="nofollow noopener noreferrer">MONAI</a> — библиотеке для медицинского AI от NVIDIA. <a href="http://t.me/shwarsico" rel="nofollow noopener noreferrer">Дмитрий Сошников</a>, выступивший ментором проектной команды, рассказал, почему стандартных инструментов PyTorch оказалось недостаточно, как MONAI упростила работу и какие модели команда планирует выложить в опенсорс. <br><br>Для обучения нейросети инженеры использовали датасет из 6 тысяч обезличенных УЗИ-снимков беременных женщин. Данные собрали и разметили специалисты НМИЦ имени Кулакова. Команда Yandex Cloud и студенты ШАДа построили архитектуру решения, включающую несколько нейросетей для поиска и классификации патологий. С помощью датасета студенты обучили модели и создали веб-интерфейс для врачей. Проект реализовали на платформе Yandex Cloud с использованием инструмента машинного обучения полного цикла Yandex DataSphere.<br><br><blockquote>Выше можно сравнить два снимка (слева — без патологии, справа — с вероятностью патологии 83%) и получить представление о том, как сложно увидеть различия невооружённым глазом.  </blockquote><br><br>Изначально проект написали на «голом» PyTorch без специализированных медицинских библиотек. Пайплайн состоял из стандартных этапов: <br><br>— предобработки изображений;<br>— детекции области интереса с помощью нейросети YOLO;<br>— фильтрации снимков по качеству;<br>— поиска признаков патологии на хороших изображениях.<br><br>Подход работал, но оказался сложным для поддержки: разобраться в кастомных скриптах было непросто — особенно новым участникам команды или внешним специалистам. Поэтому когда стало ясно, что проект будет опубликован в опенсорсе и получит развитие, решили перейти на MONAI.<br><br>Переписывание всех частей пайплайна заняло неделю: сначала перенесли загрузку данных, затем — аугментации, потом — обучение и валидацию моделей. Особенно полезной оказалась аугментация для ухудшения качества снимков, которая имитировала реальные особенности УЗИ-аппаратов. Также пригодились готовые функции потерь для борьбы с дисбалансом классов и стандартные медицинские метрики. Кроме того, в MONAI есть встроенные инструменты интерпретации моделей, такие как Grad-CAM, что особенно важно для медицины: сегодня интерпретируемость моделей обязательна по этическим нормам. <br><br>Переход дал прирост сразу по нескольким направлениям. В первую очередь, улучшилось качество моделей — за счёт более разнообразных и реалистичных аугментаций. То же ухудшение изображений дало прирост точности на 2–3 процентных пункта. Также сократился объём кода и повысилась его читаемость: любые действия можно отследить через документацию MONAI, а не разбираться в кастомных скриптах. <br><br>Команда планирует выложить обученные модели в опенсорс в рамках MONAI Model Zoo — библиотеки предобученных моделей для медицины. Сейчас в разделе нет решений для ультразвука, и команда хочет закрыть этот пробел. Также разработчики готовят пайплайн для сбора новых данных, их разметки и дообучения моделей, чтобы специалисты НМИЦ Кулакова могли сами обновлять решение в будущем. Благодаря этому наработки можно будет использовать и в других медицинских задачах.<br><br>В заключение ещё раз напомним, что проект реализовывали выпускники ШАДа. Набор в Школу анализа данных Яндекса <a href="https://clck.ru/3Lb7st" rel="nofollow noopener noreferrer">открыт</a> до 5 мая. Если хочется своими руками создавать проекты, которые меняют индустрию и мир, — самое время подать заявку.<br><br><em>В подготовке поста участвовали: главный разработчик проекта Владимир Корсунов и руководитель проекта со стороны Yandex Cloud Евгений Попов.</em><br><br><a href="https://t.me/+HRBIUUTaR-hhOTRi" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/90_480.webp" srcset="../assets/media/thumbs/90_480.webp 480w, ../assets/media/90.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/91_480.webp" srcset="../assets/media/thumbs/91_480.webp 480w, ../assets/media/91.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="90" data-image-index="1" /></div></div>
      <div class="actions">
        <span>4 508 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/90" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/90.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="89" data-search="как устроена модель исправления ошибок в нейроредакторе яндекс браузера — часть ii продолжаем говорить о модели исправления ошибок, которая работает «под капотом» нейроредактора в яндекс браузере. в прошлой части ml-разработчик никита авдосев рассказал о качестве исправления и работы с промптом, а сегодня речь пойдёт о перфомансе. для ускорения генерации в компании прибегли к методу спекулятивного декодирования. суть его заключается в использовании компактной «черновой» (draft) модели, которая предлагает варианты продолжения цепочек токенов. основная модель проверяет их и выбирает одну с помощью стохастического алгоритма выборки. существует несколько подходов к спекулятивному декодированию, а в яндексе остановились на одном из самых популярных — eagle. он предполагает дообучение небольших голов поверх основой модели. гипотезы при этом генерируются в виде дерева, а не списка, благодаря чему повышается точность принятия токенов. в качестве эксперимента инженеры яндекса решили попробовать метод на модели исправления грамматических ошибок в русскоязычных текстах. выбор был сделан не случайно — это одна из самых широко используемых моделей в браузере, ежедневно к ней обращаются более 50 тысяч человек. чтобы обучить eagle предсказывать токены быстрее, чем их придумает оригинальная модель, требуется много текстов. в яндексе использовали 250 тысяч текстов из логов пользователей. позже попробовали обучить на более чем миллионе текстов, но, к сожалению, это не улучшило результат. затем начали тестировать новую конфигурацию для инференса: подбирать количество предсказываемых токенов перебором. остановились на предсказании четырёх токенов. это золотая середина, после которой ускорения не происходило, а местами даже увеличивалась задержка. благодаря eagle время генерации текста сократилось более чем в два раза. теперь она в среднем занимает меньше секунды, что в контексте llm — почти моментально. для ускорения моделей, которые работают с промптами пользователей, применяли fp8-квантизацию. её отличительная особенность — квантизация не в целые, а в вещественные числа. подход позволил добиться ускорения на 15% по сравнению с методом smoothquant, использованным ранее. ml underhood как устроена модель исправления ошибок в нейроредакторе яндекс браузера — часть ii продолжаем говорить о модели исправления ошибок, которая работает «под капотом» нейроредактора в яндекс браузере. в прошлой части ml-разработчик никита авдосев рассказал о качестве исправления и работы с промптом, а сегодня речь пойдёт о перфомансе. для ускорения генерации в компании прибегли к методу спекулятивного декодирования. суть его заключается в использовании компактной «черновой» (draft) модели, которая предлагает варианты продолжения цепочек токенов. основная модель проверяет их и выбирает одну с помощью стохастического алгоритма выборки. существует несколько подходов к спекулятивному декодированию, а в яндексе остановились на одном из самых популярных — eagle. он предполагает дообучение небольших голов поверх основой модели. гипотезы при этом генерируются в виде дерева, а не списка, благодаря чему повышается точность принятия токенов. в качестве эксперимента инженеры яндекса решили попробовать метод на модели исправления грамматических ошибок в русскоязычных текстах. выбор был сделан не случайно — это одна из самых широко используемых моделей в браузере, ежедневно к ней обращаются более 50 тысяч человек. чтобы обучить eagle предсказывать токены быстрее, чем их придумает оригинальная модель, требуется много текстов. в яндексе использовали 250 тысяч текстов из логов пользователей. позже попробовали обучить на более чем миллионе текстов, но, к сожалению, это не улучшило результат. затем начали тестировать новую конфигурацию для инференса: подбирать количество предсказываемых токенов перебором. остановились на предсказании четырёх токенов. это золотая середина, после которой ускорения не происходило, а местами даже увеличивалась задержка. благодаря eagle время генерации текста сократилось более чем в два раза. теперь она в среднем занимает меньше секунды, что в контексте llm — почти моментально. для ускорения моделей, которые работают с промптами пользователей, применяли fp8-квантизацию. её отличительная особенность — квантизация не в целые, а в вещественные числа. подход позволил добиться ускорения на 15% по сравнению с методом smoothquant, использованным ранее. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-18T09:47:15+00:00" href="./posts/89.html">2025-04-18 09:47 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как устроена модель исправления ошибок в нейроредакторе Яндекс Браузера</strong> <strong>— часть II</strong><br><br>Продолжаем говорить о модели исправления ошибок, которая работает «под капотом» нейроредактора в Яндекс Браузере. <a href="https://t.me/MLunderhood/86" rel="nofollow noopener noreferrer">В прошлой части</a> ML-разработчик Никита Авдосев рассказал о качестве исправления и работы с промптом, а сегодня речь пойдёт о перфомансе.<br><br>Для ускорения генерации в компании прибегли к методу спекулятивного декодирования. Суть его заключается в использовании компактной «черновой» (draft) модели, которая предлагает варианты продолжения цепочек токенов. Основная модель проверяет их и выбирает одну с помощью стохастического алгоритма выборки.<br><br>Существует несколько подходов к спекулятивному декодированию, а в Яндексе остановились на одном из самых популярных —  <a href="https://arxiv.org/abs/2401.15077" rel="nofollow noopener noreferrer">EAGLE.</a> Он предполагает дообучение небольших голов поверх основой модели. Гипотезы при этом генерируются в виде дерева, а не списка, благодаря чему повышается точность принятия токенов. <br><br>В качестве эксперимента инженеры Яндекса решили попробовать метод на модели исправления грамматических ошибок в русскоязычных текстах. Выбор был сделан не случайно — это одна из самых широко используемых моделей в Браузере, ежедневно к ней обращаются более 50 тысяч человек. <br><br>Чтобы обучить EAGLE предсказывать токены быстрее, чем их придумает оригинальная модель, требуется много текстов. В Яндексе использовали 250 тысяч текстов из логов пользователей. Позже попробовали обучить на более чем миллионе текстов, но, к сожалению, это не улучшило результат. <br><br>Затем начали тестировать новую конфигурацию для инференса: подбирать количество предсказываемых токенов перебором. Остановились на предсказании четырёх токенов. Это золотая середина, после которой ускорения не происходило, а местами даже увеличивалась задержка. <br><br>Благодаря EAGLE время генерации текста сократилось более чем в два раза. Теперь она в среднем занимает меньше секунды, что в контексте LLM — почти моментально. <br><br>Для ускорения моделей, которые работают с промптами пользователей, применяли FP8-квантизацию. Её отличительная особенность — квантизация не в целые, а в вещественные числа. Подход позволил добиться ускорения на 15% по сравнению с методом SmoothQuant, использованным ранее. <br><br><a href="https://t.me/+HRBIUUTaR-hhOTRi" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 192 просмотров · 11 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/89" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/89.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="88" data-search="алиса теперь понимает английский — и делает это без ущерба для русского. в колонках и чате заработал билингвальный asr, а вместе с ним — сценарии для практики английского. в нашем новом канале @speechinfo — подробности от команды, которая это реализовала. подписывайтесь, чтобы быть в курсе свежих разборов на тему аудио и ml! алиса теперь понимает английский — и делает это без ущерба для русского. в колонках и чате заработал билингвальный asr, а вместе с ним — сценарии для практики английского. в нашем новом канале @speechinfo — подробности от команды, которая это реализовала. подписывайтесь, чтобы быть в курсе свежих разборов на тему аудио и ml!">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-15T12:21:42+00:00" href="./posts/88.html">2025-04-15 12:21 UTC</a></div>
      </div>
      <div class="post-body">Алиса теперь понимает английский — и делает это без ущерба для русского. В колонках и чате заработал билингвальный ASR, а вместе с ним — сценарии для практики английского. <br><br>В нашем новом канале @speechinfo — подробности от команды, которая это реализовала. Подписывайтесь, чтобы быть в курсе свежих разборов на тему аудио и ML!</div>
      <div class="actions">
        <span>2 067 просмотров · 28 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/88" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/88.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="87" data-search="билингвальный asr — уже в станциях и чате с алисой мы с хорошими новостями — теперь алиса знает два языкаа: русский и английский! при этом распознавание русского не пострадало, а стало даже лучше. обновлённая алиса и поддержит диалог с носителем, и поможет улучшить навыки новичка. мы ликуем, пользователи в восторге, а вот репетиторы и всем известная сова немного грустят. евгений ганкович, руководитель группы asr, рассказал, с какими вызовами столкнулась команда: — необходимо было обучить модель, которая способна работать с новым языком, при этом критически важно было не просадить качество на русском. — домен английского для русскоговорящих пользователей специфичен и не решается с помощью открытых данных. — end-of-utterance (eou) по многим причинам работает у англоговорящих пользователей иначе. разберём, почему нужно было создавать билингвальную модель, а не обучать две отдельные. сложность решения в том, что заранее неизвестно, на каком языке поступит запрос: пользователь может начать на русском, а продолжить на английском или наоборот. в теории можно использовать классификатор: задан запрос, система определяет язык и направляет его в соответствующую модель. но чтобы точно определить язык, придётся подождать несколько секунд. к тому же такая система сложнее в поддержке и плохо справляется со смешанными языками (см. «смотря какой fabric, смотря сколько details»). выходит, что разумный путь — развивать текущий русскоязычный стек до двуязычного и использовать одну модель, которая инкапсулирует логику выбора языка. однако и здесь есть подводные камни. даже незначительное ухудшение распознавания на русском негативно скажется на пользовательском опыте. поэтому новую логику в модель нужно добавлять осторожно. причём улучшения вносятся в две ключевые части голосового стека: - end-of-utterance (eou) — модель на основе аудио и паршального распознавания, которая определяет, когда пользователь закончил говорить. - e2e seq2seq на базе трансформеров — модель распознаёт завершённый фрагмент речи на русском или английском языках. чтобы улучшить эти две компоненты, нужны данные. начать можно с открытых — но это другой домен: и акустика, и пользователи отличаются. поэтому мы привлекли отдельных людей для создания более подходящих нам данных. так собрали рабочее решение, но не сразу получили нужное качество. следующим шагом провели тесты на сотрудниках яндекса, которые использовали колонку с раскатанной технологии. на этой стадии смогли собрать ошибки, необходимые для улучшения модели. группы, на которые раскатывали технологию, росли по мере улучшения модели, а мы всё тоньше настраивали модель. по мере появления данных мы проводили эксперименты с обеими моделями, подбирая датамиксы и гиперпараметры тренировок. и в какой-то момент достигли качества для полноценного распознавания целевых запросов на английском. интересно, что в этих экспериментах получилось немного улучшить качество русского, так что исходную задачу даже перевыполнили. оставалось разобраться с eou. здесь были сложности из-за режима «тьютора», в котором можно вести диалог с алисой. пользователи сценария могут делать паузы, растягивать слова, и в таких случаях обычная модель может преждевременно обрезать речь. дослушивать мы тоже не можем — это может повлиять на другие компоненты и ответы алисы сильно замедлятся. решение крылось в добавлении в пайплайн eou более робастной и стабильной модели, способной учитывать паузы и длительность речи. хотелось бы рассказать о технологии подробнее, но для этого потребуется описать весь пайплайн распознавания — если вам интересно, дайте знать в комментариях. в итоге мы получили результат, который стал важной частью большого релиза: — голосовой набор сообщений на английском языке в чате и колонке; — сценарий «тьютор» на колонке: пользователи могут вести диалог с алисой, получать фидбек и переводить текст голосом. зовём протестировать, что у нас получилось: попробуйте поговорить с алисой на английском или скажите: «алиса, давай практиковать английский». евгений ганкович ❣ специально для speech info билингвальный asr — уже в станциях и чате с алисой мы с хорошими новостями — теперь алиса знает два языкаа: русский и английский! при этом распознавание русского не пострадало, а стало даже лучше. обновлённая алиса и поддержит диалог с носителем, и поможет улучшить навыки новичка. мы ликуем, пользователи в восторге, а вот репетиторы и всем известная сова немного грустят. евгений ганкович, руководитель группы asr, рассказал, с какими вызовами столкнулась команда: — необходимо было обучить модель, которая способна работать с новым языком, при этом критически важно было не просадить качество на русском. — домен английского для русскоговорящих пользователей специфичен и не решается с помощью открытых данных. — end-of-utterance (eou) по многим причинам работает у англоговорящих пользователей иначе. разберём, почему нужно было создавать билингвальную модель, а не обучать две отдельные. сложность решения в том, что заранее неизвестно, на каком языке поступит запрос: пользователь может начать на русском, а продолжить на английском или наоборот. в теории можно использовать классификатор: задан запрос, система определяет язык и направляет его в соответствующую модель. но чтобы точно определить язык, придётся подождать несколько секунд. к тому же такая система сложнее в поддержке и плохо справляется со смешанными языками (см. «смотря какой fabric, смотря сколько details»). выходит, что разумный путь — развивать текущий русскоязычный стек до двуязычного и использовать одну модель, которая инкапсулирует логику выбора языка. однако и здесь есть подводные камни. даже незначительное ухудшение распознавания на русском негативно скажется на пользовательском опыте. поэтому новую логику в модель нужно добавлять осторожно. причём улучшения вносятся в две ключевые части голосового стека: - end-of-utterance (eou) — модель на основе аудио и паршального распознавания, которая определяет, когда пользователь закончил говорить. - e2e seq2seq на базе трансформеров — модель распознаёт завершённый фрагмент речи на русском или английском языках. чтобы улучшить эти две компоненты, нужны данные. начать можно с открытых — но это другой домен: и акустика, и пользователи отличаются. поэтому мы привлекли отдельных людей для создания более подходящих нам данных. так собрали рабочее решение, но не сразу получили нужное качество. следующим шагом провели тесты на сотрудниках яндекса, которые использовали колонку с раскатанной технологии. на этой стадии смогли собрать ошибки, необходимые для улучшения модели. группы, на которые раскатывали технологию, росли по мере улучшения модели, а мы всё тоньше настраивали модель. по мере появления данных мы проводили эксперименты с обеими моделями, подбирая датамиксы и гиперпараметры тренировок. и в какой-то момент достигли качества для полноценного распознавания целевых запросов на английском. интересно, что в этих экспериментах получилось немного улучшить качество русского, так что исходную задачу даже перевыполнили. оставалось разобраться с eou. здесь были сложности из-за режима «тьютора», в котором можно вести диалог с алисой. пользователи сценария могут делать паузы, растягивать слова, и в таких случаях обычная модель может преждевременно обрезать речь. дослушивать мы тоже не можем — это может повлиять на другие компоненты и ответы алисы сильно замедлятся. решение крылось в добавлении в пайплайн eou более робастной и стабильной модели, способной учитывать паузы и длительность речи. хотелось бы рассказать о технологии подробнее, но для этого потребуется описать весь пайплайн распознавания — если вам интересно, дайте знать в комментариях. в итоге мы получили результат, который стал важной частью большого релиза: — голосовой набор сообщений на английском языке в чате и колонке; — сценарий «тьютор» на колонке: пользователи могут вести диалог с алисой, получать фидбек и переводить текст голосом. зовём протестировать, что у нас получилось: попробуйте поговорить с алисой на английском или скажите: «алиса, давай практиковать английский». евгений ганкович ❣ специально для speech info">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-15T12:21:22+00:00" href="./posts/87.html">2025-04-15 12:21 UTC</a></div>
      </div>
      <div class="post-body"><strong>Билингвальный ASR — уже в станциях и чате с Алисой<br></strong><br>Мы с хорошими новостями — теперь Алиса знает два языкаа: русский и английский! При этом распознавание русского не пострадало, а стало даже лучше. Обновлённая Алиса и поддержит диалог с носителем, и поможет улучшить навыки новичка. Мы ликуем, пользователи в восторге, а вот репетиторы и всем известная сова немного грустят.<br><br>Евгений Ганкович, руководитель группы ASR, рассказал, с какими вызовами столкнулась команда:<br><br>— Необходимо было обучить модель, которая способна работать с новым языком, при этом критически важно было не просадить качество на русском. <br>— Домен английского для русскоговорящих пользователей специфичен и не решается с помощью открытых данных. <br>— End-of-utterance (EOU) по многим причинам работает у англоговорящих пользователей иначе.<br>Разберём, почему нужно было создавать билингвальную модель, а не обучать две отдельные. <br><br>Сложность решения в том, что заранее неизвестно, на каком языке поступит запрос: пользователь может начать на русском, а продолжить на английском или наоборот.<br><br>В теории можно использовать классификатор: задан запрос, система определяет язык и направляет его в соответствующую модель. Но чтобы точно определить язык, придётся подождать несколько секунд. К тому же такая система сложнее в поддержке и плохо справляется со смешанными языками (см. «смотря какой fabric, смотря сколько details»).<br><br>Выходит, что разумный путь — развивать текущий русскоязычный стек до двуязычного и использовать одну модель, которая инкапсулирует логику выбора языка.<br><br>Однако и здесь есть подводные камни. Даже незначительное ухудшение распознавания на русском негативно скажется на пользовательском опыте. Поэтому новую логику в модель нужно добавлять осторожно. Причём улучшения вносятся в две ключевые части голосового стека:<br><br>- End-of-utterance (EOU) — модель на основе аудио и паршального распознавания, которая определяет, когда пользователь закончил говорить. <br>- E2E Seq2Seq на базе трансформеров — модель распознаёт завершённый фрагмент речи на русском или английском языках.<br><br>Чтобы улучшить эти две компоненты, нужны данные. Начать можно с открытых — но это другой домен: и акустика, и пользователи отличаются. Поэтому мы привлекли отдельных людей для создания более подходящих нам данных. Так собрали рабочее решение, но не сразу получили нужное качество.<br><br>Следующим шагом провели тесты на сотрудниках Яндекса, которые использовали колонку с раскатанной технологии. На этой стадии смогли собрать ошибки, необходимые для улучшения модели. Группы, на которые раскатывали технологию, росли по мере улучшения модели, а мы всё тоньше настраивали модель. <br><br>По мере появления данных мы проводили эксперименты с обеими моделями, подбирая датамиксы и гиперпараметры тренировок. И в какой-то момент достигли качества для полноценного распознавания целевых запросов на английском. Интересно, что в этих экспериментах получилось немного улучшить качество русского, так что исходную задачу даже перевыполнили. <br><br>Оставалось разобраться с EOU. Здесь были сложности из-за режима «тьютора», в котором можно вести диалог с Алисой. Пользователи сценария могут делать паузы, растягивать слова, и в таких случаях обычная модель может преждевременно обрезать речь. Дослушивать мы тоже не можем — это может повлиять на другие компоненты и ответы Алисы сильно замедлятся.<br><br>Решение крылось в добавлении в пайплайн EoU более робастной и стабильной модели, способной учитывать паузы и длительность речи. Хотелось бы рассказать о технологии подробнее, но для этого потребуется описать весь пайплайн распознавания — если вам интересно, дайте знать в комментариях.<br><br>В итоге мы получили результат, который стал важной частью большого релиза:<br><br>— Голосовой набор сообщений на английском языке в чате и колонке; <br>— Сценарий «тьютор» на колонке: пользователи могут вести диалог с Алисой, получать фидбек и переводить текст голосом. <br><br>Зовём протестировать, что у нас получилось: попробуйте поговорить с Алисой на английском или скажите: «Алиса, давай практиковать английский».<br><br><em>Евгений Ганкович </em><em><tg-emoji emoji-id="5224192932302565805">❣</tg-emoji></em><em> Специально для </em><a href="https://t.me/speechinfo" rel="nofollow noopener noreferrer"><em>Speech Info</em></a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/87_480.webp" srcset="../assets/media/thumbs/87_480.webp 480w, ../assets/media/87.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="87" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 043 просмотров · 30 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/87" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/87.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="86" data-search="как устроена модель исправления ошибок в нейроредакторе яндекс браузера — часть i в конце сентября в яндекс браузере запустили нейроредактор — это инструмент, который исправляет ошибки в тексте, делает его более читабельным и грамотным. с момента релиза функциями нейроредактирования в браузере воспользовались с 18 миллионов устройств. сегодня ml-разработчик в яндексе никита авдосев расскажет о модели исправления ошибок, которая работает «под капотом» нейроредактора. в первой части разбора поговорим о качестве исправления ошибок и работы с промптом. а во второй — о перфомансе. качество исправления ошибок предполагалось, что модель для исправления будут использовать с целыми текстами или их фрагментами. сценарий такой: вы выделяете текст, а на выходе получаете скорректированный вариант без опечаток, с правильными окончаниями и корректно расставленными знаками препинания. поэтому модель обучали на целых текстах, в исправлении которых llm показывала себя хорошо. однако на практике оказалось, что весьма популярен и другой сценарий — выделить только одно слово с ошибкой и отправить его. это логично, если слово подчеркнул браузер, но изначально инженеры не учли такого варианта. проведенные инженерами проверки показали, что в 41% случаев модель исправляла слово неверно либо не исправила вовсе, потому что у неё не было контекста. результат весьма сомнительный, поэтому инженеры решили исправить эту недоработку. после всех улучшений модели доля ошибок в коррекции коротких текстов и отдельных слов сократилась до 16%. можно задаться вопросом: «а 16% — это много или мало?» для сравнения, в яндексе замерили, как хорошо срабатывает «опечаточник» — отдельный механизм внутри браузера, который отвечает за подсветку неправильно написанных слов, когда вы печатаете, и предлагает варианты исправления (если достаточно в них уверен). это не llm, а алгоритм, который обращается к словарю. задача непростая, но сейчас «опечаточник» отлично справляется с 75% ошибок. значит, в этом плане модель превосходит решение, которое давно себя зарекомендовало. качество работы с промптом в момент выхода нейроредактора наиболее важной новой фичой для разработчиков была возможность работы с промптами. чтобы можно было поставить какую-то задачу — например, переписать текст так, как будто его автор бандит или сократить текст до двух абзацев — и нейросеть постарается её выполнить, в значительной мере опираясь на предложенный текст. после релиза в яндексе учли реальные пользовательские сценарии и обновили модель, сделали больший акцент на популярные задачи. для этого пришлось обновить датасеты для обучения и замеров. однако при таком подходе, когда упор только на популярное, из виду пропадает «хвост» — редкие, нечастотные запросы, которые составляют 15-20% от общего числа. однако и на таких важно фокусироваться, потому что именно на их основе можно почувствовать реальную «умность» моделей и если способ измерения качества на «хвосте» более-менее понятен, то с вопросом дообучения дела обстоят сложнее. ведь задачи в большинстве своем уникальные, креативные и не всегда очевидно сформулированы — вдобавок их мало. научить модель чему-то на основе одного примера почти невозможно. да, она увидит пример, но вероятность того что в будущем она начнёт решать подобные задачи хорошо крайне мала. поэтому инженеры компании сфокусировались на двух направлениях: — генерализация — способность модели решать разные задачи, в том числе те которые раньше не видела. для этого нужно увеличивать разнообразие задач и их формулировок; — создание синтетических данных. речь об обучающих примерах, сгенерированных более крупными моделями. за счёт синтетики инженеры сумели количественно и качественно расширить хвост креативных запросов. в датасет из текстов и промптов добавили примерно 5 тысяч примеров. и теперь запросы вроде «перепиши как гопник» стали работать креативнее, чем раньше. ml underhood как устроена модель исправления ошибок в нейроредакторе яндекс браузера — часть i в конце сентября в яндекс браузере запустили нейроредактор — это инструмент, который исправляет ошибки в тексте, делает его более читабельным и грамотным. с момента релиза функциями нейроредактирования в браузере воспользовались с 18 миллионов устройств. сегодня ml-разработчик в яндексе никита авдосев расскажет о модели исправления ошибок, которая работает «под капотом» нейроредактора. в первой части разбора поговорим о качестве исправления ошибок и работы с промптом. а во второй — о перфомансе. качество исправления ошибок предполагалось, что модель для исправления будут использовать с целыми текстами или их фрагментами. сценарий такой: вы выделяете текст, а на выходе получаете скорректированный вариант без опечаток, с правильными окончаниями и корректно расставленными знаками препинания. поэтому модель обучали на целых текстах, в исправлении которых llm показывала себя хорошо. однако на практике оказалось, что весьма популярен и другой сценарий — выделить только одно слово с ошибкой и отправить его. это логично, если слово подчеркнул браузер, но изначально инженеры не учли такого варианта. проведенные инженерами проверки показали, что в 41% случаев модель исправляла слово неверно либо не исправила вовсе, потому что у неё не было контекста. результат весьма сомнительный, поэтому инженеры решили исправить эту недоработку. после всех улучшений модели доля ошибок в коррекции коротких текстов и отдельных слов сократилась до 16%. можно задаться вопросом: «а 16% — это много или мало?» для сравнения, в яндексе замерили, как хорошо срабатывает «опечаточник» — отдельный механизм внутри браузера, который отвечает за подсветку неправильно написанных слов, когда вы печатаете, и предлагает варианты исправления (если достаточно в них уверен). это не llm, а алгоритм, который обращается к словарю. задача непростая, но сейчас «опечаточник» отлично справляется с 75% ошибок. значит, в этом плане модель превосходит решение, которое давно себя зарекомендовало. качество работы с промптом в момент выхода нейроредактора наиболее важной новой фичой для разработчиков была возможность работы с промптами. чтобы можно было поставить какую-то задачу — например, переписать текст так, как будто его автор бандит или сократить текст до двух абзацев — и нейросеть постарается её выполнить, в значительной мере опираясь на предложенный текст. после релиза в яндексе учли реальные пользовательские сценарии и обновили модель, сделали больший акцент на популярные задачи. для этого пришлось обновить датасеты для обучения и замеров. однако при таком подходе, когда упор только на популярное, из виду пропадает «хвост» — редкие, нечастотные запросы, которые составляют 15-20% от общего числа. однако и на таких важно фокусироваться, потому что именно на их основе можно почувствовать реальную «умность» моделей и если способ измерения качества на «хвосте» более-менее понятен, то с вопросом дообучения дела обстоят сложнее. ведь задачи в большинстве своем уникальные, креативные и не всегда очевидно сформулированы — вдобавок их мало. научить модель чему-то на основе одного примера почти невозможно. да, она увидит пример, но вероятность того что в будущем она начнёт решать подобные задачи хорошо крайне мала. поэтому инженеры компании сфокусировались на двух направлениях: — генерализация — способность модели решать разные задачи, в том числе те которые раньше не видела. для этого нужно увеличивать разнообразие задач и их формулировок; — создание синтетических данных. речь об обучающих примерах, сгенерированных более крупными моделями. за счёт синтетики инженеры сумели количественно и качественно расширить хвост креативных запросов. в датасет из текстов и промптов добавили примерно 5 тысяч примеров. и теперь запросы вроде «перепиши как гопник» стали работать креативнее, чем раньше. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-04-11T09:18:08+00:00" href="./posts/86.html">2025-04-11 09:18 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как устроена модель исправления ошибок в нейроредакторе Яндекс Браузера</strong> <strong>— часть I</strong><br><br>В конце сентября в Яндекс Браузере запустили нейроредактор —  это инструмент, который исправляет ошибки в тексте, делает его более читабельным и грамотным. С момента релиза функциями нейроредактирования в Браузере воспользовались с 18 миллионов устройств.<br><br>Сегодня ML-разработчик в Яндексе Никита Авдосев  расскажет о модели исправления ошибок, которая работает «под капотом» нейроредактора. В первой части разбора поговорим о качестве исправления ошибок и работы с промптом. А во второй — о перфомансе.<br><br><strong>Качество исправления ошибок</strong><br><br>Предполагалось, что модель для исправления будут использовать с целыми текстами или их фрагментами. Сценарий такой: вы выделяете текст, а на выходе получаете скорректированный вариант без опечаток, с правильными окончаниями и корректно расставленными знаками препинания. Поэтому модель обучали на целых текстах, в исправлении которых LLM показывала себя хорошо.<br><br>Однако на практике оказалось, что весьма популярен и другой сценарий — выделить только одно слово с ошибкой и отправить его. Это логично, если слово подчеркнул браузер, но изначально инженеры не учли такого варианта. <br><br>Проведенные инженерами проверки показали, что в 41% случаев модель исправляла слово неверно либо не исправила вовсе, потому что у неё не было контекста. Результат весьма сомнительный, поэтому инженеры решили исправить эту недоработку. После всех улучшений модели доля ошибок в коррекции коротких текстов и отдельных слов сократилась до 16%.<br><br>Можно задаться вопросом: «А 16% — это много или мало?» Для сравнения, в Яндексе замерили, как хорошо срабатывает «опечаточник» —  отдельный механизм внутри Браузера, который отвечает за подсветку неправильно написанных слов, когда вы печатаете, и предлагает варианты исправления (если достаточно в них уверен). Это не LLM, а алгоритм, который обращается к словарю. Задача непростая, но сейчас «опечаточник» отлично справляется с 75% ошибок. Значит, в этом плане модель превосходит решение, которое давно себя зарекомендовало.<br><br><strong>Качество работы с промптом</strong><br><br>В момент выхода нейроредактора наиболее важной новой фичой для разработчиков была возможность работы с промптами. Чтобы можно было поставить какую-то задачу — например, переписать текст так, как будто его автор бандит или сократить текст до двух абзацев  — и нейросеть постарается её выполнить, в значительной мере опираясь на предложенный текст. <br><br>После релиза в Яндексе учли реальные пользовательские сценарии и обновили модель, сделали больший акцент на популярные задачи. Для этого пришлось обновить датасеты для обучения и замеров. <br><br>Однако при таком подходе, когда упор только на популярное, из виду пропадает «хвост» — редкие, нечастотные запросы, которые составляют 15-20% от общего числа. Однако и на таких важно фокусироваться, потому что именно на их основе можно почувствовать реальную «умность» моделей<br><br>И если способ измерения качества на «хвосте» более-менее понятен, то с вопросом дообучения дела обстоят сложнее. Ведь задачи в большинстве своем уникальные, креативные и не всегда очевидно сформулированы — вдобавок их мало. Научить модель чему-то на основе одного примера почти невозможно. Да, она увидит пример, но вероятность того что в будущем она начнёт решать подобные задачи хорошо крайне мала.<br><br>Поэтому инженеры компании сфокусировались на двух направлениях: <br><br><strong>— генерализация </strong>— способность модели решать разные задачи, в том числе те которые раньше не видела. Для этого нужно увеличивать разнообразие задач и их формулировок;<br><strong>— создание синтетических данных.</strong> Речь об обучающих примерах, сгенерированных более крупными моделями.<br><br>За счёт синтетики инженеры сумели количественно и качественно расширить хвост креативных запросов. В датасет из текстов и промптов добавили примерно 5 тысяч примеров. И теперь запросы вроде «перепиши как гопник» стали работать креативнее, чем раньше. <br><br><a href="https://t.me/+_fN6fXOsa7g3ZDNi" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 515 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/86" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/86.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="83" data-search="yandexgpt 5 lite instruct теперь в опенсорсе 🎉 в феврале в открытый доступ вышла pretrain-версия, а сейчас очередь дошла и до yandexgpt 5 lite instruct. это модель на 8 миллиардов параметров с размером контекстного окна в 32к токенов. о претрейне мы уже писали вот тут, а алайнмент аналогичен тому, через который проходит yandexgpt 5 pro. на этапе sft концентрировались на сложных запросах, а также методах фильтрации и ранжирования данных. в рамках rlhf комбинировали rl-подходы, которые дают лучшие результаты: dpo, logdpo и ppo. подробнее об этом читайте на хабре. по результатам внутреннего слепого попарного сравнения (side-by-side) новая модель yandexgpt 5 lite превосходит qwen-2.5-7b-instruct в 62% случаев и не уступает gpt-4o mini в решении стандартных задач сервисов яндекса. показатели бенчмарков можно посмотреть в таблице. а ещё обновили лицензию: теперь можно использовать модель не только в некоммерческих целях, но и в коммерческих до 10 миллионов выходных токенов в месяц. если ваши объёмы выше, напишите на почту, указанную в тексте лицензии. модель доступна на hugging face. там же есть и квантизованная версия с поддержкой gguf. yandexgpt 5 lite instruct совместима с llama.cpp и ollama. ml underhood yandexgpt 5 lite instruct теперь в опенсорсе 🎉 в феврале в открытый доступ вышла pretrain-версия, а сейчас очередь дошла и до yandexgpt 5 lite instruct. это модель на 8 миллиардов параметров с размером контекстного окна в 32к токенов. о претрейне мы уже писали вот тут , а алайнмент аналогичен тому, через который проходит yandexgpt 5 pro. на этапе sft концентрировались на сложных запросах, а также методах фильтрации и ранжирования данных. в рамках rlhf комбинировали rl-подходы, которые дают лучшие результаты: dpo, logdpo и ppo. подробнее об этом читайте на хабре . по результатам внутреннего слепого попарного сравнения (side-by-side) новая модель yandexgpt 5 lite превосходит qwen-2.5-7b-instruct в 62% случаев и не уступает gpt-4o mini в решении стандартных задач сервисов яндекса. показатели бенчмарков можно посмотреть в таблице. а ещё обновили лицензию: теперь можно использовать модель не только в некоммерческих целях, но и в коммерческих до 10 миллионов выходных токенов в месяц. если ваши объёмы выше, напишите на почту, указанную в тексте лицензии. модель доступна на hugging face . там же есть и квантизованная версия с поддержкой gguf . yandexgpt 5 lite instruct совместима с llama.cpp и ollama. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-31T08:43:38+00:00" href="./posts/83.html">2025-03-31 08:43 UTC</a></div>
      </div>
      <div class="post-body"><strong>YandexGPT 5 Lite Instruct теперь в опенсорсе 🎉</strong><br><br>В феврале в открытый доступ вышла Pretrain-версия, а сейчас очередь дошла и до YandexGPT 5 Lite Instruct. Это модель на 8 миллиардов параметров с размером контекстного окна в 32К токенов. <br><br>О претрейне мы уже писали <a href="https://t.me/MLunderhood/70" rel="nofollow noopener noreferrer">вот тут</a>, а алайнмент аналогичен тому, через который проходит YandexGPT 5 Pro. На этапе SFT концентрировались на сложных запросах, а также методах фильтрации и ранжирования данных. В рамках RLHF комбинировали RL-подходы, которые дают лучшие результаты: DPO, LogDPO и PPO. Подробнее об этом <a href="https://habr.com/ru/companies/yandex/articles/885218/" rel="nofollow noopener noreferrer">читайте на Хабре</a>.<br><br>По результатам внутреннего слепого попарного сравнения (side-by-side) новая модель YandexGPT 5 Lite превосходит Qwen-2.5-7B-instruct в 62% случаев и не уступает GPT-4o mini в решении стандартных задач сервисов Яндекса. Показатели бенчмарков можно посмотреть в таблице.<br><br>А ещё обновили лицензию: теперь можно использовать модель не только в некоммерческих целях, но и в коммерческих до 10 миллионов выходных токенов в месяц. Если ваши объёмы выше, напишите на почту, указанную в тексте лицензии.<br><br>Модель доступна на <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct" rel="nofollow noopener noreferrer">Hugging Face</a>. Там же есть и <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-instruct-GGUF" rel="nofollow noopener noreferrer">квантизованная версия с поддержкой GGUF</a><a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain-GGUF" rel="nofollow noopener noreferrer">.</a> YandexGPT 5 Lite Instruct совместима с llama.cpp и Ollama. <br><br><a href="https://t.me/+x1iAxBkfmVZjNzIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/83_480.webp" srcset="../assets/media/thumbs/83_480.webp 480w, ../assets/media/83.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="83" data-image-index="0" /></div></div>
      <div class="actions">
        <span>16 896 просмотров · 65 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/83" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/83.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="82" data-search="как ml рассаживает деревья в яндекс картах год назад в яндекс картах в москве и петербурге появились трёхмерные деревья, которые добавляют реалистичности и помогают пользователям лучше ориентироваться на местности. в этом посте стас лебедев, разработчик группы ai-картографирования, рассказывает, как устроен ml, который рассаживает деревья в картах. разработанная модель умеет три вещи: определять деревья на аэросъёмке, отличать лиственные породы от хвойных и оценивать размеры деревьев. каждому дереву подбирается подходящая 3d-модель, которую размещают на карте. фактически моделей всего две: лиственная или еловая, а для эффекта разнообразия они масштабируются и немного поворачиваются. работа с данными основная сложность была в том, чтобы собрать и правильно обработать данные. нашли несколько источников разметки — каждый со своими нюансами. в итоге использовали два: — разметку от сообщества картографов «народная карта» — она содержала несколько тысяч деревьев с достаточной точностью, но этого было мало для обучения модели; — один из публичных датасетов геоданных — показал хорошую точность, но низкую полноту; немного уступал данным от картографов и отличался по параметрам съёмки. обучение модель предобучили на публичном датасете и разметке картографов. она смогла базово отличать деревья, но качество предсказаний было низким, особенно по полноте. помогла доразметка с «активным обучением»: прицельно размечали изображения, на которых модель часто сомневалась и возвращала предсказания рядом с порогом принятия решения. за два месяца дополнительно разметили около 60 тысяч деревьев в москве, петербурге и калининграде. при этом модель определила 4 миллиона деревьев за два дня — это показывает, как автоматизация сокращает трудозатраты на разметку данных. архитектура в основе архитектуры — классическая unet-модель с resnet в качестве бэкбона. нейросеть предсказывала попиксельную карту вероятностей наличия дерева в каждой точке. изначально поиск деревьев был тесно связан с поиском точечной дорожной разметки (стрелочки на асфальте). на абстрактном уровне — это очень похожие вещи. поэтому разработанные решения базируются на одних и тех же идеях, почерпнутых из этой статьи. проблему с недооценкой количества деревьев решали с помощью focal loss — модифицированной кросс-энтропийной функции, которая увеличивает влияние сложных для локализации объектов. дополнительно повысили вес ошибок, связанных с пропусками, чтобы модель не игнорировала малозаметные деревья. без такого перераспределения потерь предсказания смещались в сторону фона — то есть модель чаще выбирала класс «нет дерева», чем «есть дерево». модель научилась хорошо определять, где находится дерево, но также ей нужно было понимать, какого оно типа и какая 3d-модель для него нужна. а для этого надо понять ширину и высоту. мы обратили внимание на модель deepforest, которая плохо находила центры, но хорошо предсказывала ширину. решили объединить усилия: нашей моделькой находили локализацию деревьев, а deepforest просили сказать, какой они ширины. в результате получили данные, на которых смогли обучить модель предсказывать ширину по локализации: где находится дерево и как выглядит этот маленький кусочек снимка. благодаря картографам у нас также были данные вида: «это дерево, и оно имеет ширину х и высоту y». мы уже научились находить дерево и определять его ширину. осталось взять имеющиеся данные и научиться с их помощью предсказывать высоту. вуаля — мы получили модель, которая умеет локализовывать (находить местоположение) + вычислять ширину (по локализации) + вычислять высоту (по ширине и тому, как дерево выглядит). результаты и планы в итоге модель помогла разметить для москвы почти 3 млн деревьев, а для петербурга — 1,1 млн деревьев. сейчас система работает на аэросъемке, но в будущем есть планы перевести её на спутниковые снимки. это ускорило бы обновление карт, поскольку спутниковая съёмка дешевле и проводится чаще. однако разрешение спутниковых снимков ниже, и для такого перехода нужны дополнительные исследования и более сложные модели. ml underhood как ml рассаживает деревья в яндекс картах год назад в яндекс картах в москве и петербурге появились трёхмерные деревья, которые добавляют реалистичности и помогают пользователям лучше ориентироваться на местности. в этом посте стас лебедев, разработчик группы ai-картографирования, рассказывает, как устроен ml, который рассаживает деревья в картах. разработанная модель умеет три вещи: определять деревья на аэросъёмке, отличать лиственные породы от хвойных и оценивать размеры деревьев. каждому дереву подбирается подходящая 3d-модель, которую размещают на карте. фактически моделей всего две: лиственная или еловая, а для эффекта разнообразия они масштабируются и немного поворачиваются. работа с данными основная сложность была в том, чтобы собрать и правильно обработать данные. нашли несколько источников разметки — каждый со своими нюансами. в итоге использовали два: — разметку от сообщества картографов «народная карта» — она содержала несколько тысяч деревьев с достаточной точностью, но этого было мало для обучения модели; — один из публичных датасетов геоданных — показал хорошую точность, но низкую полноту; немного уступал данным от картографов и отличался по параметрам съёмки. обучение модель предобучили на публичном датасете и разметке картографов. она смогла базово отличать деревья, но качество предсказаний было низким, особенно по полноте. помогла доразметка с «активным обучением»: прицельно размечали изображения, на которых модель часто сомневалась и возвращала предсказания рядом с порогом принятия решения. за два месяца дополнительно разметили около 60 тысяч деревьев в москве, петербурге и калининграде. при этом модель определила 4 миллиона деревьев за два дня — это показывает, как автоматизация сокращает трудозатраты на разметку данных. архитектура в основе архитектуры — классическая unet-модель с resnet в качестве бэкбона. нейросеть предсказывала попиксельную карту вероятностей наличия дерева в каждой точке. изначально поиск деревьев был тесно связан с поиском точечной дорожной разметки (стрелочки на асфальте). на абстрактном уровне — это очень похожие вещи. поэтому разработанные решения базируются на одних и тех же идеях, почерпнутых из этой статьи . проблему с недооценкой количества деревьев решали с помощью focal loss — модифицированной кросс-энтропийной функции, которая увеличивает влияние сложных для локализации объектов. дополнительно повысили вес ошибок, связанных с пропусками, чтобы модель не игнорировала малозаметные деревья. без такого перераспределения потерь предсказания смещались в сторону фона — то есть модель чаще выбирала класс «нет дерева», чем «есть дерево». модель научилась хорошо определять, где находится дерево, но также ей нужно было понимать, какого оно типа и какая 3d-модель для него нужна. а для этого надо понять ширину и высоту. мы обратили внимание на модель deepforest, которая плохо находила центры, но хорошо предсказывала ширину. решили объединить усилия: нашей моделькой находили локализацию деревьев, а deepforest просили сказать, какой они ширины. в результате получили данные, на которых смогли обучить модель предсказывать ширину по локализации: где находится дерево и как выглядит этот маленький кусочек снимка. благодаря картографам у нас также были данные вида: «это дерево, и оно имеет ширину х и высоту y». мы уже научились находить дерево и определять его ширину. осталось взять имеющиеся данные и научиться с их помощью предсказывать высоту. вуаля — мы получили модель, которая умеет локализовывать (находить местоположение) + вычислять ширину (по локализации) + вычислять высоту (по ширине и тому, как дерево выглядит). результаты и планы в итоге модель помогла разметить для москвы почти 3 млн деревьев, а для петербурга — 1,1 млн деревьев. сейчас система работает на аэросъемке, но в будущем есть планы перевести её на спутниковые снимки. это ускорило бы обновление карт, поскольку спутниковая съёмка дешевле и проводится чаще. однако разрешение спутниковых снимков ниже, и для такого перехода нужны дополнительные исследования и более сложные модели. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-27T07:33:59+00:00" href="./posts/82.html">2025-03-27 07:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как ML рассаживает деревья в Яндекс Картах<br></strong><br>Год назад в Яндекс Картах в Москве и Петербурге появились трёхмерные деревья, которые добавляют реалистичности и помогают пользователям лучше ориентироваться на местности. В этом посте Стас Лебедев, разработчик группы AI-картографирования, рассказывает, как устроен ML, который рассаживает деревья в Картах.<br><br>Разработанная модель умеет три вещи: определять деревья на аэросъёмке, отличать лиственные породы от хвойных и оценивать размеры деревьев. Каждому дереву подбирается подходящая 3D-модель, которую размещают на карте. Фактически моделей всего две: лиственная или еловая, а для эффекта разнообразия они масштабируются и немного поворачиваются.<br><br><strong>Работа с данными</strong><br><br>Основная сложность была в том, чтобы собрать и правильно обработать данные. Нашли несколько источников разметки — каждый со своими нюансами. В итоге использовали два:<br><br>— разметку от сообщества картографов «Народная карта» — она содержала несколько тысяч деревьев с достаточной точностью, но этого было мало для обучения модели;<br>— один из публичных датасетов геоданных — показал хорошую точность, но низкую полноту; немного уступал данным от картографов и отличался по параметрам съёмки.<br><br><strong>Обучение<br></strong><br>Модель предобучили на публичном датасете и разметке картографов. Она смогла базово отличать деревья, но качество предсказаний было низким, особенно по полноте. Помогла доразметка с «активным обучением»: прицельно размечали изображения, на которых модель часто сомневалась и возвращала предсказания рядом с порогом принятия решения.<br><br>За два месяца дополнительно разметили около 60 тысяч деревьев в Москве, Петербурге и Калининграде. При этом модель определила 4 миллиона деревьев за два дня — это показывает, как автоматизация сокращает трудозатраты на разметку данных.<br><br><strong>Архитектура<br></strong><br>В основе архитектуры — классическая UNet-модель с ResNet в качестве бэкбона. Нейросеть предсказывала попиксельную карту вероятностей наличия дерева в каждой точке. Изначально поиск деревьев был тесно связан с поиском точечной дорожной разметки (стрелочки на асфальте). На абстрактном уровне — это очень похожие вещи. Поэтому разработанные решения базируются на одних и тех же идеях, почерпнутых из этой <a href="https://arxiv.org/pdf/2008.07043" rel="nofollow noopener noreferrer">статьи</a>. <br><br>Проблему с недооценкой количества деревьев решали с помощью focal loss — модифицированной кросс-энтропийной функции, которая увеличивает влияние сложных для локализации объектов. Дополнительно повысили вес ошибок, связанных с пропусками, чтобы модель не игнорировала малозаметные деревья. Без такого перераспределения потерь предсказания смещались в сторону фона — то есть модель чаще выбирала класс «нет дерева», чем «есть дерево».<br><br>Модель научилась хорошо определять, где находится дерево, но также ей нужно было понимать, какого оно типа и какая 3D-модель для него нужна. А для этого надо понять ширину и высоту. Мы обратили внимание на модель DeepForest, которая плохо находила центры, но хорошо предсказывала ширину. Решили объединить усилия: нашей моделькой находили локализацию деревьев, а DeepForest просили сказать, какой они ширины. В результате получили данные, на которых смогли обучить модель предсказывать ширину по локализации: где находится дерево и как выглядит этот маленький кусочек снимка.<br><br>Благодаря картографам у нас также были данные вида: «это дерево, и оно имеет ширину Х и высоту Y». Мы уже научились находить дерево и определять его ширину. Осталось взять имеющиеся данные и научиться с их помощью предсказывать высоту. Вуаля — мы получили модель, которая умеет локализовывать (находить местоположение) + вычислять ширину (по локализации) + вычислять высоту (по ширине и тому, как дерево выглядит). <br><br><strong>Результаты и планы<br></strong><br>В итоге модель помогла разметить для Москвы почти 3 млн деревьев, а для Петербурга — 1,1 млн деревьев. <br>Сейчас система работает на аэросъемке, но в будущем есть планы перевести её на спутниковые снимки. Это ускорило бы обновление карт, поскольку спутниковая съёмка дешевле и проводится чаще. Однако разрешение спутниковых снимков ниже, и для такого перехода нужны дополнительные исследования и более сложные модели.<br><br><a href="https://t.me/+aC8hI-0dTh43ZGQ6" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/82_480.webp" srcset="../assets/media/thumbs/82_480.webp 480w, ../assets/media/82.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="82" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 974 просмотров · 47 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/82" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/82.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="78" data-search="личный опыт инженеров яндекса — никита киселёв сотрудники компании продолжают рассказывать нашему каналу о своей работе, успехах и вызовах. сегодня на очереди никита киселёв, руководитель службы любви к дискавери в яндекс картах. #yamlpeople ml underhood личный опыт инженеров яндекса — никита киселёв сотрудники компании продолжают рассказывать нашему каналу о своей работе, успехах и вызовах. сегодня на очереди никита киселёв, руководитель службы любви к дискавери в яндекс картах. #yamlpeople ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-10T13:25:10+00:00" href="./posts/78.html">2025-03-10 13:25 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личный опыт инженеров Яндекса — Никита Киселёв</strong><br><br>Сотрудники компании продолжают рассказывать нашему каналу о своей работе, успехах и вызовах. Сегодня на очереди Никита Киселёв, руководитель службы любви к дискавери в Яндекс Картах. <br><br>#YaMLpeople<br><br><a href="https://t.me/+aC8hI-0dTh43ZGQ6" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/78_480.webp" srcset="../assets/media/thumbs/78_480.webp 480w, ../assets/media/78.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/79_480.webp" srcset="../assets/media/thumbs/79_480.webp 480w, ../assets/media/79.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/80_480.webp" srcset="../assets/media/thumbs/80_480.webp 480w, ../assets/media/80.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/81_480.webp" srcset="../assets/media/thumbs/81_480.webp 480w, ../assets/media/81.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="78" data-image-index="3" /></div></div>
      <div class="actions">
        <span>2 799 просмотров · 17 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/78" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/78.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="77" data-search="как и зачем алису учат понимать интонации алису учат откликаться не только на её имя, но и на определённые команды и даже интонации. причём вторая задача — на порядок сложнее. в этом посте дмитрий солодуха, руководитель команды голосовой активации, рассказал, как она решается. голосовая активация — это распознавание фраз в потоке звука, поступающих на микрофоны умного устройства. а модель, которая решает задачу голосовой активации, называют споттером. в яндексе работают над командными и интонационными споттерами. и если первые в проде уже какое-то время, то вторые — только с 27 февраля этого года и пока только на станции миди. как устроен споттер в целом: 1. на вход поступает сырой сигнал с частотой 16 кгц. 2. преобразуем его в спектрограммы с помощью оконного преобразования фурье. это позволяет перейти от временной размерности к частотной. 3. затем уменьшаем размерность, используя мел-шкалу и логарифмирование. после этого можно подавать данные в свёрточную сеть. мы используем свёрточную сеть до 1 млн параметров, похожую на mobilenet, но с одномерными depthwiseseparable свёртками вместо двумерных. линейные слои заменяем их низкоранговым приближением, а вместо swish берём hard-swish — его адаптацию, которую удобно вычислять на железе. идея интонационного споттера в какой-то момент базовый споттер улучшили настолько, что он стал отличать произнесённое в девайс слово «алиса» от обращённого к человеку. мы подумали, что можно пойти дальше и обучить другой споттер понимать по интонации, что нужно активироваться и отправить запрос на сервер. это упростит жизнь пользователям и позволит нам сэкономить на произносимых «алисах». пользователи часто забывают произносить «алиса» перед запросом, то есть ожидают от неё диалога. умение активироваться на интонацию становится точкой входа в общение ассистента с пользователем. но здесь столкнулись с проблемами: неочевидно, откуда брать данные, нет готовой разметки, а также непонятно, в какой момент устройство должно активироваться. сначала попробовали использовать данные, которые наговорили асессоры, но из-за того, что люди использовали неестественные интонации, датасет выходил плохим. тогда решили взять данные от asr — не только из активаций, но и из дослушиваний — режима, в котором колонка проактивно продолжает диалог. например, если я спрашиваю: «алиса, какая погода в минске?», она отвечает и уточняет: «а хотите узнать погоду в белграде?». при этом пользователь не говорит «алиса» повторно. это уже похоже на естественный диалог, хотя и не лишено ограничений, которых не будет у интонационного споттера: дослушивания работают не на каждый запрос и ждут пользователя только в коротком интервале около 3–5 секунд. мы пересэмплировали полученные данные, чтобы убрать смещение в сторону популярных запросов, и получили нужный датасет. для разметки использовали решение соседней команды asr — классификацию на side-speech. суть в том, что asr пытается на последнем этапе своей работы понять, действительно ли речь имела полезный смысл. мы немного доработали исходные метки и получили для себя псевдолейблы, которые буквально говорят нам, подходящая интонация для активации или нет. на видео показано, как интонационный споттер работает и решает более сложные задачи, чем стандартная активация на имя. в итоге это позволяет алисе быть более человечной в диалоге. ml underhood как и зачем алису учат понимать интонации алису учат откликаться не только на её имя, но и на определённые команды и даже интонации. причём вторая задача — на порядок сложнее. в этом посте дмитрий солодуха, руководитель команды голосовой активации, рассказал, как она решается. голосовая активация — это распознавание фраз в потоке звука, поступающих на микрофоны умного устройства. а модель, которая решает задачу голосовой активации, называют споттером. в яндексе работают над командными и интонационными споттерами. и если первые в проде уже какое-то время, то вторые — только с 27 февраля этого года и пока только на станции миди. как устроен споттер в целом: 1. на вход поступает сырой сигнал с частотой 16 кгц. 2. преобразуем его в спектрограммы с помощью оконного преобразования фурье. это позволяет перейти от временной размерности к частотной. 3. затем уменьшаем размерность, используя мел-шкалу и логарифмирование. после этого можно подавать данные в свёрточную сеть. мы используем свёрточную сеть до 1 млн параметров, похожую на mobilenet, но с одномерными depthwiseseparable свёртками вместо двумерных. линейные слои заменяем их низкоранговым приближением, а вместо swish берём hard-swish — его адаптацию, которую удобно вычислять на железе. идея интонационного споттера в какой-то момент базовый споттер улучшили настолько, что он стал отличать произнесённое в девайс слово «алиса» от обращённого к человеку. мы подумали, что можно пойти дальше и обучить другой споттер понимать по интонации, что нужно активироваться и отправить запрос на сервер. это упростит жизнь пользователям и позволит нам сэкономить на произносимых «алисах». пользователи часто забывают произносить «алиса» перед запросом, то есть ожидают от неё диалога. умение активироваться на интонацию становится точкой входа в общение ассистента с пользователем. но здесь столкнулись с проблемами: неочевидно, откуда брать данные, нет готовой разметки, а также непонятно, в какой момент устройство должно активироваться. сначала попробовали использовать данные, которые наговорили асессоры, но из-за того, что люди использовали неестественные интонации, датасет выходил плохим. тогда решили взять данные от asr — не только из активаций, но и из дослушиваний — режима, в котором колонка проактивно продолжает диалог. например, если я спрашиваю: «алиса, какая погода в минске?», она отвечает и уточняет: «а хотите узнать погоду в белграде?». при этом пользователь не говорит «алиса» повторно. это уже похоже на естественный диалог, хотя и не лишено ограничений, которых не будет у интонационного споттера: дослушивания работают не на каждый запрос и ждут пользователя только в коротком интервале около 3–5 секунд. мы пересэмплировали полученные данные, чтобы убрать смещение в сторону популярных запросов, и получили нужный датасет. для разметки использовали решение соседней команды asr — классификацию на side-speech. суть в том, что asr пытается на последнем этапе своей работы понять, действительно ли речь имела полезный смысл. мы немного доработали исходные метки и получили для себя псевдолейблы, которые буквально говорят нам, подходящая интонация для активации или нет. на видео показано, как интонационный споттер работает и решает более сложные задачи, чем стандартная активация на имя. в итоге это позволяет алисе быть более человечной в диалоге. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-03-03T07:33:37+00:00" href="./posts/77.html">2025-03-03 07:33 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как и зачем Алису учат понимать интонации</strong><br><br>Алису учат откликаться не только на её имя, но и на определённые команды и даже интонации. Причём вторая задача — на порядок сложнее. В этом посте Дмитрий Солодуха, руководитель команды голосовой активации, рассказал, как она решается.<br><br>Голосовая активация — это распознавание фраз в потоке звука, поступающих на микрофоны умного устройства. А модель, которая решает задачу голосовой активации, называют споттером. В Яндексе работают над командными и интонационными споттерами. И если первые в проде уже какое-то время, то вторые — только с 27 февраля этого года и пока только на Станции Миди.<br><br><strong>Как устроен споттер в целом:</strong><br><br>1. На вход поступает сырой сигнал с частотой 16 кГц.<br>2. Преобразуем его в спектрограммы с помощью оконного преобразования Фурье. Это позволяет перейти от временной размерности к частотной.<br>3. Затем уменьшаем размерность, используя мел-шкалу и логарифмирование.<br><br>После этого можно подавать данные в свёрточную сеть. Мы используем свёрточную сеть до 1 млн параметров, похожую на MobileNet, но с одномерными DepthwiseSeparable свёртками вместо двумерных. Линейные слои заменяем их низкоранговым приближением, а вместо Swish берём Hard-Swish — его адаптацию, которую удобно вычислять на железе.<br><br><strong>Идея интонационного споттера</strong><br><br>В какой-то момент базовый споттер улучшили настолько, что он стал отличать произнесённое в девайс слово «Алиса» от обращённого к человеку. Мы подумали, что можно пойти дальше и обучить другой споттер понимать по интонации, что нужно активироваться и отправить запрос на сервер. Это упростит жизнь пользователям и позволит нам сэкономить на произносимых «Алисах». <br><br>Пользователи часто забывают произносить «Алиса» перед запросом, то есть ожидают от неё диалога. Умение активироваться на интонацию становится точкой входа в общение ассистента с пользователем. <br><br>Но здесь столкнулись с проблемами: неочевидно, откуда брать данные, нет готовой разметки, а также непонятно, в какой момент устройство должно активироваться.<br><br>Сначала попробовали использовать данные, которые наговорили асессоры, но из-за того, что люди использовали неестественные интонации, датасет выходил плохим.<br><br>Тогда решили взять данные от ASR — не только из активаций, но и из дослушиваний — режима, в котором колонка проактивно продолжает диалог. Например, если я спрашиваю: «Алиса, какая погода в Минске?», она отвечает и уточняет: «А хотите узнать погоду в Белграде?». При этом пользователь не говорит «Алиса» повторно. Это уже похоже на естественный диалог, хотя и не лишено ограничений, которых не будет у интонационного споттера: дослушивания работают не на каждый запрос и ждут пользователя только в коротком интервале около 3–5 секунд.<br><br>Мы пересэмплировали полученные данные, чтобы убрать смещение в сторону популярных запросов, и получили нужный датасет. <br><br>Для разметки использовали решение соседней команды ASR — классификацию на side-speech. Суть в том, что ASR пытается на последнем этапе своей работы понять, действительно ли речь имела полезный смысл. Мы немного доработали исходные метки и получили для себя псевдолейблы, которые буквально говорят нам, подходящая интонация для активации или нет.<br><br>На видео показано, как интонационный споттер работает и решает более сложные задачи, чем стандартная активация на имя. В итоге это позволяет Алисе быть более человечной в диалоге.<br><br><a href="https://t.me/+9GKzRNPGwYk1OGUy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><video controls preload="metadata" src="../assets/media/77_IMG_6133.MP4.mp4"></video></div></div>
      <div class="actions">
        <span>3 328 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/77" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/77.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="71" data-search="личный опыт инженеров яндекса — петр вытовтов погода в доме, конечно, важна, но нужно и на улицу выходить. а чтобы дождь или снег не застали вас врасплох, стоит ознакомиться с прогнозом. в его создании сегодня ключевую роль играют нейросети, которые анализируют огромные массивы данных. о том, как эти технологии преобразили сервис яндекс погода, а также о своих профессиональных достижениях и вызовах нашему каналу рассказал петр вытовтов — руководитель группы машинного обучения направления. #yamlpeople ml underhood личный опыт инженеров яндекса — петр вытовтов погода в доме, конечно, важна, но нужно и на улицу выходить. а чтобы дождь или снег не застали вас врасплох, стоит ознакомиться с прогнозом. в его создании сегодня ключевую роль играют нейросети, которые анализируют огромные массивы данных. о том, как эти технологии преобразили сервис яндекс погода, а также о своих профессиональных достижениях и вызовах нашему каналу рассказал петр вытовтов — руководитель группы машинного обучения направления. #yamlpeople ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-28T11:16:26+00:00" href="./posts/71.html">2025-02-28 11:16 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личный опыт инженеров Яндекса — Петр Вытовтов<br></strong><br>Погода в доме, конечно, важна, но нужно и на улицу выходить. А чтобы дождь или снег не застали вас врасплох, стоит ознакомиться с прогнозом.<br><br>В его создании сегодня ключевую роль играют нейросети, которые анализируют огромные массивы данных. О том, как эти технологии преобразили сервис Яндекс Погода, а также о своих профессиональных достижениях и вызовах нашему каналу рассказал Петр Вытовтов — руководитель группы машинного обучения направления.<br><br>#YaMLpeople<br><br><a href="https://t.me/+yqeMHEzdpx43ZGNi" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/71_480.webp" srcset="../assets/media/thumbs/71_480.webp 480w, ../assets/media/71.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/72_480.webp" srcset="../assets/media/thumbs/72_480.webp 480w, ../assets/media/72.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/73_480.webp" srcset="../assets/media/thumbs/73_480.webp 480w, ../assets/media/73.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/74_480.webp" srcset="../assets/media/thumbs/74_480.webp 480w, ../assets/media/74.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/75_480.webp" srcset="../assets/media/thumbs/75_480.webp 480w, ../assets/media/75.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/76_480.webp" srcset="../assets/media/thumbs/76_480.webp 480w, ../assets/media/76.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="71" data-image-index="5" /></div></div>
      <div class="actions">
        <span>2 422 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/71" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/71.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="70" data-search="yandexgpt 5 уже в опенсорсе и алисе сегодня яндекс показал миру новое поколение больших языковых моделей — yandexgpt 5. старшая модель yandexgpt 5 pro доступна в чате с алисой и yandex cloud через api. ну а претрейн-версия младшей модели yandexgpt 5 lite pretrain — уже лежит на hugging face. все подробности о процессе обучения можно прочитать в статье на хабре. а в этом посте — главные факты о свежей опенсорсной модели яндекса. yandexgpt 5 lite pretrain — модель на 8 миллиардов параметров с длиной контекста 32 тысячи токенов. претрейн проходил в два этапа: сначала модель обучили на 15 триллионах токенов текста на русском и английском языках, а потом использовали 320 миллиардов токенов высококачественных данных, включая образовательный контент. на первом этапе датасет больше чем на половину состоял из веб-документов, остальное — код, математика и специфичные данные. под последними подразумеваются синтетика (сгенерированные yandexgpt 4 вопросы на основе проверенных источников) и внутренние наработки компании (например, внутренняя база яндекса fact snippet и новый корпус данных переводчика). на втором этапе датасет на четверть состоял из веб-страниц и почти в равных пропорциях содержал математику, код и образовательные данные. также была небольшая часть аугментаций фактовых документов, другой синтетики и датасетов сервисов. по сравнению с моделью предыдущего поколения, yandexgpt 4 lite pretrain, новая модель показывает ощутимый рост качества в решении математических задач и написании кода. а в сравнении с зарубежными аналогами, такими как llama3.1-8b и qwen-2.5-7b-base, она лидирует почти во всех типах задач. ещё раз приглашаем пощупать модель, почитать статью на хабре с деталями обучения и не забыть поделиться впечатлениями в комментариях! ml underhood yandexgpt 5 уже в опенсорсе и алисе сегодня яндекс показал миру новое поколение больших языковых моделей — yandexgpt 5. старшая модель yandexgpt 5 pro доступна в чате с алисой и yandex cloud через api. ну а претрейн-версия младшей модели yandexgpt 5 lite pretrain — уже лежит на hugging face . все подробности о процессе обучения можно прочитать в статье на хабре . а в этом посте — главные факты о свежей опенсорсной модели яндекса. yandexgpt 5 lite pretrain — модель на 8 миллиардов параметров с длиной контекста 32 тысячи токенов. претрейн проходил в два этапа: сначала модель обучили на 15 триллионах токенов текста на русском и английском языках, а потом использовали 320 миллиардов токенов высококачественных данных, включая образовательный контент. на первом этапе датасет больше чем на половину состоял из веб-документов, остальное — код, математика и специфичные данные. под последними подразумеваются синтетика (сгенерированные yandexgpt 4 вопросы на основе проверенных источников) и внутренние наработки компании (например, внутренняя база яндекса fact snippet и новый корпус данных переводчика). на втором этапе датасет на четверть состоял из веб-страниц и почти в равных пропорциях содержал математику, код и образовательные данные. также была небольшая часть аугментаций фактовых документов, другой синтетики и датасетов сервисов. по сравнению с моделью предыдущего поколения, yandexgpt 4 lite pretrain, новая модель показывает ощутимый рост качества в решении математических задач и написании кода. а в сравнении с зарубежными аналогами, такими как llama3.1-8b и qwen-2.5-7b-base, она лидирует почти во всех типах задач. ещё раз приглашаем пощупать модель , почитать статью на хабре с деталями обучения и не забыть поделиться впечатлениями в комментариях! ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-25T10:50:43+00:00" href="./posts/70.html">2025-02-25 10:50 UTC</a></div>
      </div>
      <div class="post-body"><strong>YandexGPT 5 уже в опенсорсе и Алисе</strong><br><br>Сегодня Яндекс показал миру новое поколение больших языковых моделей — YandexGPT 5. Старшая модель YandexGPT 5 Pro доступна в чате с Алисой и Yandex Cloud через API. Ну а претрейн-версия младшей модели YandexGPT 5 Lite Pretrain — уже лежит <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain" rel="nofollow noopener noreferrer">на Hugging Face</a>.<br><br>Все подробности о процессе обучения можно прочитать <a href="https://habr.com/ru/companies/yandex/articles/885218/" rel="nofollow noopener noreferrer">в статье на Хабре</a>. А в этом посте — главные факты о свежей опенсорсной модели Яндекса.<br><br>YandexGPT 5 Lite Pretrain — модель на 8 миллиардов параметров с длиной контекста 32 тысячи токенов. Претрейн проходил в два этапа: сначала модель обучили на 15 триллионах токенов текста на русском и английском языках, а потом использовали 320 миллиардов токенов высококачественных данных, включая образовательный контент.<br><br>На первом этапе датасет больше чем на половину состоял из веб-документов, остальное — код, математика и специфичные данные. Под последними подразумеваются синтетика (сгенерированные YandexGPT 4 вопросы на основе проверенных источников) и внутренние наработки компании (например, внутренняя база Яндекса Fact Snippet и новый корпус данных Переводчика).<br><br>На втором этапе датасет на четверть состоял из веб-страниц и почти в равных пропорциях содержал математику, код и образовательные данные. Также была небольшая часть аугментаций фактовых документов, другой синтетики и датасетов сервисов. <br><br>По сравнению с моделью предыдущего поколения, YandexGPT 4 Lite Pretrain, новая модель показывает ощутимый рост качества в решении математических задач и написании кода. А в сравнении с зарубежными аналогами, такими как LLaMa3.1-8B и Qwen-2.5-7B-base, она лидирует почти во всех типах задач. <br><br>Ещё раз приглашаем <a href="https://huggingface.co/yandex/YandexGPT-5-Lite-8B-pretrain" rel="nofollow noopener noreferrer">пощупать модель</a>, <a href="https://habr.com/ru/companies/yandex/articles/885218/" rel="nofollow noopener noreferrer">почитать статью</a> на Хабре с деталями обучения и не забыть поделиться впечатлениями в комментариях!<br><br><br><a href="https://t.me/+x1iAxBkfmVZjNzIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/70_480.webp" srcset="../assets/media/thumbs/70_480.webp 480w, ../assets/media/70.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="70" data-image-index="0" /></div></div>
      <div class="actions">
        <span>7 648 просмотров · 39 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/70" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/70.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="69" data-search="документный llm-переводчик в яндексе яндекс запустил новую модель для документного перевода на основе yandexgpt. она уже работает в поиске, умной камере и нейропереводчике яндекс браузера, а также заняла первое место в бенчмарке dibimt по переводу с английского на русский. обо всех нюансах работы переводчика и о том, как его создавали, на хабре рассказал руководитель группы базового качества перевода николай карпачёв. а здесь — кратко о главном. документный перевод предполагает адаптацию на другой язык не каждого отдельного предложения, а всего текста. почему это важно? причин несколько. например, английское «you» может означать как «ты», так и «вы», но без контекста модель не понимает, какой вариант выбрать. термины и стилистика могут «прыгать» внутри текста, а пропущенные элементы, понятные носителю языка, в переводе превращаются в бессмысленный набор слов. люди воспринимают текст иначе: мы читаем книги, статьи, субтитры — всё целиком. значит, и машинный перевод должен работать так же. инженеры яндекса попробовали перевести тексты llm-моделью «из коробки», без дообучения, но столкнулись с типичными ошибками: пропущенные фрагменты, лишние добавления, галлюцинации. чтобы этого избежать, модель пришлось адаптировать. на первом этапе подготовили данные, включая не только классические парные предложения, но и переводы документов, полученные автоматическим выравниванием и с помощью синтетики. дообучение проходило в форматах lora и p-tuning. на следующем этапе модель дообучалась с помощью технологии alignment. разные варианты переводов сравнивались редакторами-профессионалами. полученные оценки использовали для оптимизации методом contrastive preference optimization (cpo). на этой стадии происходит исправление существующих ошибок и проблем llm-модели, найденных редакторами. это позволило минимизировать ошибки, связанные с потерей информации и несогласованностью. в итоге по метрике mqm новая модель переводит тексты почти так же хорошо, как человек. количество грубых ошибок сократилось в два раза по сравнению с предыдущей версией, а финальный результат оказался даже лучше gpt-4o. ml underhood документный llm-переводчик в яндексе яндекс запустил новую модель для документного перевода на основе yandexgpt. она уже работает в поиске, умной камере и нейропереводчике яндекс браузера, а также заняла первое место в бенчмарке dibimt по переводу с английского на русский. обо всех нюансах работы переводчика и о том, как его создавали, на хабре рассказал руководитель группы базового качества перевода николай карпачёв. а здесь — кратко о главном. документный перевод предполагает адаптацию на другой язык не каждого отдельного предложения, а всего текста. почему это важно? причин несколько. например, английское «you» может означать как «ты», так и «вы», но без контекста модель не понимает, какой вариант выбрать. термины и стилистика могут «прыгать» внутри текста, а пропущенные элементы, понятные носителю языка, в переводе превращаются в бессмысленный набор слов. люди воспринимают текст иначе: мы читаем книги, статьи, субтитры — всё целиком. значит, и машинный перевод должен работать так же. инженеры яндекса попробовали перевести тексты llm-моделью «из коробки», без дообучения, но столкнулись с типичными ошибками: пропущенные фрагменты, лишние добавления, галлюцинации. чтобы этого избежать, модель пришлось адаптировать. на первом этапе подготовили данные, включая не только классические парные предложения, но и переводы документов, полученные автоматическим выравниванием и с помощью синтетики. дообучение проходило в форматах lora и p-tuning. на следующем этапе модель дообучалась с помощью технологии alignment. разные варианты переводов сравнивались редакторами-профессионалами. полученные оценки использовали для оптимизации методом contrastive preference optimization (cpo). на этой стадии происходит исправление существующих ошибок и проблем llm-модели, найденных редакторами. это позволило минимизировать ошибки, связанные с потерей информации и несогласованностью. в итоге по метрике mqm новая модель переводит тексты почти так же хорошо, как человек. количество грубых ошибок сократилось в два раза по сравнению с предыдущей версией, а финальный результат оказался даже лучше gpt-4o. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-21T10:01:02+00:00" href="./posts/69.html">2025-02-21 10:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Документный LLM-переводчик в Яндексе</strong><br><br>Яндекс запустил новую модель для документного перевода на основе YandexGPT. Она уже работает в Поиске, Умной камере и Нейропереводчике Яндекс Браузера, а также заняла первое место в <a href="https://nlp.uniroma1.it/dibimt/public/leaderboard" rel="nofollow noopener noreferrer">бенчмарке DiBiMT</a> по переводу с английского на русский. Обо всех нюансах работы переводчика и о том, как его создавали, <a href="https://habr.com/ru/companies/yandex/articles/884416/" rel="nofollow noopener noreferrer">на Хабре</a> рассказал руководитель группы базового качества перевода Николай Карпачёв. А здесь — кратко о главном. <br><br>Документный перевод предполагает адаптацию на другой язык не каждого отдельного предложения, а всего текста. Почему это важно? Причин несколько. Например, английское «you» может означать как «ты», так и «вы», но без контекста модель не понимает, какой вариант выбрать. Термины и стилистика могут «прыгать» внутри текста, а пропущенные элементы, понятные носителю языка, в переводе превращаются в бессмысленный набор слов. Люди воспринимают текст иначе: мы читаем книги, статьи, субтитры — всё целиком. Значит, и машинный перевод должен работать так же.<br><br>Инженеры Яндекса попробовали перевести тексты LLM-моделью «из коробки», без дообучения, но столкнулись с типичными ошибками: пропущенные фрагменты, лишние добавления, галлюцинации. Чтобы этого избежать, модель пришлось адаптировать. На первом этапе подготовили данные, включая не только классические парные предложения, но и переводы документов, полученные автоматическим выравниванием и с помощью синтетики. Дообучение проходило в форматах LoRA и P-Tuning. <br><br>На следующем этапе модель дообучалась с помощью технологии alignment. Разные варианты переводов сравнивались редакторами-профессионалами. Полученные оценки использовали для оптимизации методом Contrastive Preference Optimization (CPO). На этой стадии происходит исправление существующих ошибок и проблем LLM-модели, найденных редакторами. Это позволило минимизировать ошибки, связанные с потерей информации и несогласованностью. <br><br>В итоге по метрике MQM новая модель переводит тексты почти так же хорошо, как человек. Количество грубых ошибок сократилось в два раза по сравнению с предыдущей версией, а финальный результат оказался даже лучше GPT-4o.<br><br><a href="https://t.me/+x1iAxBkfmVZjNzIy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/69_480.webp" srcset="../assets/media/thumbs/69_480.webp 480w, ../assets/media/69.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="69" data-image-index="0" /></div></div>
      <div class="actions">
        <span>7 058 просмотров · 43 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/69" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/69.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="62" data-search="личные итоги года инженеров яндекса — максим спорышев середина февраля 2025-го — не помеха для подведения итогов 2024-го. тем более, если они такие интересные, как сегодняшние. ими поделился руководитель группы алайнмента модели планирования движения в яндексе максим спорышев. он рассказал о собственных успехах и о том, чем ему запомнился прошлый год. #yamlpeople ml underhood личные итоги года инженеров яндекса — максим спорышев середина февраля 2025-го — не помеха для подведения итогов 2024-го. тем более, если они такие интересные, как сегодняшние. ими поделился руководитель группы алайнмента модели планирования движения в яндексе максим спорышев. он рассказал о собственных успехах и о том, чем ему запомнился прошлый год. #yamlpeople ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-17T11:13:06+00:00" href="./posts/62.html">2025-02-17 11:13 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личные итоги года инженеров Яндекса — Максим Спорышев</strong><br><br>Середина февраля 2025-го — не помеха для подведения итогов 2024-го. Тем более, если они такие интересные, как сегодняшние. Ими поделился руководитель группы алайнмента модели планирования движения в Яндексе Максим Спорышев. Он рассказал о собственных успехах и о том, чем ему запомнился прошлый год. <br><br>#YaMLpeople<br><br><a href="https://t.me/+M-mq1aAc3rAxYWE6" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/62_480.webp" srcset="../assets/media/thumbs/62_480.webp 480w, ../assets/media/62.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/63_480.webp" srcset="../assets/media/thumbs/63_480.webp 480w, ../assets/media/63.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/64_480.webp" srcset="../assets/media/thumbs/64_480.webp 480w, ../assets/media/64.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/65_480.webp" srcset="../assets/media/thumbs/65_480.webp 480w, ../assets/media/65.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/66_480.webp" srcset="../assets/media/thumbs/66_480.webp 480w, ../assets/media/66.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/67_480.webp" srcset="../assets/media/thumbs/67_480.webp 480w, ../assets/media/67.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="5" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/68_480.webp" srcset="../assets/media/thumbs/68_480.webp 480w, ../assets/media/68.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="62" data-image-index="6" /></div></div>
      <div class="actions">
        <span>2 624 просмотров · 19 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/62" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/62.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="61" data-search="как в яндексе заменили сложную разметку на llm в конце прошлой осени команда качества поиска одной из первых в яндексе смогла существенно удешевить сложную асессорскую разметку за счёт генеративной модели. татьяна климук, руководитель службы исследований и качества ранжирования, рассказала, как работали над технологией. яндекс использует услуги тысяч асессоров, которые каждый день выполняют десятки тысяч заданий по оценке выдачи с точки зрения качества и релевантности. это дорогой, долгий и сложный процесс. идея проекта в том, чтобы отдать рутинную работу по разметке сильной нейронке. при этом мы не отказываемся от асессоров, а переключаем их на разметку более важных и сложных кейсов, а также поручаем контролировать корректность работы модели. архитектура мы начали с экспериментов с базовым претрейном от yandexgpt. на вход подавали сжатую инструкцию, запрос и контент документа, на выходе получали решение о принадлежности к одной из категорий релевантности. однако промптинг даже sota-моделей пока не даёт нужного качества на нестандартных кейсах. инструкция оказывается для них настолько сложной, что без дообучения ни одна модель не справляется с ней. поэтому на старте получилось выжать только 55% качества асессоров. тогда мы сделали ряд улучшений: — взяли претрейн от нейро, который лучше понимает поисковый домен и легче обучается решать поисковые задачи. — обучались не просто на метку класса, но и на подготовленные chain-of-thoughts, чтобы научить модель больше думать перед тем, как она даёт ответ. — добавили внешние данные — знания, необходимые для понимания контекста, которые нельзя извлечь из текста. пример таких знаний — то, какие страницы в сети официальные, а какие — нет. — подавали данные для обучения в нужном порядке — от более мусорных к более качественным. так мы добились качества 102% относительно разметки асессоров, что уже было неплохо. но оставался риск «сломать» поиск — поэтому нужно было проверить модель на разных классах запросов, исключить риск деградации со временем и учесть другие нюансы. решение в итоге мы придумали решение, которое использует оценку как от людей, так и от нейросети. мы стали извлекать из неё не только ответ по инструкции, но ещё и уверенность в этом предсказании. в зависимости от степени уверенности мы принимали решение, использовать ли в задаче человеческий ресурс. — если модель уверена в ответе, скорее всего, задача простая и не требует помощи асессоров. с этими кейсами она нередко справляется даже лучше людей. таких задач оказалось около половины от общей массы. — если модель не до конца уверена в ответе, привлекаем её вместо одного из трёх асессоров. размер этой зоны — около 30%. — когда модель говорит, что совсем не уверена в решении, отдаём задачу трём сильным асессорам — как это происходит в стандартном процессе. таких задач порядка 20%. результаты и планы с помощью этого решения мы получили 105% качества и 60% экономии денег. мы уже используем его экспериментально в разметке обучающих и валидационных пулов для моделей ранжирования, но конечный мониторинг интегрального качества поиска пока остаётся на людях. планируем продолжать наращивать качество и запускаться на новых разметках. также в долгосрочных планах — свести процесс к промптингу, когда ты не обучаешь модель, а описываешь задачу текстом. так мы сможем более гибко менять инструкции разметок без переобучения модели. мы рассчитываем, что решение поможет нам перекинуть рутину на нейронки, а людям давать более интересные и сложные задачи. ml underhood как в яндексе заменили сложную разметку на llm в конце прошлой осени команда качества поиска одной из первых в яндексе смогла существенно удешевить сложную асессорскую разметку за счёт генеративной модели. татьяна климук, руководитель службы исследований и качества ранжирования, рассказала, как работали над технологией. яндекс использует услуги тысяч асессоров, которые каждый день выполняют десятки тысяч заданий по оценке выдачи с точки зрения качества и релевантности. это дорогой, долгий и сложный процесс. идея проекта в том, чтобы отдать рутинную работу по разметке сильной нейронке. при этом мы не отказываемся от асессоров, а переключаем их на разметку более важных и сложных кейсов, а также поручаем контролировать корректность работы модели. архитектура мы начали с экспериментов с базовым претрейном от yandexgpt. на вход подавали сжатую инструкцию, запрос и контент документа, на выходе получали решение о принадлежности к одной из категорий релевантности. однако промптинг даже sota-моделей пока не даёт нужного качества на нестандартных кейсах. инструкция оказывается для них настолько сложной, что без дообучения ни одна модель не справляется с ней. поэтому на старте получилось выжать только 55% качества асессоров. тогда мы сделали ряд улучшений: — взяли претрейн от нейро, который лучше понимает поисковый домен и легче обучается решать поисковые задачи. — обучались не просто на метку класса, но и на подготовленные chain-of-thoughts, чтобы научить модель больше думать перед тем, как она даёт ответ. — добавили внешние данные — знания, необходимые для понимания контекста, которые нельзя извлечь из текста. пример таких знаний — то, какие страницы в сети официальные, а какие — нет. — подавали данные для обучения в нужном порядке — от более мусорных к более качественным. так мы добились качества 102% относительно разметки асессоров, что уже было неплохо. но оставался риск «сломать» поиск — поэтому нужно было проверить модель на разных классах запросов, исключить риск деградации со временем и учесть другие нюансы. решение в итоге мы придумали решение, которое использует оценку как от людей, так и от нейросети. мы стали извлекать из неё не только ответ по инструкции, но ещё и уверенность в этом предсказании. в зависимости от степени уверенности мы принимали решение, использовать ли в задаче человеческий ресурс. — если модель уверена в ответе, скорее всего, задача простая и не требует помощи асессоров. с этими кейсами она нередко справляется даже лучше людей. таких задач оказалось около половины от общей массы. — если модель не до конца уверена в ответе, привлекаем её вместо одного из трёх асессоров. размер этой зоны — около 30%. — когда модель говорит, что совсем не уверена в решении, отдаём задачу трём сильным асессорам — как это происходит в стандартном процессе. таких задач порядка 20%. результаты и планы с помощью этого решения мы получили 105% качества и 60% экономии денег. мы уже используем его экспериментально в разметке обучающих и валидационных пулов для моделей ранжирования, но конечный мониторинг интегрального качества поиска пока остаётся на людях. планируем продолжать наращивать качество и запускаться на новых разметках. также в долгосрочных планах — свести процесс к промптингу, когда ты не обучаешь модель, а описываешь задачу текстом. так мы сможем более гибко менять инструкции разметок без переобучения модели. мы рассчитываем, что решение поможет нам перекинуть рутину на нейронки, а людям давать более интересные и сложные задачи. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-12T10:46:03+00:00" href="./posts/61.html">2025-02-12 10:46 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как в Яндексе заменили сложную разметку на LLM </strong><br><br>В конце прошлой осени Команда качества поиска одной из первых в Яндексе смогла существенно удешевить сложную асессорскую разметку за счёт генеративной модели. Татьяна Климук, руководитель Службы исследований и качества ранжирования, рассказала, как работали над технологией.<br><br>Яндекс использует услуги тысяч асессоров, которые каждый день выполняют десятки тысяч заданий по оценке выдачи с точки зрения качества и релевантности. Это дорогой, долгий и сложный процесс.<br><br>Идея проекта в том, чтобы отдать рутинную работу по разметке сильной нейронке. При этом мы не отказываемся от асессоров, а переключаем их на разметку более важных и сложных кейсов, а также поручаем контролировать корректность работы модели. <br><br><strong>Архитектура<br></strong><br>Мы начали с экспериментов с базовым претрейном от YandexGPT. На вход подавали сжатую инструкцию, запрос и контент документа, на выходе получали решение о принадлежности к одной из категорий релевантности.<br><br>Однако промптинг даже SoTA-моделей пока не даёт нужного качества на нестандартных кейсах. Инструкция оказывается для них настолько сложной, что без дообучения ни одна модель не справляется с ней. Поэтому на старте получилось выжать только 55% качества асессоров.<br><br>Тогда мы сделали ряд улучшений:<br><br>— Взяли претрейн от Нейро, который лучше понимает поисковый домен и легче обучается решать поисковые задачи.<br>— Обучались не просто на метку класса, но и на подготовленные Chain-of-Thoughts, чтобы научить модель больше думать перед тем, как она даёт ответ. <br> — Добавили внешние данные — знания, необходимые для понимания контекста, которые нельзя извлечь из текста. Пример таких знаний — то, какие страницы в сети официальные, а какие — нет.<br>— Подавали данные для обучения в нужном порядке — от более мусорных к более качественным. <br><br>Так мы добились качества 102% относительно разметки асессоров, что уже было неплохо. Но оставался риск «сломать» Поиск — поэтому нужно было проверить модель на разных классах запросов, исключить риск деградации со временем и учесть другие нюансы. <br><br><strong>Решение </strong><br><br>В итоге мы придумали решение, которое использует оценку как от людей, так и от нейросети. Мы стали извлекать из неё не только ответ по инструкции, но ещё и уверенность в этом предсказании. В зависимости от степени уверенности мы принимали решение, использовать ли в задаче человеческий ресурс.<br><br>— Если модель уверена в ответе, скорее всего, задача простая и не требует помощи асессоров. С этими кейсами она нередко справляется даже лучше людей. Таких задач оказалось около половины от общей массы.<br>— Если модель не до конца уверена в ответе, привлекаем её вместо одного из трёх асессоров. Размер этой зоны — около 30%. <br>— Когда модель говорит, что совсем не уверена в решении, отдаём задачу трём сильным асессорам — как это происходит в стандартном процессе. Таких задач порядка 20%.<br><br><strong>Результаты и планы</strong><br><br>С помощью этого решения мы получили 105% качества и 60% экономии денег. <br><br>Мы уже используем его экспериментально в разметке обучающих и валидационных пулов для моделей ранжирования, но конечный мониторинг интегрального качества поиска пока остаётся на людях. <br><br>Планируем продолжать наращивать качество и запускаться на новых разметках. Также в долгосрочных планах — свести процесс к промптингу, когда ты не обучаешь модель, а описываешь задачу текстом. Так мы сможем более гибко менять инструкции разметок без переобучения модели. <br><br>Мы рассчитываем, что решение поможет нам перекинуть рутину на нейронки, а людям давать более интересные и сложные задачи.<br><br><a href="https://t.me/+9GKzRNPGwYk1OGUy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/61_480.webp" srcset="../assets/media/thumbs/61_480.webp 480w, ../assets/media/61.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="61" data-image-index="0" /></div></div>
      <div class="actions">
        <span>15 790 просмотров · 69 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/61" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/61.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="60" data-search="как создаются трейлеры в яндекс музыке трейлеры дают возможность быстро познакомиться с новой музыкой, чтобы решить, стоит ли погружаться в неё дальше. трейлеры в яндекс музыке есть у треков, альбомов, плейлистов и исполнителей. фрагмент для трейлера каждого трека выбирается на основе предсказаний нейросети. и в ваших итогах 2024 года тоже играл трейлер из любимых треков. о том, как создаются такие трейлеры, нашему каналу рассказал старший разработчик из команды музыки николай глазырин. чтобы сделать трейлер для трека, нужно совсем немного: определить его начало и конец 🙂 мы хотим, чтобы в трейлер попал самый яркий и узнаваемый законченный фрагмент трека. а ещё — чтобы фрагменты разных композиций могли плавно перетекать друг в друга. мы обучили модель, которая умеет предсказывать в треке одновременно границы тактов, позиции битов (по-русски их обычно называют тактовыми долями) и наилучшие моменты для начала трейлера. это небольшой encoder-only-трансформер на 0,5м параметров, который принимает на вход аудио с частотой дискретизации 22050 гц, а на выходе с шагом в 1/75 секунды предсказывает три числа: вероятность найти в этот момент бит, границу такта и начало подходящего для трейлера фрагмента. для обучения мы используем нашу нейромузыку, несколько открытых датасетов с границами тактов и тактовых долей, а также небольшой собственный датасет с размеченными вручную позициями начала трейлера. при обучении модель осваивает одновременное решение этих трёх задач. можно заметить, что вся разметка состоит из каких-то позиций внутри трека и привязана к его темпу. поэтому очень помогает во время обучения аугментировать данные путем ускорения-замедления аудио, чтобы модель не привязывалась к каким-то более популярным темпам. выходы модели мы сглаживаем полосовыми фильтрами, чтобы избавиться от лишнего шума. диапазон частот для каждого выхода подобран отдельно, чтобы соответствовать разумному. например, темп музыки обычно находится в диапазоне 50–200 ударов в минуту, поэтому и тактовые доли должны встречаться с примерно такой же частотой. в сглаженных выходах уже очень просто определить локальные максимумы, которые и отмечают интересующие нас моменты времени. для начала трейлера мы выбираем момент с максимальным значением на соответствующем выходе модели. если в треке есть вокал (что проверяется другой моделью), то выбираем такой момент, чтобы в трейлере тоже был вокал. конец трейлера ставим ровно через 8 тактов после начала. а чтобы трейлеры можно было красиво смиксовать, добавляем ещё небольшой фрагмент трека (1 такт) прямо перед началом. на этом фрагменте во время воспроизведения громкость плавно поднимается, а на последнем такте — наоборот, плавно затухает, что даёт плавный переход между трейлерами разных треков. наша модель маленькая, и поэтому достаточно быстрая, и при необходимости может работать даже на cpu. это важно, поскольку в нашем каталоге десятки миллионов треков и ещё десятки тысяч добавляются каждый день. ml underhood как создаются трейлеры в яндекс музыке трейлеры дают возможность быстро познакомиться с новой музыкой, чтобы решить, стоит ли погружаться в неё дальше. трейлеры в яндекс музыке есть у треков, альбомов, плейлистов и исполнителей. фрагмент для трейлера каждого трека выбирается на основе предсказаний нейросети. и в ваших итогах 2024 года тоже играл трейлер из любимых треков. о том, как создаются такие трейлеры, нашему каналу рассказал старший разработчик из команды музыки николай глазырин. чтобы сделать трейлер для трека, нужно совсем немного: определить его начало и конец 🙂 мы хотим, чтобы в трейлер попал самый яркий и узнаваемый законченный фрагмент трека. а ещё — чтобы фрагменты разных композиций могли плавно перетекать друг в друга. мы обучили модель, которая умеет предсказывать в треке одновременно границы тактов, позиции битов (по-русски их обычно называют тактовыми долями) и наилучшие моменты для начала трейлера. это небольшой encoder-only-трансформер на 0,5м параметров, который принимает на вход аудио с частотой дискретизации 22050 гц, а на выходе с шагом в 1/75 секунды предсказывает три числа: вероятность найти в этот момент бит, границу такта и начало подходящего для трейлера фрагмента. для обучения мы используем нашу нейромузыку, несколько открытых датасетов с границами тактов и тактовых долей, а также небольшой собственный датасет с размеченными вручную позициями начала трейлера. при обучении модель осваивает одновременное решение этих трёх задач. можно заметить, что вся разметка состоит из каких-то позиций внутри трека и привязана к его темпу. поэтому очень помогает во время обучения аугментировать данные путем ускорения-замедления аудио, чтобы модель не привязывалась к каким-то более популярным темпам. выходы модели мы сглаживаем полосовыми фильтрами, чтобы избавиться от лишнего шума. диапазон частот для каждого выхода подобран отдельно, чтобы соответствовать разумному. например, темп музыки обычно находится в диапазоне 50–200 ударов в минуту, поэтому и тактовые доли должны встречаться с примерно такой же частотой. в сглаженных выходах уже очень просто определить локальные максимумы, которые и отмечают интересующие нас моменты времени. для начала трейлера мы выбираем момент с максимальным значением на соответствующем выходе модели. если в треке есть вокал (что проверяется другой моделью), то выбираем такой момент, чтобы в трейлере тоже был вокал. конец трейлера ставим ровно через 8 тактов после начала. а чтобы трейлеры можно было красиво смиксовать, добавляем ещё небольшой фрагмент трека (1 такт) прямо перед началом. на этом фрагменте во время воспроизведения громкость плавно поднимается, а на последнем такте — наоборот, плавно затухает, что даёт плавный переход между трейлерами разных треков. наша модель маленькая, и поэтому достаточно быстрая, и при необходимости может работать даже на cpu. это важно, поскольку в нашем каталоге десятки миллионов треков и ещё десятки тысяч добавляются каждый день. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-02-04T12:12:22+00:00" href="./posts/60.html">2025-02-04 12:12 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как создаются трейлеры в Яндекс Музыке</strong><br><br>Трейлеры дают возможность быстро познакомиться с новой музыкой, чтобы решить, стоит ли погружаться в неё дальше. Трейлеры в Яндекс Музыке есть у треков, альбомов, плейлистов и исполнителей. Фрагмент для трейлера каждого трека выбирается на основе предсказаний нейросети. И в ваших итогах 2024 года тоже играл трейлер из любимых треков. О том, как создаются такие трейлеры, нашему каналу рассказал старший разработчик из команды Музыки Николай Глазырин.<br><br>Чтобы сделать трейлер для трека, нужно совсем немного: определить его начало и конец 🙂 Мы хотим, чтобы в трейлер попал самый яркий и узнаваемый законченный фрагмент трека. А ещё — чтобы фрагменты разных композиций могли плавно перетекать друг в друга.<br><br>Мы обучили модель, которая умеет предсказывать в треке одновременно границы тактов, позиции битов (по-русски их обычно называют тактовыми долями) и наилучшие моменты для начала трейлера. Это небольшой encoder-only-трансформер на 0,5М параметров, который принимает на вход аудио с частотой дискретизации 22050 Гц, а на выходе с шагом в 1/75 секунды предсказывает три числа: вероятность найти в этот момент бит, границу такта и начало подходящего для трейлера фрагмента. Для обучения мы используем нашу нейромузыку, несколько открытых датасетов с границами тактов и тактовых долей, а также небольшой собственный датасет с размеченными вручную позициями начала трейлера.<br><br>При обучении модель осваивает одновременное решение этих трёх задач. Можно заметить, что вся разметка состоит из каких-то позиций внутри трека и привязана к его темпу. Поэтому очень помогает во время обучения аугментировать данные путем ускорения-замедления аудио, чтобы модель не привязывалась к каким-то более популярным темпам.<br><br>Выходы модели мы сглаживаем полосовыми фильтрами, чтобы избавиться от лишнего шума. Диапазон частот для каждого выхода подобран отдельно, чтобы соответствовать разумному. Например, темп музыки обычно находится в диапазоне 50–200 ударов в минуту, поэтому и тактовые доли должны встречаться с примерно такой же частотой. В сглаженных выходах уже очень просто определить локальные максимумы, которые и отмечают интересующие нас моменты времени.<br><br>Для начала трейлера мы выбираем момент с максимальным значением на соответствующем выходе модели. Если в треке есть вокал (что проверяется другой моделью), то выбираем такой момент, чтобы в трейлере тоже был вокал. Конец трейлера ставим ровно через 8 тактов после начала. А чтобы трейлеры можно было красиво смиксовать, добавляем ещё небольшой фрагмент трека (1 такт) прямо перед началом. На этом фрагменте во время воспроизведения громкость плавно поднимается, а на последнем такте — наоборот, плавно затухает, что даёт плавный переход между трейлерами разных треков.<br><br>Наша модель маленькая, и поэтому достаточно быстрая, и при необходимости может работать даже на CPU. Это важно, поскольку в нашем каталоге десятки миллионов треков и ещё десятки тысяч добавляются каждый день.<br><br><a href="https://t.me/+9GKzRNPGwYk1OGUy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/60_480.webp" srcset="../assets/media/thumbs/60_480.webp 480w, ../assets/media/60.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="60" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 844 просмотров · 25 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/60" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/60.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="54" data-search="личные итоги года инженеров яндекса — александр шишеня 2025 год вступил в свои права, поэтому можно хорошенько осмыслить, что произошло в 2024-м. мы попросили ml-специалистов из яндекса рассказать, какими были для них минувшие 12 месяцев. первый на очереди — ведущий разработчик службы компьютерного зрения александр шишеня. он рассказал о своих профессиональных успехах и планах. александр упоминает статью physics of language models. а в канале cv time вы сможете почитать о лучших статьях по мнению александра. там, кстати, ещё много интересного — подписывайтесь! #yamlpeople ml underhood личные итоги года инженеров яндекса — александр шишеня 2025 год вступил в свои права, поэтому можно хорошенько осмыслить, что произошло в 2024-м. мы попросили ml-специалистов из яндекса рассказать, какими были для них минувшие 12 месяцев. первый на очереди — ведущий разработчик службы компьютерного зрения александр шишеня. он рассказал о своих профессиональных успехах и планах. александр упоминает статью physics of language models. а в канале cv time вы сможете почитать о лучших статьях по мнению александра. там, кстати, ещё много интересного — подписывайтесь! #yamlpeople ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-27T13:07:06+00:00" href="./posts/54.html">2025-01-27 13:07 UTC</a></div>
      </div>
      <div class="post-body"><strong>Личные итоги года инженеров Яндекса — Александр Шишеня</strong><br><br>2025 год вступил в свои права, поэтому можно хорошенько осмыслить, что произошло в 2024-м. Мы попросили ML-специалистов из Яндекса рассказать, какими были для них минувшие 12 месяцев. Первый на очереди — ведущий разработчик службы компьютерного зрения Александр Шишеня. Он рассказал о своих профессиональных успехах и планах. <br><br>Александр упоминает статью <a href="https://arxiv.org/abs/2305.13673" rel="nofollow noopener noreferrer">Physics of Language Models.</a><br><br>А в канале CV Time вы сможете почитать <a href="https://t.me/timeforcv/58" rel="nofollow noopener noreferrer">о лучших статьях по мнению Александра.</a> Там, кстати, ещё много интересного — подписывайтесь!<br><br>#YaMLpeople<br><br><a href="https://t.me/+fhiKIfhIc0VhNmQy" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/54_480.webp" srcset="../assets/media/thumbs/54_480.webp 480w, ../assets/media/54.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/55_480.webp" srcset="../assets/media/thumbs/55_480.webp 480w, ../assets/media/55.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/56_480.webp" srcset="../assets/media/thumbs/56_480.webp 480w, ../assets/media/56.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/57_480.webp" srcset="../assets/media/thumbs/57_480.webp 480w, ../assets/media/57.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="3" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/58_480.webp" srcset="../assets/media/thumbs/58_480.webp 480w, ../assets/media/58.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="4" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/59_480.webp" srcset="../assets/media/thumbs/59_480.webp 480w, ../assets/media/59.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="54" data-image-index="5" /></div></div>
      <div class="actions">
        <span>2 627 просмотров · 24 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/54" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/54.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="53" data-search="npu в яндекс станции в станции миди, дуо макс и тв станции устанавливают нейронные процессоры (npu). о том, зачем они нужны и с какими трудностями специалисты компании столкнулись при их использовании, нашему каналу рассказал ml-разработчик александр ростов. вычислительные ресурсы станции ограничены, а если поставить в такое небольшое устройство слишком мощный процессор, он будет сильно нагреваться. решение — npu, который берёт на себя расчёты, связанные с моделями. это чип, созданный специально для работы с нейросетями и ускорения ml-задач. так, до внедрения нейронного процессора разработчикам приходилось делать маленькие модели голосовой активации или споттерные модели — до миллиона параметров. благодаря npu в станции теперь есть модели и на десятки миллионов параметров. это позволило существенно прибавить в качестве, а также снизить число ложных срабатываний и случаев, когда колонка не реагирует на запрос пользователя. по сути, npu выступает в роли верификатора, проверяя, не допустила ли ложного срабатывания модель, работающая на cpu. кроме того, npu позволяет уменьшить cpu-модель и тем самым снять часть нагрузки с центрального процессора. из-за этого вся система стала работать быстрее. npu работает со свёрточными нейросетями. они учатся на gpu, затем конвертируются для применения с помощью библиотеки tensorflow lite для работы на npu. чип полностью используется споттерными моделями, однако на станции дуо макс он отвечает и за распознавание жестов. при тренировке npu-моделей возникла проблема: стандартные датасеты оказались непригодны для обучения хорошей npu-модели. это было связано как со спецификой работы верификатора, так и с увеличением размера модели. cpu-модель работает на всём потоке звука, и количество негативных примеров преобладает над активациями. в свою очередь, верификатор работает только на активациях cpu-модели, поэтому видит в основном позитивные примеры. для снижения числа случаев, когда колонка не реагирует на пользователя, понадобилось «ухудшить» cpu-модель, чтобы она активировалась чаще. так как обучающие данные изначально состояли из активаций более хорошей cpu-модели, это вызвало появление ложных активаций, которых верификационная модель не видела при обучении. чтобы решить эти проблемы, потребовалось собрать новый датасет, а также увеличить его в несколько раз. ml underhood npu в яндекс станции в станции миди, дуо макс и тв станции устанавливают нейронные процессоры (npu). о том, зачем они нужны и с какими трудностями специалисты компании столкнулись при их использовании, нашему каналу рассказал ml-разработчик александр ростов. вычислительные ресурсы станции ограничены, а если поставить в такое небольшое устройство слишком мощный процессор, он будет сильно нагреваться. решение — npu, который берёт на себя расчёты, связанные с моделями. это чип, созданный специально для работы с нейросетями и ускорения ml-задач. так, до внедрения нейронного процессора разработчикам приходилось делать маленькие модели голосовой активации или споттерные модели — до миллиона параметров. благодаря npu в станции теперь есть модели и на десятки миллионов параметров. это позволило существенно прибавить в качестве, а также снизить число ложных срабатываний и случаев, когда колонка не реагирует на запрос пользователя. по сути, npu выступает в роли верификатора, проверяя, не допустила ли ложного срабатывания модель, работающая на cpu. кроме того, npu позволяет уменьшить cpu-модель и тем самым снять часть нагрузки с центрального процессора. из-за этого вся система стала работать быстрее. npu работает со свёрточными нейросетями. они учатся на gpu, затем конвертируются для применения с помощью библиотеки tensorflow lite для работы на npu. чип полностью используется споттерными моделями, однако на станции дуо макс он отвечает и за распознавание жестов. при тренировке npu-моделей возникла проблема: стандартные датасеты оказались непригодны для обучения хорошей npu-модели. это было связано как со спецификой работы верификатора, так и с увеличением размера модели. cpu-модель работает на всём потоке звука, и количество негативных примеров преобладает над активациями. в свою очередь, верификатор работает только на активациях cpu-модели, поэтому видит в основном позитивные примеры. для снижения числа случаев, когда колонка не реагирует на пользователя, понадобилось «ухудшить» cpu-модель, чтобы она активировалась чаще. так как обучающие данные изначально состояли из активаций более хорошей cpu-модели, это вызвало появление ложных активаций, которых верификационная модель не видела при обучении. чтобы решить эти проблемы, потребовалось собрать новый датасет, а также увеличить его в несколько раз. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-17T12:51:43+00:00" href="./posts/53.html">2025-01-17 12:51 UTC</a></div>
      </div>
      <div class="post-body"><strong>NPU в Яндекс Станции</strong><br><br>В Станции Миди, Дуо Макс и ТВ Станции устанавливают нейронные процессоры (NPU). О том, зачем они нужны и с какими трудностями специалисты компании столкнулись при их использовании, нашему каналу рассказал ML-разработчик Александр Ростов.<br><br>Вычислительные ресурсы Станции ограничены, а если поставить в такое небольшое устройство слишком мощный процессор, он будет сильно нагреваться. Решение — NPU, который берёт на себя расчёты, связанные с моделями. Это чип, созданный специально для работы с нейросетями и ускорения ML-задач. <br><br>Так, до внедрения нейронного процессора разработчикам приходилось делать маленькие модели голосовой активации или споттерные модели — до миллиона параметров. Благодаря NPU в Станции теперь есть модели и на десятки миллионов параметров. Это позволило существенно прибавить в качестве, а также снизить число ложных срабатываний и случаев, когда колонка не реагирует на запрос пользователя.<br><br>По сути, NPU выступает в роли верификатора, проверяя, не допустила ли ложного срабатывания модель, работающая на CPU. Кроме того, NPU позволяет уменьшить CPU-модель и тем самым снять часть нагрузки с центрального процессора. Из-за этого вся система стала работать быстрее. <br><br>NPU работает со свёрточными нейросетями. Они учатся на GPU, затем конвертируются для применения с помощью библиотеки TensorFlow Lite для работы на NPU. Чип полностью используется споттерными моделями, однако на Станции Дуо Макс он отвечает и за распознавание жестов. <br><br>При тренировке NPU-моделей возникла проблема: стандартные датасеты оказались непригодны для обучения хорошей NPU-модели. Это было связано как со спецификой работы верификатора, так и с увеличением размера модели. CPU-модель работает на всём потоке звука, и количество негативных примеров преобладает над активациями. В свою очередь, верификатор работает только на активациях CPU-модели, поэтому видит в основном позитивные примеры. <br><br>Для снижения числа случаев, когда колонка не реагирует на пользователя, понадобилось «ухудшить» CPU-модель, чтобы она активировалась чаще. Так как обучающие данные изначально состояли из активаций более хорошей CPU-модели, это вызвало появление ложных активаций, которых верификационная модель не видела при обучении.<br><br>Чтобы решить эти проблемы, потребовалось собрать новый датасет, а также увеличить его в несколько раз.<br><br><a href="https://t.me/+3JrRFFP3PqxiNWVi" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>3 104 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/53" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/53.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="52" data-search="яркие истории о разработках яндекса в 2024 году за минувший год ml’щики яндекса сделали много интересного, и о многом мы писали в нашем канале. перед вами небольшая ретроспектива — предлагаем вспомнить разработки и улучшения в продуктах за 2024 год. библиотека yafsdp блиц-интервью с руководителем службы претрейна yandexgpt михаилом хрущевым. узнали много интересного о библиотеке yafsdp, которая ускоряет обучение больших языковых моделей. как устроен yandexart, yandexart 1.3 и генерация видео в шедевруме сразу три поста о прекрасном — об искусстве. первый — о мультимодальной модели для генерации изображений yandexart в целом. второй — о новой версии нейросети, основанной на латентной диффузии, что позволяет здорово экономить вычислительные ресурсы. третий пост вышел в сентябре — аккурат под костры рябин. в публикации речь идет об улучшенной генерации видео в шедевруме. рассказали, как она устроена и как обучали модель. как работает нейро весной яндекс запустил сервис для поиска ответов на вопросы, заданные на естественном языке — нейро. под капотом у него, разумеется, llm. а как модель себя проявляет — читайте в этом кулинарном посте с сакраментальным вопросом: «а какие же щи без капусты?» алиса на казахском языке, локальный asr в яндекс станции и эхоподавление три поста о голосовом ассистенте яндекса. первый рассказывает, как алису учили говорить на казахском языке. архитектура здесь схожа с той, что и у русскоязычного ассистента, однако есть свои тонкости. подробно рассказали о них в самом лингвистически заряженном посте года. второй пост — о создании локальной системы автоматического распознавания речи в станции. технические возможности колонки накладывают на разработчиков ограничения, с которыми приходится мириться. об этом (и многом другом) и рассказали. третий пост об эхоподавлении (aec). рассказываем, как оно устроено, а заодно о новом бета-датасете, на котором инженеры перебирали гиперпараметры, чтобы добиться улучшения качества. omnicast в погоде omnicast — новая технология, принятая на вооружение яндекс погодой. она позволяет точно предсказывать осадки и циклоны, используя данные как с профессиональных, так и с любительских метеостанций. улучшенный фотоперевод осенью мы обновили фотоперевод — теперь понимать зарубежные мемы, распечатанные на бумаге (бывает и такое), совсем просто. новая модель лучше выделяет семантические блоки, а переведённый текст стал больше похож на оригинальный благодаря алгоритму затирания. о том, что и как ещё изменилось — рассказываем в посте. ml underhood яркие истории о разработках яндекса в 2024 году за минувший год ml’щики яндекса сделали много интересного, и о многом мы писали в нашем канале. перед вами небольшая ретроспектива — предлагаем вспомнить разработки и улучшения в продуктах за 2024 год. библиотека yafsdp блиц-интервью с руководителем службы претрейна yandexgpt михаилом хрущевым. узнали много интересного о библиотеке yafsdp, которая ускоряет обучение больших языковых моделей. как устроен yandexart , yandexart 1.3 и генерация видео в шедевруме сразу три поста о прекрасном — об искусстве. первый — о мультимодальной модели для генерации изображений yandexart в целом. второй — о новой версии нейросети, основанной на латентной диффузии, что позволяет здорово экономить вычислительные ресурсы. третий пост вышел в сентябре — аккурат под костры рябин. в публикации речь идет об улучшенной генерации видео в шедевруме. рассказали, как она устроена и как обучали модель. как работает нейро весной яндекс запустил сервис для поиска ответов на вопросы, заданные на естественном языке — нейро. под капотом у него, разумеется, llm. а как модель себя проявляет — читайте в этом кулинарном посте с сакраментальным вопросом: «а какие же щи без капусты?» алиса на казахском языке , локальный asr в яндекс станции и эхоподавление три поста о голосовом ассистенте яндекса. первый рассказывает, как алису учили говорить на казахском языке. архитектура здесь схожа с той, что и у русскоязычного ассистента, однако есть свои тонкости. подробно рассказали о них в самом лингвистически заряженном посте года. второй пост — о создании локальной системы автоматического распознавания речи в станции. технические возможности колонки накладывают на разработчиков ограничения, с которыми приходится мириться. об этом (и многом другом) и рассказали. третий пост об эхоподавлении (aec). рассказываем, как оно устроено, а заодно о новом бета-датасете, на котором инженеры перебирали гиперпараметры, чтобы добиться улучшения качества. omnicast в погоде omnicast — новая технология, принятая на вооружение яндекс погодой. она позволяет точно предсказывать осадки и циклоны, используя данные как с профессиональных, так и с любительских метеостанций. улучшенный фотоперевод осенью мы обновили фотоперевод — теперь понимать зарубежные мемы, распечатанные на бумаге (бывает и такое), совсем просто. новая модель лучше выделяет семантические блоки, а переведённый текст стал больше похож на оригинальный благодаря алгоритму затирания. о том, что и как ещё изменилось — рассказываем в посте. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2025-01-10T08:58:24+00:00" href="./posts/52.html">2025-01-10 08:58 UTC</a></div>
      </div>
      <div class="post-body"><strong>Яркие истории о разработках Яндекса в 2024 году</strong><br><br>За минувший год ML’щики Яндекса сделали много интересного, и о многом мы писали в нашем канале. Перед вами небольшая ретроспектива — предлагаем вспомнить разработки и улучшения в продуктах за 2024 год. <br><br><a href="https://t.me/MLunderhood/13" rel="nofollow noopener noreferrer"><strong>Библиотека YaFSDP</strong></a><br><br>Блиц-интервью с руководителем службы претрейна YandexGPT Михаилом Хрущевым. Узнали много интересного о библиотеке YaFSDP, которая ускоряет обучение больших языковых моделей.<br><br><a href="https://t.me/MLunderhood/5?single" rel="nofollow noopener noreferrer"><strong>Как устроен YandexART</strong></a>, <a href="https://t.me/MLunderhood/8" rel="nofollow noopener noreferrer"><strong>YandexART 1.3</strong></a> и <a href="https://t.me/MLunderhood/20" rel="nofollow noopener noreferrer"><strong>генерация видео в Шедевруме</strong></a><br><br>Сразу три поста о прекрасном — об искусстве. Первый — о мультимодальной модели для генерации изображений YandexART в целом. Второй — о новой версии нейросети, основанной на латентной диффузии, что позволяет здорово экономить вычислительные ресурсы. Третий пост вышел в сентябре — аккурат под костры рябин. В публикации речь идет об улучшенной генерации видео в Шедевруме. Рассказали, как она устроена и как обучали модель.<br><br><a href="https://t.me/MLunderhood/7" rel="nofollow noopener noreferrer"><strong>Как работает Нейро</strong></a><br><br>Весной Яндекс запустил сервис для поиска ответов на вопросы, заданные на естественном языке — Нейро. Под капотом у него, разумеется, LLM. А как модель себя проявляет — читайте в этом кулинарном посте с сакраментальным вопросом: «А какие же щи без капусты?» <br><br><a href="https://t.me/MLunderhood/14" rel="nofollow noopener noreferrer"><strong>Алиса на казахском языке</strong></a>, <a href="https://t.me/MLunderhood/18" rel="nofollow noopener noreferrer"><strong>локальный ASR в Яндекс Станции</strong></a> и <a href="https://t.me/MLunderhood/51" rel="nofollow noopener noreferrer"><strong>эхоподавление</strong></a><br><br>Три поста о голосовом ассистенте Яндекса. Первый рассказывает, как Алису учили говорить на казахском языке. Архитектура здесь схожа с той, что и у русскоязычного ассистента, однако есть свои тонкости. Подробно рассказали о них в самом лингвистически заряженном посте года.<br><br>Второй пост — о создании локальной системы автоматического распознавания речи в Станции. Технические возможности колонки накладывают на разработчиков ограничения, с которыми приходится мириться. Об этом (и многом другом) и рассказали. <br><br>Третий пост об эхоподавлении (AEC). Рассказываем, как оно устроено, а заодно о новом бета-датасете, на котором инженеры перебирали гиперпараметры, чтобы добиться улучшения качества. <br><br><a href="https://t.me/MLunderhood/19" rel="nofollow noopener noreferrer"><strong>OmniCast в погоде</strong></a><br><br>OmniCast — новая технология, принятая на вооружение Яндекс Погодой. Она позволяет точно предсказывать осадки и циклоны, используя данные как с профессиональных, так и с любительских метеостанций.<br><br><a href="https://t.me/MLunderhood/23" rel="nofollow noopener noreferrer"><strong>Улучшенный фотоперевод</strong></a><br><br>Осенью мы обновили фотоперевод — теперь понимать зарубежные мемы, распечатанные на бумаге (бывает и такое), совсем просто. Новая модель лучше выделяет семантические блоки, а переведённый текст стал больше похож на оригинальный благодаря алгоритму затирания. О том, что и как ещё изменилось — рассказываем в посте.  <br><br><a href="https://t.me/+ym9we9EVZrlkMzIy" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 932 просмотров · 22 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/52" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/52.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="51" data-search="🎄эхоподавление у алисы — как оно устроено и как улучшилось за год представим ситуацию. яндекс станция играет музыку, пока вы готовите новогодний стол. очередное блюдо отправляется в духовку, и вам нужно узнать, на какое время ставить таймер. вы спокойно спрашиваете у алисы, а она отвечает даже несмотря на шум, который сама производит. но как это работает? ведь колонка небольшая, динамики находятся рядом с микрофонами. к просьбе от человека примешивается музыка, а сам звук постоянно отражается как от стен помещения, так и внутри самой колонки. как же станция поняла ваш запрос? всё дело в эхоподавлении (acoustic echo cancellation, aec). небольшая модель внутри станции разделяет входящие аудиосигналы на фрагменты, а затем отфильтровывает их с помощью фильтра калмана. если во фрагменте есть активационная фраза, то алиса войдет в режим ожидания команды. здесь помогает и шумоподавление. оно нужно, чтобы «отрезать» от активационной фразы посторонние шумы — например, голоса родни или стенания ипполита из «иронии судьбы» из телевизора. в 2024 году aec улучшилась. о том, что добавилось, и сопутствующих трудностях нашему каналу рассказал разработчик из команды улучшения качества звука антон порфирьев. во-первых, у алисы появились быстрые команды. благодаря им не нужно обращаться к ассистенту по имени, чтобы переключить или выключить песню. внедрение этой возможности потребовало перебора параметров aeс, ведь нужно было сделать так, чтобы алиса реагировала не только на своё имя, но и на обособленные команды вроде «тише» или «следующая». работа над быстрыми командами помогла сделать систему эхоподавления в целом более эффективной. второе улучшение коснулось не только эхо, но и шумоподавления — aec применяется как отдельно, так и после него. раньше для обеих операций использовались одни и те же гиперпараметры, теперь — разные. конфигурации подаются на отдельные каналы, и такое изменение дало прирост в качестве. ещё одно интересное нововведение — бета-датасет. раньше у яндекса был доступ только к «лабораторным» данным, записанным в специальной студии. они не отражали реальную ситуацию полностью, ведь эхо и реверберация звука в лабораториях всегда примерно одни и те же. новый датасет даёт модели возможность учиться на реальных случаях во всём их многообразии. датасет начали разрабатывать ещё в прошлом году, но внедрили в 2024-м. этот набор собран из данных, полученных от бета-тестеров, у которых станция логирует всё, что происходит за несколько секунд до фразы активации. эти секунды отрезаются от записи и складываются с чистыми активациями, записанными в лабораторных условиях. так получаются синтетические данные, которых легко можно получить очень много. при этом здесь активационная фраза не так важна, ведь первостепенное значение играет именно эхо. поэтому для разных записей бета-тестеров можно использовать одну и ту же запись активации из лаборатории. разница в объёмах датасетов значительная: если лабораторный состоит из примерно 5 тысяч записей, то бета-датасет — из порядка 50 тысяч. на получившемся наборе и перебирали гиперпараметры модели, что дало значительный прирост качества. вот так работает эхоподавление в яндекс станции. напоследок, поздравляем вас с наступающим новым годом! а команда ml underhood уходит на каникулы, чтобы в январе вернуться с новыми интересными историями из мира машинного обучения. ml underhood 🎄 эхоподавление у алисы — как оно устроено и как улучшилось за год представим ситуацию. яндекс станция играет музыку, пока вы готовите новогодний стол. очередное блюдо отправляется в духовку, и вам нужно узнать, на какое время ставить таймер. вы спокойно спрашиваете у алисы, а она отвечает даже несмотря на шум, который сама производит. но как это работает? ведь колонка небольшая, динамики находятся рядом с микрофонами. к просьбе от человека примешивается музыка, а сам звук постоянно отражается как от стен помещения, так и внутри самой колонки. как же станция поняла ваш запрос? всё дело в эхоподавлении (acoustic echo cancellation, aec). небольшая модель внутри станции разделяет входящие аудиосигналы на фрагменты, а затем отфильтровывает их с помощью фильтра калмана . если во фрагменте есть активационная фраза, то алиса войдет в режим ожидания команды. здесь помогает и шумоподавление. оно нужно, чтобы «отрезать» от активационной фразы посторонние шумы — например, голоса родни или стенания ипполита из «иронии судьбы» из телевизора. в 2024 году aec улучшилась. о том, что добавилось, и сопутствующих трудностях нашему каналу рассказал разработчик из команды улучшения качества звука антон порфирьев. во-первых, у алисы появились быстрые команды. благодаря им не нужно обращаться к ассистенту по имени, чтобы переключить или выключить песню. внедрение этой возможности потребовало перебора параметров aeс, ведь нужно было сделать так, чтобы алиса реагировала не только на своё имя, но и на обособленные команды вроде «тише» или «следующая». работа над быстрыми командами помогла сделать систему эхоподавления в целом более эффективной. второе улучшение коснулось не только эхо, но и шумоподавления — aec применяется как отдельно, так и после него. раньше для обеих операций использовались одни и те же гиперпараметры, теперь — разные. конфигурации подаются на отдельные каналы, и такое изменение дало прирост в качестве. ещё одно интересное нововведение — бета-датасет. раньше у яндекса был доступ только к «лабораторным» данным, записанным в специальной студии. они не отражали реальную ситуацию полностью, ведь эхо и реверберация звука в лабораториях всегда примерно одни и те же. новый датасет даёт модели возможность учиться на реальных случаях во всём их многообразии. датасет начали разрабатывать ещё в прошлом году, но внедрили в 2024-м. этот набор собран из данных, полученных от бета-тестеров, у которых станция логирует всё, что происходит за несколько секунд до фразы активации. эти секунды отрезаются от записи и складываются с чистыми активациями, записанными в лабораторных условиях. так получаются синтетические данные, которых легко можно получить очень много. при этом здесь активационная фраза не так важна, ведь первостепенное значение играет именно эхо. поэтому для разных записей бета-тестеров можно использовать одну и ту же запись активации из лаборатории. разница в объёмах датасетов значительная: если лабораторный состоит из примерно 5 тысяч записей, то бета-датасет — из порядка 50 тысяч. на получившемся наборе и перебирали гиперпараметры модели, что дало значительный прирост качества. вот так работает эхоподавление в яндекс станции. напоследок, поздравляем вас с наступающим новым годом! а команда ml underhood уходит на каникулы, чтобы в январе вернуться с новыми интересными историями из мира машинного обучения. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-27T11:42:03+00:00" href="./posts/51.html">2024-12-27 11:42 UTC</a></div>
      </div>
      <div class="post-body">🎄<strong>Эхоподавление у Алисы — как оно устроено и как улучшилось за год</strong><br><br>Представим ситуацию. Яндекс Станция играет музыку, пока вы готовите новогодний стол. Очередное блюдо отправляется в духовку, и вам нужно узнать, на какое время ставить таймер. Вы спокойно спрашиваете у Алисы, а она отвечает даже несмотря на шум, который сама производит. Но как это работает? Ведь колонка небольшая, динамики находятся рядом с микрофонами. К просьбе от человека примешивается музыка, а сам звук постоянно отражается как от стен помещения, так и внутри самой колонки. Как же Станция поняла ваш запрос? <br><br>Всё дело в эхоподавлении (Acoustic Echo Cancellation, AEC). Небольшая модель внутри станции разделяет входящие аудиосигналы на фрагменты, а затем отфильтровывает их с помощью <a href="https://en.wikipedia.org/wiki/Kalman_filter" rel="nofollow noopener noreferrer">фильтра Калмана</a>. Если во фрагменте есть активационная фраза, то Алиса войдет в режим ожидания команды. Здесь помогает и шумоподавление. Оно нужно, чтобы «отрезать» от активационной фразы посторонние шумы — например, голоса родни или стенания Ипполита из «Иронии судьбы» из телевизора.<br><br>В 2024 году AEC улучшилась. О том, что добавилось, и сопутствующих трудностях нашему каналу рассказал разработчик из команды улучшения качества звука Антон Порфирьев.<br><br>Во-первых, у Алисы появились быстрые команды. Благодаря им не нужно обращаться к ассистенту по имени, чтобы переключить или выключить песню. Внедрение этой возможности потребовало перебора параметров AEС, ведь нужно было сделать так, чтобы Алиса реагировала не только на своё имя, но и на обособленные команды вроде «тише» или «следующая». Работа над быстрыми командами помогла сделать систему эхоподавления в целом более эффективной. <br><br>Второе улучшение коснулось не только эхо, но и шумоподавления — AEC применяется как отдельно, так и после него. Раньше для обеих операций использовались одни и те же гиперпараметры, теперь — разные. Конфигурации подаются на отдельные каналы, и такое изменение дало прирост в качестве. <br><br>Ещё одно интересное нововведение — бета-датасет. Раньше у Яндекса был доступ только к «лабораторным» данным, записанным в специальной студии. Они не отражали реальную ситуацию полностью, ведь эхо и реверберация звука в лабораториях всегда примерно одни и те же. Новый датасет даёт модели возможность учиться на реальных случаях во всём их многообразии. <br><br>Датасет начали разрабатывать ещё в прошлом году, но внедрили в 2024-м. Этот набор собран из данных, полученных от бета-тестеров, у которых Станция логирует всё, что происходит за несколько секунд до фразы активации. Эти секунды отрезаются от записи и складываются с чистыми активациями, записанными в лабораторных условиях. Так получаются синтетические данные, которых легко можно получить очень много. <br><br>При этом здесь активационная фраза не так важна, ведь первостепенное значение играет именно эхо. Поэтому для разных записей бета-тестеров можно использовать одну и ту же запись активации из лаборатории. Разница в объёмах датасетов значительная: если лабораторный состоит из примерно 5 тысяч записей, то бета-датасет — из порядка 50 тысяч. На получившемся наборе и перебирали гиперпараметры модели, что дало значительный прирост качества.  <br><br>Вот так работает эхоподавление в Яндекс Станции. Напоследок, поздравляем вас с наступающим Новым годом! А команда ML Underhood уходит на каникулы, чтобы в январе вернуться с новыми интересными историями из мира машинного обучения. <br><br><a href="https://t.me/+NWItQ4yxp084NDBi" rel="nofollow noopener noreferrer">ML Underhood</a></div>
      <div class="actions">
        <span>2 723 просмотров · 47 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/51" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/51.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="50" data-search="как модель перевода от яндекса вошла в топ-5 на конкурсе в рамках конференции wmt в последний год у яндекса появилась хорошая llm для перевода. а насколько хорошая мы решили проверить на соревнованиях, которые прошли в рамках девятой конференции по машинному переводу (wmt). по итогам модель яндекса заняла пятое место из 14. речь идёт о модели, которая сейчас доступна всем в видеопереводе. почему именно она? всё дело в условиях соревнований. для перевода там давали документы с html-структурой, поэтому и модели-участники должны были поддерживать её. у части документов была озвучка, которой позволяли воспользоваться — то есть сперва распознать речь, а уже затем перевести её. организаторы стремились к тому, чтобы на конкурсные тексты невозможно было заранее обучить модель. это значит, что использовали весьма редкие документы. специальную модель для wmt в яндексе не делали, а заработанное пятое место связывают сразу с несколькими факторами. во-первых, это переход от перевода отдельных предложений к переводу целых параграфов, что позволило учитывать контекст, а значит — улучшить итоговый результат. важно и то, что модель яндекса училась на перефразах. специалисты брали уже переведённое автоматикой предложение и просили изменить его так, чтобы перевод был больше похож на человеческий. на этих перефразах, в частности, и обучали модель. кроме того, модель яндекса умеет сохранять структуру оригинальных размеченных документов. вероятно, что модель яндекса — самая маленькая из тех, кому удалось попасть в топ-5. остальные конкурсанты представляют собой большие мультилингвальные llm. наша же ориентирована на пару языков русский и английский. победителя определяли с помощью оценки человеком. однако для отбора конкурсантов использовали llm-as-a-judge-метрику — metricx от google. ml underhood как модель перевода от яндекса вошла в топ-5 на конкурсе в рамках конференции wmt в последний год у яндекса появилась хорошая llm для перевода. а насколько хорошая мы решили проверить на соревнованиях , которые прошли в рамках девятой конференции по машинному переводу (wmt) . по итогам модель яндекса заняла пятое место из 14. речь идёт о модели, которая сейчас доступна всем в видеопереводе. почему именно она? всё дело в условиях соревнований. для перевода там давали документы с html-структурой, поэтому и модели-участники должны были поддерживать её. у части документов была озвучка, которой позволяли воспользоваться — то есть сперва распознать речь, а уже затем перевести её. организаторы стремились к тому, чтобы на конкурсные тексты невозможно было заранее обучить модель. это значит, что использовали весьма редкие документы. специальную модель для wmt в яндексе не делали, а заработанное пятое место связывают сразу с несколькими факторами. во-первых, это переход от перевода отдельных предложений к переводу целых параграфов, что позволило учитывать контекст, а значит — улучшить итоговый результат. важно и то, что модель яндекса училась на перефразах. специалисты брали уже переведённое автоматикой предложение и просили изменить его так, чтобы перевод был больше похож на человеческий. на этих перефразах, в частности, и обучали модель. кроме того, модель яндекса умеет сохранять структуру оригинальных размеченных документов. вероятно, что модель яндекса — самая маленькая из тех, кому удалось попасть в топ-5. остальные конкурсанты представляют собой большие мультилингвальные llm. наша же ориентирована на пару языков русский и английский. победителя определяли с помощью оценки человеком. однако для отбора конкурсантов использовали llm-as-a-judge-метрику — metricx от google. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-20T09:00:45+00:00" href="./posts/50.html">2024-12-20 09:00 UTC</a></div>
      </div>
      <div class="post-body"><strong>Как модель перевода от Яндекса вошла в топ-5 на конкурсе в рамках конференции WMT</strong><br><br>В последний год у Яндекса появилась хорошая LLM для перевода. А насколько хорошая мы решили проверить <a href="https://www2.statmt.org/wmt24/translation-task.html" rel="nofollow noopener noreferrer">на соревнованиях</a>, которые прошли <a href="https://www2.statmt.org/wmt24/" rel="nofollow noopener noreferrer">в рамках девятой Конференции по машинному переводу (WMT)</a>. По итогам модель Яндекса заняла пятое место из 14. <br><br>Речь идёт о модели, которая сейчас доступна всем в видеопереводе. Почему именно она? Всё дело в условиях соревнований. Для перевода там давали документы с HTML-структурой, поэтому и модели-участники должны были поддерживать её. У части документов была озвучка, которой позволяли воспользоваться — то есть сперва распознать речь, а уже затем перевести её. <br><br>Организаторы стремились к тому, чтобы на конкурсные тексты невозможно было заранее обучить модель. Это значит, что использовали весьма редкие документы. <br><br>Специальную модель для WMT в Яндексе не делали, а заработанное пятое место связывают сразу с несколькими факторами. Во-первых, это переход от перевода отдельных предложений к переводу целых параграфов, что позволило учитывать контекст, а значит — улучшить итоговый результат. <br><br>Важно и то, что модель Яндекса училась на перефразах. Специалисты брали уже переведённое автоматикой предложение и просили изменить его так, чтобы перевод был больше похож на человеческий. На этих перефразах, в частности, и обучали модель. Кроме того, модель Яндекса умеет сохранять структуру оригинальных размеченных документов. <br><br>Вероятно, что модель Яндекса — самая маленькая из тех, кому удалось попасть в топ-5. Остальные конкурсанты представляют собой большие мультилингвальные LLM. Наша же ориентирована на пару языков русский и английский. <br><br>Победителя определяли с помощью оценки человеком. Однако для отбора конкурсантов использовали LLM-as-a-Judge-метрику — MetricX от Google.<br><br><a href="https://t.me/+AIc3M1hmvXUzODli" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/50_480.webp" srcset="../assets/media/thumbs/50_480.webp 480w, ../assets/media/50.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="50" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 584 просмотров · 27 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/50" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/50.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="49" data-search="суцкевер на neurips: хайлайты доклад ильи суцкевера на neurips уже разобрали на цитаты. больше всего обсуждают идеи о росте агентности, конце претрейна и закона масштабирования. нам удалось послушать доклад на конференции — делимся моментами, которые показались важными. нерезиновые данные претрейна на данных из интернета скоро не будет — всё, что можно, уже собрали, а синтетика просто копирует старое. будущее за уникальными проприетарными датасетами, а успех моделей будет зависеть не от их размера, а от качества данных, на которых они обучаются. конец scaling law больше не работает scaling law, который до сих пор точно предсказывал рост качества моделей при увеличении данных и вычислений. но это не значит, что прогресс остановится: как с законом мура, новые этапы развития технологий откроют другие способы оптимизации. будущее за агентами (умными) сейчас модели чаще всего повторяют то, что уже видели на этапе обучения. агенты же смогут самостоятельно анализировать ситуацию и придумывать новые стратегии. точного рецепта для создания таких агентов пока нет, но илья уверен, что это вопрос времени. интуитивность и непредсказуемость будущее будет связано с самосознанием систем. пока что моделям не хватает умения находить нестандартные решения, они опираются на заранее заложенные шаблоны. когда ии научится рассуждать по-настоящему, системы станут интуитивнее и одновременно непредсказуемее. эволюция как подсказка для ии идея в том, что ии может развиваться по принципам природы: адаптироваться, «мутировать» и улучшаться. илья провёл аналогию между развитием интеллекта у животных и масштабированием моделей. генетические алгоритмы уже используют для оптимизации архитектур, в будущем модели смогут разрабатывать такие алгоритмы сами. в конце доклада илья сказал: “all kinds of stuff is possible” — уже распечатали и повесили этот слайд в офисе. ml underhood #yaneurips суцкевер на neurips: хайлайты доклад ильи суцкевера на neurips уже разобрали на цитаты. больше всего обсуждают идеи о росте агентности, конце претрейна и закона масштабирования. нам удалось послушать доклад на конференции — делимся моментами, которые показались важными. нерезиновые данные претрейна на данных из интернета скоро не будет — всё, что можно, уже собрали, а синтетика просто копирует старое. будущее за уникальными проприетарными датасетами, а успех моделей будет зависеть не от их размера, а от качества данных, на которых они обучаются. конец scaling law больше не работает scaling law , который до сих пор точно предсказывал рост качества моделей при увеличении данных и вычислений. но это не значит, что прогресс остановится: как с законом мура , новые этапы развития технологий откроют другие способы оптимизации. будущее за агентами (умными) сейчас модели чаще всего повторяют то, что уже видели на этапе обучения. агенты же смогут самостоятельно анализировать ситуацию и придумывать новые стратегии. точного рецепта для создания таких агентов пока нет, но илья уверен, что это вопрос времени. интуитивность и непредсказуемость будущее будет связано с самосознанием систем. пока что моделям не хватает умения находить нестандартные решения, они опираются на заранее заложенные шаблоны. когда ии научится рассуждать по-настоящему, системы станут интуитивнее и одновременно непредсказуемее. эволюция как подсказка для ии идея в том, что ии может развиваться по принципам природы: адаптироваться, «мутировать» и улучшаться. илья провёл аналогию между развитием интеллекта у животных и масштабированием моделей. генетические алгоритмы уже используют для оптимизации архитектур, в будущем модели смогут разрабатывать такие алгоритмы сами. в конце доклада илья сказал: “all kinds of stuff is possible” — уже распечатали и повесили этот слайд в офисе. ml underhood #yaneurips">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-18T12:59:08+00:00" href="./posts/49.html">2024-12-18 12:59 UTC</a></div>
      </div>
      <div class="post-body"><strong>Суцкевер на NeurIPS: хайлайты </strong><br><br><a href="https://www.youtube.com/watch?v=YD-9NG1Ke5Y)" rel="nofollow noopener noreferrer">Доклад Ильи Суцкевера</a> на NeurIPS уже разобрали на цитаты. Больше всего обсуждают идеи о росте агентности, конце претрейна и закона масштабирования. Нам удалось послушать доклад на конференции — делимся моментами, которые показались важными.<br><br><strong>Нерезиновые данные</strong><br>Претрейна на данных из интернета скоро не будет — всё, что можно, уже собрали, а синтетика просто копирует старое. Будущее за уникальными проприетарными датасетами, а успех моделей будет зависеть не от их размера, а от качества данных, на которых они обучаются. <br><br><strong>Конец scaling</strong> <strong>law</strong><br>Больше не работает <a href="https://en.m.wikipedia.org/wiki/Neural_scaling_law" rel="nofollow noopener noreferrer">scaling law</a>, который до сих пор точно предсказывал рост качества моделей при увеличении данных и вычислений. Но это не значит, что прогресс остановится: как с <a href="https://ru.wikipedia.org/wiki/%D0%97%D0%B0%D0%BA%D0%BE%D0%BD_%D0%9C%D1%83%D1%80%D0%B0#:~:text=%D0%97%D0%B0%D0%BA%D0%BE%CC%81%D0%BD%20%D0%9C%D1%83%CC%81%D1%80%D0%B0%20(%D0%B0%D0%BD%D0%B3%D0%BB.,%D1%81%D1%85%D0%B5%D0%BC%D1%8B%2C%20%D1%83%D0%B4%D0%B2%D0%B0%D0%B8%D0%B2%D0%B0%D0%B5%D1%82%D1%81%D1%8F%20%D0%BA%D0%B0%D0%B6%D0%B4%D1%8B%D0%B5%2024%20%D0%BC%D0%B5%D1%81%D1%8F%D1%86%D0%B0." rel="nofollow noopener noreferrer">законом Мура</a>, новые этапы развития технологий откроют другие способы оптимизации.<br><br><strong>Будущее за агентами (умными)</strong><br>Сейчас модели чаще всего повторяют то, что уже видели на этапе обучения. Агенты же смогут самостоятельно анализировать ситуацию и придумывать новые стратегии. Точного рецепта для создания таких агентов пока нет, но Илья уверен, что это вопрос времени.<br><br><strong>Интуитивность и непредсказуемость</strong><br>Будущее будет связано с самосознанием систем. Пока что моделям не хватает умения находить нестандартные решения, они опираются на заранее заложенные шаблоны. Когда ИИ научится рассуждать по-настоящему, системы станут интуитивнее и одновременно непредсказуемее. <br><br><strong>Эволюция как подсказка для ИИ</strong><br>Идея в том, что ИИ может развиваться по принципам природы: адаптироваться, «мутировать» и улучшаться. Илья провёл аналогию между развитием интеллекта у животных и масштабированием моделей. Генетические алгоритмы уже используют для оптимизации архитектур, в будущем модели смогут разрабатывать такие алгоритмы сами.<br><br>В конце доклада Илья сказал: “All kinds of stuff is possible” — уже распечатали и повесили этот слайд в офисе.<br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaNeurIPS<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/49_480.webp" srcset="../assets/media/thumbs/49_480.webp 480w, ../assets/media/49.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="49" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 621 просмотров · 26 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/49" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/49.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="44" data-search="ещё немного атмосферных фото и видео ⚪команда яндекса: встреча у стенда со статьёй. ⚪наша первая статья (pv-tuning: beyond straight-through estimation for extreme llm compression). ⚪аншлаг на докладе fei-fei li (from seeing to doing: ascending the ladder of visual intelligence). ⚪масштабы и пространства конференции. ⚪новогодний робот на стенде booster robotics. ml underhood #yaneurips ещё немного атмосферных фото и видео ⚪ команда яндекса: встреча у стенда со статьёй. ⚪ наша первая статья ( pv-tuning: beyond straight-through estimation for extreme llm compression ). ⚪ аншлаг на докладе fei-fei li ( from seeing to doing: ascending the ladder of visual intelligence ). ⚪ масштабы и пространства конференции. ⚪ новогодний робот на стенде booster robotics. ml underhood #yaneurips">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-16T08:01:47+00:00" href="./posts/44.html">2024-12-16 08:01 UTC</a></div>
      </div>
      <div class="post-body"><strong>Ещё немного атмосферных фото и видео</strong><br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Команда Яндекса: встреча у стенда со статьёй.<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Наша первая статья (<a href="https://arxiv.org/abs/2405.14852" rel="nofollow noopener noreferrer">PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression</a>).<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Аншлаг на докладе Fei-Fei Li (<a href="https://neurips.cc/virtual/2024/invited-talk/101127" rel="nofollow noopener noreferrer">From Seeing to Doing: Ascending the Ladder of Visual Intelligence</a>).<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Масштабы и пространства конференции.<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Новогодний робот на стенде Booster Robotics.<br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaNeurIPS<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/44_480.webp" srcset="../assets/media/thumbs/44_480.webp 480w, ../assets/media/44.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="44" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/45_480.webp" srcset="../assets/media/thumbs/45_480.webp 480w, ../assets/media/45.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="44" data-image-index="1" /><video controls preload="metadata" src="../assets/media/46_3.mp4"></video><video controls preload="metadata" src="../assets/media/47_tolpa.MOV.mov"></video><video controls preload="metadata" src="../assets/media/48_robot.mp4"></video></div></div>
      <div class="actions">
        <span>1 736 просмотров · 23 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/44" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/44.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="39" data-search="атмосфера neurips: стенды участников на конференции представлены стенды как гигантов, так и малоизвестных компаний. показываем те, которые привлекли наше внимание. ⚪amazon — основной акцент сделали на демонстрации технологий: посетители могут увидеть и попробовать решения в действии. ⚪alibaba cloud — представляют себя как полнофункциональную платформу для инженеров. ⚪writer — фокус на оптимизации рабочих процессов для корпоративных клиентов. ⚪lambda — продвигают private cloud, представлены физические серверные стойки и оборудование. ⚪meta ai* — показывают llama и демо других своих продуктов. *meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф ml underhood #yaneurips атмосфера neurips: стенды участников на конференции представлены стенды как гигантов, так и малоизвестных компаний. показываем те, которые привлекли наше внимание. ⚪ amazon — основной акцент сделали на демонстрации технологий: посетители могут увидеть и попробовать решения в действии. ⚪ alibaba cloud — представляют себя как полнофункциональную платформу для инженеров. ⚪ writer — фокус на оптимизации рабочих процессов для корпоративных клиентов. ⚪ lambda — продвигают private cloud, представлены физические серверные стойки и оборудование. ⚪ meta ai* — показывают llama и демо других своих продуктов. *meta признана экстремистской организацией, а facebook и instagram запрещены на территории рф ml underhood #yaneurips">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-14T09:02:21+00:00" href="./posts/39.html">2024-12-14 09:02 UTC</a></div>
      </div>
      <div class="post-body"><strong>Атмосфера NeurIPS: стенды участников</strong><br><br>На конференции представлены стенды как гигантов, так и малоизвестных компаний. Показываем те, которые привлекли наше внимание. <br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Amazon — основной акцент сделали на демонстрации технологий: посетители могут увидеть и попробовать решения в действии.<br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Alibaba Cloud — представляют себя как полнофункциональную платформу для инженеров.<br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Writer — фокус на оптимизации рабочих процессов для корпоративных клиентов. <br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Lambda — продвигают Private Cloud, представлены физические серверные стойки и оборудование.<br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>Meta AI* — показывают Llama и демо других своих продуктов.<br><br><em>*Meta признана экстремистской организацией, а Facebook и Instagram запрещены на территории РФ</em><br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaNeurIPS<div class="media"><video controls preload="metadata" src="../assets/media/39_1.mp4"></video><video controls preload="metadata" src="../assets/media/40_2.mp4"></video><video controls preload="metadata" src="../assets/media/41_3.mp4"></video><video controls preload="metadata" src="../assets/media/42_4.mp4"></video><video controls preload="metadata" src="../assets/media/43_5.mp4"></video></div></div>
      <div class="actions">
        <span>1 608 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/39" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/39.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="35" data-search="удаление облаков, трекинг антилоп и другие кейсы с neurips продолжаем следить за neurips 2024. сегодня на связи алексей друца, director, technology adoption at yandex cloud. делимся его заметками о туториалах и статьях второго дня конференции. первым привлёк внимание туториал evaluating large language models — principles, approaches, and applications о базовых подходах к оценке llm. особенно полезно ознакомиться тем, кто сталкивается с острым желанием контрагентов решить все проблемы за счёт получения модели идеального качества. мы-то с вами знаем, что копать надо ещё и в сторону продуктовых метрик. подробнее об этом туториале — в канале душный nlp. туториал opening the language model pipeline: a tutorial on data preparation, model training, and adaptation от ребят из ai2. базовый обзор того, как готовится llm, из каких этапов состоит пайплайн её построения. подробно описано, как работать с данными для обучения, обрабатывать, вычищать, принимать решения по ним. разбираются этапы предобучения и постобучения. а ещё — несколько статей с увлекательными и местами неожиданными кейсами. to believe or not to believe your llm: iterativeprompting for estimating epistemic uncertainty авторы предложили метрику для оценки уровня неопределённости llm и того, насколько ей можно доверять. метод основан на итеративных промптах без привлечения внешних данных. allclear: a comprehensive dataset and benchmark for cloud removal in satellite imagery кейс с датасетом об удалении облаков со спутниковых снимков. исследователи определяют, что происходит под облаком, за счёт данных в других диапазонах спектра. потенциально полезная вещь для тех, кто занимается картами. облака — те что на небе, а не вычислительные. spiqa: a dataset for multimodal question answering on scientific papers датасет по мультимодальным вопросам и ответам из научных статей. пример работы со сложными текстами, специфической терминологией и задачей поиска ответа. особенность датасета — наличие картинок и таблиц (которые часто встречаются в научных статьях), а главное — вопросов и ответов по их содержанию. прогнав несколько моделей и разных вариаций промтов, делают вывод, что использование полного текста статьи и приемов chain-of-thoughts приводит к значительно более высокому перформансу модели на датасете. bucktales: a multi-uav dataset for multi-object tracking and re-identification of wild antelopes авторы с помощью дронов и искусственного интеллекта создали набор данных для изучения диких антилоп. он позволяет отслеживать движения множества животных одновременно и распознавать каждую особь в сложных условиях дикой природы. контент с neurips продолжает залетать прямиком в вашу ленту. будем на связи! ml underhood #yaneurips удаление облаков, трекинг антилоп и другие кейсы с neurips продолжаем следить за neurips 2024. сегодня на связи алексей друца, director, technology adoption at yandex cloud. делимся его заметками о туториалах и статьях второго дня конференции. первым привлёк внимание туториал evaluating large language models — principles, approaches, and applications о базовых подходах к оценке llm. особенно полезно ознакомиться тем, кто сталкивается с острым желанием контрагентов решить все проблемы за счёт получения модели идеального качества. мы-то с вами знаем, что копать надо ещё и в сторону продуктовых метрик. подробнее об этом туториале — в канале душный nlp. туториал opening the language model pipeline: a tutorial on data preparation, model training, and adaptation от ребят из ai2. базовый обзор того, как готовится llm, из каких этапов состоит пайплайн её построения. подробно описано, как работать с данными для обучения, обрабатывать, вычищать, принимать решения по ним. разбираются этапы предобучения и постобучения. а ещё — несколько статей с увлекательными и местами неожиданными кейсами. to believe or not to believe your llm: iterativeprompting for estimating epistemic uncertainty авторы предложили метрику для оценки уровня неопределённости llm и того, насколько ей можно доверять. метод основан на итеративных промптах без привлечения внешних данных. allclear: a comprehensive dataset and benchmark for cloud removal in satellite imagery кейс с датасетом об удалении облаков со спутниковых снимков. исследователи определяют, что происходит под облаком, за счёт данных в других диапазонах спектра. потенциально полезная вещь для тех, кто занимается картами. облака — те что на небе, а не вычислительные. spiqa: a dataset for multimodal question answering on scientific papers датасет по мультимодальным вопросам и ответам из научных статей. пример работы со сложными текстами, специфической терминологией и задачей поиска ответа. особенность датасета — наличие картинок и таблиц (которые часто встречаются в научных статьях), а главное — вопросов и ответов по их содержанию. прогнав несколько моделей и разных вариаций промтов, делают вывод, что использование полного текста статьи и приемов chain-of-thoughts приводит к значительно более высокому перформансу модели на датасете. bucktales: a multi-uav dataset for multi-object tracking and re-identification of wild antelopes авторы с помощью дронов и искусственного интеллекта создали набор данных для изучения диких антилоп. он позволяет отслеживать движения множества животных одновременно и распознавать каждую особь в сложных условиях дикой природы. контент с neurips продолжает залетать прямиком в вашу ленту. будем на связи! ml underhood #yaneurips">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-13T09:26:57+00:00" href="./posts/35.html">2024-12-13 09:26 UTC</a></div>
      </div>
      <div class="post-body"><strong>Удаление облаков, трекинг антилоп и другие кейсы с NeurIPS </strong><br><br>Продолжаем следить за NeurIPS 2024. Сегодня на связи Алексей Друца, Director, Technology Adoption at Yandex Cloud. Делимся его заметками о туториалах и статьях второго дня конференции. <br><br><blockquote>Первым привлёк внимание туториал <a href="https://neurips.cc/media/neurips-2024/Slides/99524.pdf" rel="nofollow noopener noreferrer">Evaluating Large Language Models — Principles, Approaches, and Applications</a> о базовых подходах к оценке LLM. Особенно полезно ознакомиться тем, кто сталкивается с острым желанием контрагентов решить все проблемы за счёт получения модели идеального качества. Мы-то с вами знаем, что копать надо ещё и в сторону продуктовых метрик. Подробнее об этом туториале — <a href="https://t.me/stuffyNLP/51" rel="nofollow noopener noreferrer">в канале Душный NLP.</a><br><br>Туториал <a href="https://neurips.cc/virtual/2024/tutorial/99526" rel="nofollow noopener noreferrer">Opening the Language Model Pipeline: A Tutorial on Data Preparation, Model Training, and Adaptation</a> от ребят из Ai2. Базовый обзор того, как готовится LLM, из каких этапов состоит пайплайн её построения. Подробно описано, как работать с данными для обучения, обрабатывать, вычищать, принимать решения по ним. Разбираются этапы предобучения и постобучения.<br><br>А ещё — несколько статей с увлекательными и местами неожиданными кейсами.<br><br><a href="https://openreview.net/pdf?id=k6iyUfwdI9" rel="nofollow noopener noreferrer">To Believe or Not to Believe Your LLM: IterativePrompting for Estimating Epistemic Uncertainty</a><br>Авторы предложили метрику для оценки уровня неопределённости LLM и того, насколько ей можно доверять. Метод основан на итеративных промптах без привлечения внешних данных.<br><br><a href="https://arxiv.org/pdf/2410.23891" rel="nofollow noopener noreferrer">AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery</a><br>Кейс с датасетом об удалении облаков со спутниковых снимков. Исследователи определяют, что происходит под облаком, за счёт данных в других диапазонах спектра. Потенциально полезная вещь для тех, кто занимается картами. Облака — те что на небе, а не вычислительные.<br><br><a href="https://arxiv.org/pdf/2407.09413" rel="nofollow noopener noreferrer">SPIQA: A dataset for multimodal question answering on scientific papers</a><br>Датасет по мультимодальным вопросам и ответам из научных статей. Пример работы со сложными текстами, специфической терминологией и задачей поиска ответа. Особенность датасета — наличие картинок и таблиц (которые часто встречаются в научных статьях), а главное — вопросов и ответов по их содержанию. Прогнав несколько моделей и разных вариаций промтов, делают вывод, что использование полного текста статьи и приемов Chain-of-Thoughts приводит к значительно более высокому перформансу модели на датасете. <br><br><a href="https://arxiv.org/pdf/2411.06896" rel="nofollow noopener noreferrer">BuckTales: A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes</a><br>Авторы с помощью дронов и искусственного интеллекта создали набор данных для изучения диких антилоп. Он позволяет отслеживать движения множества животных одновременно и распознавать каждую особь в сложных условиях дикой природы.</blockquote><br><br>Контент с NeurIPS продолжает залетать прямиком в вашу ленту. Будем на связи!<br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaNeurIPS<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/35_480.webp" srcset="../assets/media/thumbs/35_480.webp 480w, ../assets/media/35.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="35" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/36_480.webp" srcset="../assets/media/thumbs/36_480.webp 480w, ../assets/media/36.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="35" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/37_480.webp" srcset="../assets/media/thumbs/37_480.webp 480w, ../assets/media/37.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="35" data-image-index="2" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/38_480.webp" srcset="../assets/media/thumbs/38_480.webp 480w, ../assets/media/38.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="35" data-image-index="3" /></div></div>
      <div class="actions">
        <span>1 641 просмотров · 20 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/35" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/35.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="32" data-search="первый день neurips: интересные доклады и немного инсайтов на конференции сейчас находится катя серажим, руководитель управления качества поиска. она поделилась впечатлениями от выступлений и поговорила с некоторыми участниками. слово кате. понравился доклад от waymo: ai for autonomous driving at scale. они обзорно рассказали про свой стек: используют трансформеры, пытаются создать собственную foundation-модель для автономных машин и интегрируют vlm для разметки. докладчик, правда, больше «лицо» проекта, а за подробностями отсылает к команде. интересный туториал opening the language model pipeline: a tutorial on data preparation, model training, and adaptation — с практическими советами по обучению llm. любопытно, что некоторые их выводы сходятся с нашими: например, 10k примеров для sft (supervised fine-tuning) и 100k для обучения с подкреплением по вознаграждению (reward) оказались разумными цифрами. а ещё пообщалась на экспо с исследователями из разных компаний и собрала немного любопытного: ⚪️ресёрчер из facebook поделился, что компания всё активнее вкладывается в обучение с подкреплением (rl). считают, что задача претрейна уже решена, и теперь важно оптимизировать последовательность обучения (rs, dpo, ppo). используют синтетику, меняя инстракты: например, из диалога о физике «делают» диалог о биологии. ⚪️а вот google, как оказалось, по-прежнему ориентированы на улучшение результатов через претрейн. говорят, что их новая модель не уступает на некоторых бенчмарках o1 как раз благодаря качественному предобучению. продолжаем следить за neurips, будем делиться самым интересным. ml underhood #yaneurips первый день neurips: интересные доклады и немного инсайтов на конференции сейчас находится катя серажим, руководитель управления качества поиска. она поделилась впечатлениями от выступлений и поговорила с некоторыми участниками. слово кате. понравился доклад от waymo: ai for autonomous driving at scale. они обзорно рассказали про свой стек: используют трансформеры, пытаются создать собственную foundation-модель для автономных машин и интегрируют vlm для разметки. докладчик, правда, больше «лицо» проекта, а за подробностями отсылает к команде. интересный туториал opening the language model pipeline: a tutorial on data preparation, model training, and adaptation — с практическими советами по обучению llm. любопытно, что некоторые их выводы сходятся с нашими: например, 10k примеров для sft (supervised fine-tuning) и 100k для обучения с подкреплением по вознаграждению (reward) оказались разумными цифрами. а ещё пообщалась на экспо с исследователями из разных компаний и собрала немного любопытного: ⚪️ ресёрчер из facebook поделился, что компания всё активнее вкладывается в обучение с подкреплением (rl). считают, что задача претрейна уже решена, и теперь важно оптимизировать последовательность обучения (rs, dpo, ppo). используют синтетику, меняя инстракты: например, из диалога о физике «делают» диалог о биологии. ⚪️ а вот google, как оказалось, по-прежнему ориентированы на улучшение результатов через претрейн. говорят, что их новая модель не уступает на некоторых бенчмарках o1 как раз благодаря качественному предобучению. продолжаем следить за neurips, будем делиться самым интересным. ml underhood #yaneurips">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-12T07:31:13+00:00" href="./posts/32.html">2024-12-12 07:31 UTC</a></div>
      </div>
      <div class="post-body"><strong>Первый день NeurIPS: интересные доклады и немного инсайтов</strong><br><br>На конференции сейчас находится Катя Серажим, руководитель управления качества поиска. Она поделилась впечатлениями от выступлений и поговорила с некоторыми участниками. Слово Кате.<br><br><blockquote>Понравился доклад от Waymo: <a href="https://nips.cc/Expo/Conferences/2024/talk%20panel/100349" rel="nofollow noopener noreferrer">AI for Autonomous Driving at Scale.</a> Они обзорно рассказали про свой стек: используют трансформеры, пытаются создать собственную Foundation-модель для автономных машин и интегрируют VLM для разметки. Докладчик, правда, больше «лицо» проекта, а за подробностями отсылает к команде.<br><br>Интересный туториал <a href="https://docs.google.com/presentation/d/179dpzWSQ9G7EAUlvaJdeE0av9PLuk9Rl33nfhHSJ4xI/mobilepresent?slide=id.g30a4c7e9678_0_0" rel="nofollow noopener noreferrer">Opening the Language Model Pipeline: A Tutorial on Data Preparation, Model Training, and Adaptation</a> — с практическими советами по обучению LLM. Любопытно, что некоторые их выводы сходятся с нашими: например, 10k примеров для SFT (Supervised Fine-Tuning) и 100k для обучения с подкреплением по вознаграждению (Reward) оказались разумными цифрами. <br><br>А ещё пообщалась на экспо с исследователями из разных компаний и собрала немного любопытного:<br><br><tg-emoji emoji-id="5341470192595653209">⚪️</tg-emoji>Ресёрчер из Facebook поделился, что компания всё активнее вкладывается в обучение с подкреплением (RL). Считают, что задача претрейна уже решена, и теперь важно оптимизировать последовательность обучения (RS, DPO, PPO). Используют синтетику, меняя инстракты: например, из диалога о физике «делают» диалог о биологии.<br><br><tg-emoji emoji-id="5341470192595653209">⚪️</tg-emoji>А вот Google, как оказалось, по-прежнему ориентированы на улучшение результатов через претрейн. Говорят, что их новая модель не уступает на некоторых бенчмарках o1 как раз благодаря качественному предобучению.</blockquote><br><br>Продолжаем следить за NeurIPS, будем делиться самым интересным.<br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaNeurIPS<div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/32_480.webp" srcset="../assets/media/thumbs/32_480.webp 480w, ../assets/media/32.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="32" data-image-index="0" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/33_480.webp" srcset="../assets/media/thumbs/33_480.webp 480w, ../assets/media/33.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="32" data-image-index="1" /><img class="media-img" loading="lazy" src="../assets/media/thumbs/34_480.webp" srcset="../assets/media/thumbs/34_480.webp 480w, ../assets/media/34.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="32" data-image-index="2" /></div></div>
      <div class="actions">
        <span>1 540 просмотров · 37 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/32" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/32.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="31" data-search="neurips 2024 в ванкувере: первые цифры и впечатления вчера стартовала neurips, одна из самых крупных и значимых конференций по машинному обучению. для затравки — несколько интересных цифр: ⚪16 500 зарегистрированных участников (рекорд для конференции) ⚪4 037 статей основной секции ⚪460 статей по датасетам и бенчмаркам а вот и впечатления первого дня. костя лахман: бесконечная очередь на регистрацию, которая, не останавливаясь, идёт змейкой. кажется, на самом деле, это лента мёбиуса, и нас обманывают. катя серажим: если говорить про тренды, было уже два доклада с упоминанием специализированных foundation models: для автономного транспорта и e-commerce. настя беззубцева: в exhibition area крайне оживлённо. провела там около двух часов — успела обойти только треть стендов и пообщаться с некоторыми участниками. например, спросила у hr tesla, дают ли они имена своим роботам — к сожалению, нет. в следующих постах продолжим делиться инсайтами и трендами! ml underhood #yaneurips neurips 2024 в ванкувере: первые цифры и впечатления вчера стартовала neurips, одна из самых крупных и значимых конференций по машинному обучению. для затравки — несколько интересных цифр: ⚪ 16 500 зарегистрированных участников (рекорд для конференции) ⚪ 4 037 статей основной секции ⚪ 460 статей по датасетам и бенчмаркам а вот и впечатления первого дня. костя лахман: бесконечная очередь на регистрацию, которая, не останавливаясь, идёт змейкой. кажется, на самом деле, это лента мёбиуса, и нас обманывают. катя серажим: если говорить про тренды, было уже два доклада с упоминанием специализированных foundation models: для автономного транспорта и e-commerce. настя беззубцева: в exhibition area крайне оживлённо. провела там около двух часов — успела обойти только треть стендов и пообщаться с некоторыми участниками. например, спросила у hr tesla, дают ли они имена своим роботам — к сожалению, нет. в следующих постах продолжим делиться инсайтами и трендами! ml underhood #yaneurips">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-11T12:58:48+00:00" href="./posts/31.html">2024-12-11 12:58 UTC</a></div>
      </div>
      <div class="post-body"><strong>NeurIPS 2024 в Ванкувере: первые цифры и впечатления</strong><br><br>Вчера стартовала <a href="https://neurips.cc/" rel="nofollow noopener noreferrer">NeurIPS,</a> одна из самых крупных и значимых конференций по машинному обучению. Для затравки — несколько интересных цифр:<br><br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>16 500 зарегистрированных участников (рекорд для конференции)<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>4 037 статей основной секции<br><tg-emoji emoji-id="5341470192595653209">⚪</tg-emoji>460 статей по датасетам и бенчмаркам<br><br>А вот и впечатления первого дня. <br><br><strong>Костя Лахман:</strong><br><blockquote>Бесконечная очередь на регистрацию, которая, не останавливаясь, идёт змейкой. Кажется, на самом деле, это лента Мёбиуса, и нас обманывают.</blockquote><br><br><strong>Катя Серажим:</strong><br><blockquote>Если говорить про тренды, было уже два доклада с упоминанием специализированных Foundation Models: для автономного транспорта и e-commerce.</blockquote><br><br><strong>Настя Беззубцева:</strong><br><blockquote>В Exhibition Area крайне оживлённо. Провела там около двух часов — успела обойти только треть стендов и пообщаться с некоторыми участниками. Например, спросила у HR Tesla, дают ли они имена своим роботам — к сожалению, нет.</blockquote><br><br>В следующих постах продолжим делиться инсайтами и трендами!<br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><br><br>#YaNeurIPS<div class="media"><video controls preload="metadata" src="../assets/media/31_video_2024-12-11_13-48-47.mp4"></video></div></div>
      <div class="actions">
        <span>1 674 просмотров · 33 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/31" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/31.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    <article class="post" data-post-id="30" data-search="icd — метод быстрого редактирования изображений мы уже рассказывали о восьми статьях, которые yandex research представит на конференции neurips. сегодня подробнее остановимся на одной из них, посвящённой методу инвертируемой дистилляции (invertible consistency distillation, icd). современные модели хороши в генерации изображений, но когда дело касается редактуры — могут возникать проблемы. основная проблема заключается в необходимости точной инверсии: кодирования изображения в латентное пространство и последующей реконструкции, которая должна сохранить исходные детали и при этом интегрировать текстовые запросы. существующие методы редактирования требуют больших вычислительных ресурсов и значительного времени — вплоть до 50 и более шагов инференса. icd призван решить этот недостаток. суть метода заключается в создании двунаправленных моделей: одна из них кодирует изображения в латентное пространство, другая — декодирует их обратно. таким образом, можно довольно быстро отредактировать картинку. скажем, добавить на фотографию какой-нибудь объект или заменить собаку на медведя или енота. чтобы улучшить соответствие между текстовым запросом и сгенерированным изображением используют технику dynamic classifier-free guidance (dynamic cfg). на начальных этапах преобразования, когда уровень шума высок, cfg не работает, что способствует разнообразию генераций. на поздних этапах cfg работает, обеспечивая более точное соответствие текстовому промпту. благодаря всему этому, редактирование изображения с использованием метода icd занимает всего 7–8 шагов инференса. ml underhood icd — метод быстрого редактирования изображений мы уже рассказывали о восьми статьях , которые yandex research представит на конференции neurips. сегодня подробнее остановимся на одной из них , посвящённой методу инвертируемой дистилляции (invertible consistency distillation, icd). современные модели хороши в генерации изображений, но когда дело касается редактуры — могут возникать проблемы. основная проблема заключается в необходимости точной инверсии: кодирования изображения в латентное пространство и последующей реконструкции, которая должна сохранить исходные детали и при этом интегрировать текстовые запросы. существующие методы редактирования требуют больших вычислительных ресурсов и значительного времени — вплоть до 50 и более шагов инференса. icd призван решить этот недостаток. суть метода заключается в создании двунаправленных моделей: одна из них кодирует изображения в латентное пространство, другая — декодирует их обратно. таким образом, можно довольно быстро отредактировать картинку. скажем, добавить на фотографию какой-нибудь объект или заменить собаку на медведя или енота. чтобы улучшить соответствие между текстовым запросом и сгенерированным изображением используют технику dynamic classifier-free guidance (dynamic cfg). на начальных этапах преобразования, когда уровень шума высок, cfg не работает, что способствует разнообразию генераций. на поздних этапах cfg работает, обеспечивая более точное соответствие текстовому промпту. благодаря всему этому, редактирование изображения с использованием метода icd занимает всего 7–8 шагов инференса. ml underhood">
      <div class="post-header">
        <div class="left"></div>
        <div class="right"><a class="post-date" data-iso-date="2024-12-06T14:17:10+00:00" href="./posts/30.html">2024-12-06 14:17 UTC</a></div>
      </div>
      <div class="post-body"><strong>iCD — метод быстрого редактирования изображений</strong><br><br>Мы уже <a href="https://t.me/MLunderhood/27" rel="nofollow noopener noreferrer">рассказывали о восьми статьях</a>, которые Yandex Research представит на конференции NeurIPS. Сегодня подробнее остановимся на <a href="https://arxiv.org/abs/2406.14539" rel="nofollow noopener noreferrer">одной из них</a>, посвящённой методу инвертируемой дистилляции (Invertible Consistency Distillation, iCD).<br><br>Современные модели хороши в генерации изображений, но когда дело касается редактуры — могут возникать проблемы. Основная проблема заключается в необходимости точной инверсии: кодирования изображения в латентное пространство и последующей реконструкции, которая должна сохранить исходные детали и при этом интегрировать текстовые запросы. Существующие методы редактирования требуют больших вычислительных ресурсов и значительного времени — вплоть до 50 и более шагов инференса. <br><br>iCD призван решить этот недостаток. Суть метода заключается в создании двунаправленных моделей: одна из них кодирует изображения в латентное пространство, другая — декодирует их обратно. Таким образом, можно довольно быстро отредактировать картинку. Скажем, добавить на фотографию какой-нибудь объект или заменить собаку на медведя или енота. <br><br>Чтобы улучшить соответствие между текстовым запросом и сгенерированным изображением используют технику Dynamic Classifier-Free Guidance (Dynamic CFG). На начальных этапах преобразования, когда уровень шума высок, CFG не работает, что способствует разнообразию генераций. На поздних этапах CFG работает, обеспечивая более точное соответствие текстовому промпту. <br><br>Благодаря всему этому, редактирование изображения с использованием метода iCD занимает всего 7–8 шагов инференса. <br><br><a href="https://t.me/+59mQ5tDcUnc4OTRi" rel="nofollow noopener noreferrer">ML Underhood</a><div class="media"><img class="media-img" loading="lazy" src="../assets/media/thumbs/30_480.webp" srcset="../assets/media/thumbs/30_480.webp 480w, ../assets/media/30.jpg 1200w" sizes="(max-width: 768px) 100vw, 800px" alt="" data-post-id="30" data-image-index="0" /></div></div>
      <div class="actions">
        <span>2 081 просмотров · 21 реакций</span>
        <span class="action-links"><a href="https://t.me/MLunderhood/30" target="_blank" rel="noopener">Открыть в Telegram</a> · <a href="./posts/30.html">Открыть пост на сайте</a></span>
      </div>
    </article>
    
    </div>
    
    <div class="pager static-pager" style="justify-content:center">
      <div class="page-links">
        <a class="nav-link" href="page-2.html">←</a>
        <a class="page-link" href="index.html">1</a> <a class="page-link" href="page-2.html">2</a> <a class="page-link current" href="page-3.html">3</a> <a class="page-link" href="page-4.html">4</a>
        <a class="nav-link" href="page-4.html">→</a>
      </div>
    </div>
    
  </main>

  <footer class="footer">
    <div class="container">
      <div class="footer-inner">
        <span>based on <a href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">tg-to-gh-pages</a> (created by <a href="https://github.com/ml-brand" target="_blank" rel="noopener">ML Brand</a>)</span>
        <a id="repoLink" href="https://github.com/ml-brand/tg-to-gh-pages" target="_blank" rel="noopener">Do the same with your channel.</a>
        <span class="footer-links">
          static copy ·
          <a href="../feed.xml" target="_blank" rel="noopener">RSS</a> ·
          <a href="../atom.xml" target="_blank" rel="noopener">Atom</a>
        </span>
      </div>
    </div>
  </footer>

  <script>
    window.__STATIC_POSTS = [{"id": 97, "media": [{"kind": "photo", "path": "../assets/media/97.jpg", "thumb": "../assets/media/thumbs/97_480.webp", "size": 57606, "mime": "image/jpeg", "name": null}]}, {"id": 94, "media": [{"kind": "photo", "path": "../assets/media/94.jpg", "thumb": "../assets/media/thumbs/94_480.webp", "size": 184768, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/95.jpg", "thumb": "../assets/media/thumbs/95_480.webp", "size": 169259, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/96.jpg", "thumb": "../assets/media/thumbs/96_480.webp", "size": 200994, "mime": "image/jpeg", "name": null}]}, {"id": 92, "media": [{"kind": "photo", "path": "../assets/media/92.jpg", "thumb": "../assets/media/thumbs/92_480.webp", "size": 75241, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/93.jpg", "thumb": "../assets/media/thumbs/93_480.webp", "size": 102738, "mime": "image/jpeg", "name": null}]}, {"id": 90, "media": [{"kind": "photo", "path": "../assets/media/90.jpg", "thumb": "../assets/media/thumbs/90_480.webp", "size": 61415, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/91.jpg", "thumb": "../assets/media/thumbs/91_480.webp", "size": 54749, "mime": "image/jpeg", "name": null}]}, {"id": 89, "media": []}, {"id": 88, "media": []}, {"id": 87, "media": [{"kind": "photo", "path": "../assets/media/87.jpg", "thumb": "../assets/media/thumbs/87_480.webp", "size": 39543, "mime": "image/jpeg", "name": null}]}, {"id": 86, "media": []}, {"id": 83, "media": [{"kind": "photo", "path": "../assets/media/83.jpg", "thumb": "../assets/media/thumbs/83_480.webp", "size": 82114, "mime": "image/jpeg", "name": null}]}, {"id": 82, "media": [{"kind": "photo", "path": "../assets/media/82.jpg", "thumb": "../assets/media/thumbs/82_480.webp", "size": 117605, "mime": "image/jpeg", "name": null}]}, {"id": 78, "media": [{"kind": "photo", "path": "../assets/media/78.jpg", "thumb": "../assets/media/thumbs/78_480.webp", "size": 94647, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/79.jpg", "thumb": "../assets/media/thumbs/79_480.webp", "size": 127375, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/80.jpg", "thumb": "../assets/media/thumbs/80_480.webp", "size": 159697, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/81.jpg", "thumb": "../assets/media/thumbs/81_480.webp", "size": 125796, "mime": "image/jpeg", "name": null}]}, {"id": 77, "media": [{"kind": "video", "path": "../assets/media/77_IMG_6133.MP4.mp4", "thumb": null, "size": 9714619, "mime": "video/mp4", "name": "IMG_6133.MP4"}]}, {"id": 71, "media": [{"kind": "photo", "path": "../assets/media/71.jpg", "thumb": "../assets/media/thumbs/71_480.webp", "size": 111199, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/72.jpg", "thumb": "../assets/media/thumbs/72_480.webp", "size": 99662, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/73.jpg", "thumb": "../assets/media/thumbs/73_480.webp", "size": 194820, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/74.jpg", "thumb": "../assets/media/thumbs/74_480.webp", "size": 118970, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/75.jpg", "thumb": "../assets/media/thumbs/75_480.webp", "size": 109263, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/76.jpg", "thumb": "../assets/media/thumbs/76_480.webp", "size": 203795, "mime": "image/jpeg", "name": null}]}, {"id": 70, "media": [{"kind": "photo", "path": "../assets/media/70.jpg", "thumb": "../assets/media/thumbs/70_480.webp", "size": 99274, "mime": "image/jpeg", "name": null}]}, {"id": 69, "media": [{"kind": "photo", "path": "../assets/media/69.jpg", "thumb": "../assets/media/thumbs/69_480.webp", "size": 73381, "mime": "image/jpeg", "name": null}]}, {"id": 62, "media": [{"kind": "photo", "path": "../assets/media/62.jpg", "thumb": "../assets/media/thumbs/62_480.webp", "size": 124986, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/63.jpg", "thumb": "../assets/media/thumbs/63_480.webp", "size": 76495, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/64.jpg", "thumb": "../assets/media/thumbs/64_480.webp", "size": 110019, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/65.jpg", "thumb": "../assets/media/thumbs/65_480.webp", "size": 86701, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/66.jpg", "thumb": "../assets/media/thumbs/66_480.webp", "size": 176609, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/67.jpg", "thumb": "../assets/media/thumbs/67_480.webp", "size": 203965, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/68.jpg", "thumb": "../assets/media/thumbs/68_480.webp", "size": 123746, "mime": "image/jpeg", "name": null}]}, {"id": 61, "media": [{"kind": "photo", "path": "../assets/media/61.jpg", "thumb": "../assets/media/thumbs/61_480.webp", "size": 38935, "mime": "image/jpeg", "name": null}]}, {"id": 60, "media": [{"kind": "photo", "path": "../assets/media/60.jpg", "thumb": "../assets/media/thumbs/60_480.webp", "size": 125216, "mime": "image/jpeg", "name": null}]}, {"id": 54, "media": [{"kind": "photo", "path": "../assets/media/54.jpg", "thumb": "../assets/media/thumbs/54_480.webp", "size": 104610, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/55.jpg", "thumb": "../assets/media/thumbs/55_480.webp", "size": 117060, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/56.jpg", "thumb": "../assets/media/thumbs/56_480.webp", "size": 120222, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/57.jpg", "thumb": "../assets/media/thumbs/57_480.webp", "size": 111595, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/58.jpg", "thumb": "../assets/media/thumbs/58_480.webp", "size": 209357, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/59.jpg", "thumb": "../assets/media/thumbs/59_480.webp", "size": 194587, "mime": "image/jpeg", "name": null}]}, {"id": 53, "media": []}, {"id": 52, "media": []}, {"id": 51, "media": []}, {"id": 50, "media": [{"kind": "photo", "path": "../assets/media/50.jpg", "thumb": "../assets/media/thumbs/50_480.webp", "size": 157187, "mime": "image/jpeg", "name": null}]}, {"id": 49, "media": [{"kind": "photo", "path": "../assets/media/49.jpg", "thumb": "../assets/media/thumbs/49_480.webp", "size": 182499, "mime": "image/jpeg", "name": null}]}, {"id": 44, "media": [{"kind": "photo", "path": "../assets/media/44.jpg", "thumb": "../assets/media/thumbs/44_480.webp", "size": 215498, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/45.jpg", "thumb": "../assets/media/thumbs/45_480.webp", "size": 254506, "mime": "image/jpeg", "name": null}, {"kind": "video", "path": "../assets/media/46_3.mp4", "thumb": null, "size": 8714968, "mime": "video/mp4", "name": "3.mp4"}, {"kind": "video", "path": "../assets/media/47_tolpa.MOV.mov", "thumb": null, "size": 5862251, "mime": "video/quicktime", "name": "tolpa.MOV"}, {"kind": "video", "path": "../assets/media/48_robot.mp4", "thumb": null, "size": 5613588, "mime": "video/mp4", "name": "robot.mp4"}]}, {"id": 39, "media": [{"kind": "video", "path": "../assets/media/39_1.mp4", "thumb": null, "size": 8025558, "mime": "video/mp4", "name": "1.mp4"}, {"kind": "video", "path": "../assets/media/40_2.mp4", "thumb": null, "size": 7213615, "mime": "video/mp4", "name": "2.mp4"}, {"kind": "video", "path": "../assets/media/41_3.mp4", "thumb": null, "size": 2459980, "mime": "video/mp4", "name": "3.mp4"}, {"kind": "video", "path": "../assets/media/42_4.mp4", "thumb": null, "size": 5866526, "mime": "video/mp4", "name": "4.mp4"}, {"kind": "video", "path": "../assets/media/43_5.mp4", "thumb": null, "size": 6847542, "mime": "video/mp4", "name": "5.mp4"}]}, {"id": 35, "media": [{"kind": "photo", "path": "../assets/media/35.jpg", "thumb": "../assets/media/thumbs/35_480.webp", "size": 309364, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/36.jpg", "thumb": "../assets/media/thumbs/36_480.webp", "size": 217288, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/37.jpg", "thumb": "../assets/media/thumbs/37_480.webp", "size": 249243, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/38.jpg", "thumb": "../assets/media/thumbs/38_480.webp", "size": 251455, "mime": "image/jpeg", "name": null}]}, {"id": 32, "media": [{"kind": "photo", "path": "../assets/media/32.jpg", "thumb": "../assets/media/thumbs/32_480.webp", "size": 107877, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/33.jpg", "thumb": "../assets/media/thumbs/33_480.webp", "size": 159488, "mime": "image/jpeg", "name": null}, {"kind": "photo", "path": "../assets/media/34.jpg", "thumb": "../assets/media/thumbs/34_480.webp", "size": 302152, "mime": "image/jpeg", "name": null}]}, {"id": 31, "media": [{"kind": "video", "path": "../assets/media/31_video_2024-12-11_13-48-47.mp4", "thumb": null, "size": 7899570, "mime": "video/mp4", "name": "video_2024-12-11_13-48-47.mp4"}]}, {"id": 30, "media": [{"kind": "photo", "path": "../assets/media/30.jpg", "thumb": "../assets/media/thumbs/30_480.webp", "size": 136580, "mime": "image/jpeg", "name": null}]}];
    window.__STATIC_META = {"title": "ML Underhood", "username": "MLunderhood", "channel": "MLunderhood", "last_sync_utc": "2026-02-10T01:04:03Z", "posts_count": 111, "last_seen_message_id": 283, "stats": {"new": 161, "updated": 4, "media_downloaded": 161}, "avatar": "assets/channel_avatar.jpg", "meta_schema_version": "1.0.0", "posts_schema_version": "1.0.0"};
  </script>
  <script src="../common.js"></script>
  <script src="../static.js"></script>
</body>
</html>
